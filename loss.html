<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Understanding Loss Functions: BCE, CE, and CTC</title>
  <style>
    body {
      font-family: sans-serif;
      max-width: 900px;
      margin: 0 auto;
      padding: 2rem;
      line-height: 1.6;
      color: #333;
    }
    h1, h2, h3, h4 {
      color: #2c3e50; /* Darker shade for better contrast */
      margin-top: 1.5em;
      margin-bottom: 0.5em;
    }
    h1 { font-size: 2.5em; border-bottom: 2px solid #3498db; padding-bottom: 0.3em;}
    h2 { font-size: 2em; border-bottom: 1px solid #bdc3c7; padding-bottom: 0.2em;}
    h3 { font-size: 1.5em; }
    h4 { font-size: 1.2em; color: #555;}

    nav { margin-bottom: 30px; padding: 10px; background: #ecf0f1; border: 1px solid #bdc3c7; border-radius: 4px;}
    nav ul { list-style: none; padding: 0; }
    nav li { display: inline-block; margin-right: 15px; }
    nav a { text-decoration: none; color: #3498db; font-weight: bold;}
    nav a:hover { text-decoration: underline; color: #2980b9;}

    pre {
      background: #f8f9f9; /* Lighter background for code blocks */
      padding: 1rem;
      overflow-x: auto;
      border: 1px solid #e1e4e8; /* Softer border */
      border-left: 4px solid #3498db; /* Accent border */
      border-radius: 4px;
      font-size: 0.9em;
    }
    code {
      font-family: 'SFMono-Regular', Consolas, 'Liberation Mono', Menlo, Courier, monospace;
    }
    /* For inline code */
    p > code, li > code, table td > code {
      background: #e8eaed;
      padding: 0.2em 0.4em;
      border-radius: 3px;
      font-size: 0.85em;
    }
    pre code { /* Reset for code inside pre, already handled by pre styling */
        background: none;
        padding: 0;
        font-size: 1em; /* Ensure pre's font size is inherited */
    }
    ul, ol {
        padding-left: 20px;
    }
    li {
        margin-bottom: 0.5em;
    }
    strong {
        color: #2980b9;
    }
    hr {
      border: 0;
      height: 1px;
      background: #bdc3c7;
      margin-top: 2em;
      margin-bottom: 2em;
    }
  </style>
 
  <script type="text/javascript" async
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
  </script>
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [['$','$'], ['\\(','\\)']],
        displayMath: [['$$','$$'], ['\\[','\\]']],
        processEscapes: true
      }
    });
  </script>

  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
  <script>hljs.highlightAll();</script>
</head>
<body>
    <nav>
        <ul>
            <li><a href="#loss-functions">Loss Functions</a></li>
            <li><a href="#bce">Binary Cross-Entropy</a></li>
            <li><a href="#ce">Categorical Cross-Entropy</a></li>
            <li><a href="#ctc">CTC Loss</a></li>
        </ul>
    </nav>

    <h1 id="loss-functions">Understanding Loss Functions for Sequence Problems</h1>
    
    <h2 id="bce">Binary Cross-Entropy (BCE)</h2>
    <p>
       Let’s start with **Binary Cross-Entropy (BCE)**.
    </p>
    $$ L = -\sum_i (y_i \log(p'_i) + (1-y_i)\log(1-p'_i)) $$
    <p>
        Where $p'_i$ is the output of a sigmoid function applied to the logit for class $i$, and $y_i$ is the ground truth label (0 or 1) indicating if that class was present.
    </p>
    <p>
        First, you squash the logits into probabilities using a sigmoid. Then, you compare that prediction with the ground truth using the BCE loss formula. With BCE, each class is treated as an independent binary prediction. You can extend this to multiple labels by passing each logit through a sigmoid, computing the BCE loss per class, and then averaging over all classes. This lets you model multiple classes co-occurring.
    </p>
    <p>
        With BCE, you can’t compress the label into a single integer anymore; you need a multi-hot vector because each class is treated as an independent yes/no problem. The full formula for $C$ classes is:
    </p>
    $$ \text{BCE}(y,p) = -\sum_{i=1}^{C}(y_i \log(p_i) + (1-y_i) \log(1-p_i)) $$
    <pre><code class="language-python">
import torch

def bce_with_logits_loss(logits, targets):
    """
    logits: (N, C) raw outputs
    targets: (N, C) multi-hot labels {0,1}
    """
    # sigmoid
    probs = 1 / (1 + torch.exp(-logits))

    # binary cross entropy per class
    bce = -(targets * torch.log(probs) + (1 - targets) * torch.log(1 - probs))

    # average over classes and batch
    loss = bce.mean()
    return loss
    </code></pre>
    
    <hr>
    
    <h2 id="ce">Categorical Cross-Entropy (CE)</h2>
    <p>
        <strong>Categorical Cross-Entropy (CE)</strong> is for multi-class problems where the classes are mutually exclusive—exactly one class is correct at a time.
    </p>
    <p>With CE, you first apply a **softmax** over all the logits to get a probability distribution over the $C$ classes.</p>
    $$ P_i = \frac{\exp(z_i)}{\sum_j \exp(z_j)} $$
    <p>
        Then you apply the cross-entropy loss against the one-hot target vector. Since only one class is true, the loss simplifies to just looking at the predicted probability for the correct class:
    </p>
    $$ \text{CE} = -\log(P(\text{true class})) $$
    <p>
        The negative log means a lower loss corresponds to a higher probability for the true class. If the model was sure (e.g., probability 0.9), the loss is small. If it was unsure (e.g., 0.1), the loss is large. CE punishes the model when it assigns a low probability to the true class.
    </p>
    <pre><code class="language-python">
import torch

def cross_entropy_loss(logits, target_index):
    """
    logits: (N, C) raw outputs
    target_index: (N,) integer labels (0 <= y < C)
    """
    # softmax
    exp_logits = torch.exp(logits)
    probs = exp_logits / exp_logits.sum(dim=1, keepdim=True)

    # pick probability of the correct class
    correct_class_probs = probs[torch.arange(len(target_index)), target_index]

    # negative log likelihood
    loss = -torch.log(correct_class_probs).mean()
    return loss
    </code></pre>

    <hr>

    <h2 id="ctc">Connectionist Temporal Classification (CTC) Loss</h2>
    <p>
        If we had a sequence of input frames and we knew the ground-truth label for each frame, we could just apply CE at each frame and average the losses.
    </p>
    $$ \text{Loss} = -\frac{1}{T} \sum_{t=1}^{T} \log(p(t, y_t)) $$
    <p>
        But this assumes a one-to-one alignment between frames and labels. In tasks like speech recognition, we don’t have this. The input speech sequence (e.g., 1000 frames for 10 seconds) is much longer than the output text tokens (e.g., 20 words), and we don't know the frame-to-token alignment.
        A model like Conformer gives us per-frame token probabilities. This is a probability distribution over classes for each time step. So, why can’t we just use a standard categorical cross-entropy here?
    </p>
    <p>
        We need a loss that doesn’t require explicit alignment and can collapse variable-length frame predictions into a shorter token sequence. This is where the <strong>Connectionist Temporal Classification (CTC)</strong> loss comes to the rescue.
    </p>

    <h3>The Core Idea of CTC</h3>
    <p>
        CTC augments the vocabulary with a special <strong>blank token</strong> ('_'). This allows some frames to correspond to no output symbol. So, for each of the $T$ frames, the model now outputs a probability distribution over $|V|+1$ classes (all tokens + the blank token).
    </p>
    <p>
        To get the final transcription, you first merge repeated tokens and then remove all the blank tokens from the frame-level sequence. For example, both <code>{C, C, _, A, A, T, _}</code> and <code>{_, C, A, _, T, T, _}</code> would collapse to the final transcription "CAT".
    </p>
    <p>
        The CTC loss uses dynamic programming to efficiently sum the probabilities of all possible frame-level sequences (alignments) that correctly collapse to the ground-truth text.
    </p>
    
    <h3>The CTC Loss Function</h3>
    <p>
        The total probability of a ground-truth sequence $y$ given an input sequence $x$ is the sum of probabilities of all valid alignments.
    </p>
    $$ P(y|x) = \sum_{\pi \in \mathcal{B}^{-1}(y)} \prod_{t=1}^{T} p(\pi_t | x_t) $$
    <p>Where:</p>
    <ul>
        <li>$y$ is the ground-truth sequence (e.g., "CAT").</li>
        <li>$x$ is the input sequence of frames.</li>
        <li>$p(\pi_t | x_t)$ is the model’s softmax probability of symbol $\pi_t$ at frame $t$.</li>
        <li>$\pi = (\pi_1, \dots, \pi_T)$ is one possible alignment path.</li>
        <li>$\mathcal{B}(\pi)$ is the collapse function that removes blanks and merges repeats.</li>
        <li>$\mathcal{B}^{-1}(y)$ is the set of all alignment paths that collapse to the ground truth $y$.</li>
    </ul>
    <p>The CTC loss is then just the negative log-likelihood of this probability:</p>
    $$ L_{CTC} = -\log P(y|x) $$

    <h3>The Forward-Backward Algorithm</h3>
    <p>
        Calculating the sum over all possible alignments is computationally intractable. CTC uses a forward-backward dynamic programming algorithm to compute this sum efficiently.
    </p>
    <p>
        The first step is to create an extended target sequence by inserting blank tokens. For a target "CAT", the extended sequence is <code>_C_A_T_</code>. We then build a DP table (a lattice) of size $T \times (2|y|+1)$. A path through this grid represents one possible alignment. The forward algorithm fills this table to calculate the total probability $P(y|x)$.
    </p>
    <pre><code class="language-python">
import numpy as np

def ctc_forward(probs, target_map, target):
    """
    probs: array (T, V) with softmax probs for each frame
           V includes all symbols + blank
    target_map: dict mapping symbol string to index
    target: list of symbols, e.g. ['C','A','T']
    """
    # 1. Build extended target
    extended = ['_']
    for ch in target:
        extended += [ch, '_']
    S = len(extended)
    T = probs.shape[0]
    alpha = np.zeros((T, S))

    # 2. Initialize first frame
    alpha[0][0] = probs[0][target_map[extended[0]]]  # prob of blank at t=0
    alpha[0][1] = probs[0][target_map[extended[1]]]  # prob of 'C' at t=0

    # 3. Fill DP table
    for t in range(1, T):
        for s in range(S):
            stay = alpha[t-1][s]
            prev = alpha[t-1][s-1] if s-1 >= 0 else 0
            # Skip transition is only allowed if current and prev-prev symbols are different (and not blank)
            skip = alpha[t-1][s-2] if (s-2 >= 0 and extended[s] != '_' and extended[s] != extended[s-2]) else 0
            
            # Probability of current symbol at time t
            p_ts = probs[t][target_map[extended[s]]]
            alpha[t][s] = p_ts * (stay + prev + skip)

    # 4. Final probability is sum of last two states in extended target
    return alpha[T-1][S-1] + alpha[T-1][S-2]
    </code></pre>
    <p>
        The forward pass is enough to compute the loss. However, for backpropagation, we need the gradient of the loss with respect to each logit at each frame. This requires knowing the **posterior probability**—given all possible alignments, what is the probability that a specific frame $t$ corresponds to a specific symbol in the extended target?
    </p>
    <p>
        This is calculated using both the forward probability $\alpha$ (the probability of reaching a state) and a backward probability $\beta$ (the probability of completing the sequence from that state). Multiplying them gives the total probability of all paths passing through that state.
    </p>

    <h3>Inference with CTC</h3>
    <p>
        At inference time, we need to find the most likely output sequence from the frame-level probabilities.
    </p>
    <ul>
        <li><strong>Greedy Decoding</strong>: At each frame, take the most probable symbol (the argmax). Then collapse the resulting sequence. This is simple and fast but often suboptimal because it doesn't consider the full space of alignments.</li>
        <li><strong>Beam Search</strong>: A more effective heuristic. At each time step, it keeps track of the top-k most probable candidate sequences (the "beam"). It extends each candidate with new symbols and merges paths that collapse to the same output string by summing their probabilities, then prunes the list back down to the beam size. At the end, it returns the candidate with the highest total probability.</li>
    </ul>
    <pre><code class="language-python">
def simple_ctc_beam_search(probs, vocab, beam_size=3):
    """
    probs: (T, V) softmax probabilities over vocab (incl blank at index 0)
    vocab: list of symbols
    """
    T, V = probs.shape
    beams = [("", 1.0)] # (prefix, probability)

    for t in range(T):
        new_beams = {}
        # Sort candidates by probability
        sorted_beams = sorted(beams, key=lambda x: -x[1])[:beam_size]

        for prefix, prob in sorted_beams:
            for i in range(V):
                p = probs[t][i]
                char = vocab[i]

                if char == '_':
                    new_prefix = prefix
                else:
                    new_prefix = prefix + char if not prefix.endswith(char) else prefix

                # Update probability of the new prefix
                new_beams[new_prefix] = new_beams.get(new_prefix, 0) + prob * p
        
        # Prune to top beam_size
        beams = sorted(new_beams.items(), key=lambda x: -x[1])[:beam_size]
    
    # Return the best beam
    return beams[0] if beams else ("", 0.0)
    </code></pre>
    
</body>
</html>
