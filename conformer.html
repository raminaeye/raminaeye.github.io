<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Conformer: Convolution-Augmented Transformer for ASR</title>
    <style>
        body {
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
            line-height: 1.6;
            color: #333;
            max-width: 800px;
            margin: 0 auto;
            padding: 20px;
        }
        h1, h2 {
            border-bottom: 1px solid #eaecef;
            padding-bottom: 0.3em;
        }
        code {
            font-family: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, Courier, monospace;
            background-color: #f6f8fa;
            padding: 0.2em 0.4em;
            margin: 0;
            font-size: 85%;
            border-radius: 3px;
        }
        pre {
            background-color: #f6f8fa;
            padding: 16px;
            overflow: auto;
            border-radius: 6px;
        }
        pre code {
            padding: 0;
            margin: 0;
            font-size: 100%;
            background-color: transparent;
        }
    </style>
</head>
<body>

    <h1>Understanding the Conformer ASR Model</h1>

    <p>Let’s take a look at an ASR model that blends CNNs with transformers for speech. The vanilla transformers are great for global context but weak at capturing local details like phoneme edges and formants. CNNs are excellent for local structure, but they miss global dependencies. The Conformer, a "convolution-augmented transformer," has both.</p>

    <h2>Input Preprocessing</h2>

    <p>Unlike text, AST takes continuous audio as input. Because audio data has a high sampling rate, such as 16 kHz, we first need to extract acoustic features. Common choices are log-mel spectrogram or filterbank features. For 1 second of speech, you might get 100 frames by 80 dimensions ($100 \times 80$). Just like the embedding layer that projects 1D tokens into a vector space, in speech, there is a linear layer or convolutional layer that projects the acoustic features into the model dimension, $d_{model}$. For example, it can be an $80 \times 256$ matrix, so now your token embeddings are $T \times d_{model}$ (e.g., $100 \times 256$).</p>

    <h2>Architecture: From Transformer to Conformer</h2>

    <p>A transformer encoder block looked like this:</p>
    <p><code>x → [MHSA] → +residual → [FFN] → +residual → out</code></p>

    <p>Conformer changes this to:</p>
    <p><code>x → [FFN(0.5)] → [MHSA] → [Conv] → [FFN(0.5)] → out</code></p>
    
    <p>The Conformer block calls these two feed-forward networks a "macaron-style feed-forward network." They add capacity and non-linearity but don’t mix across time steps. They scale the FFN by half to match the contribution of one FFN in a vanilla transformer.</p>
    
    <p>The first FFN applies a 2-layer MLP to each frame independently. $d_{ff}$ might be 1024 with some activations, which then scale back so a half residual connection can be added with the input.</p>

    <h2>Multi-Head Self-Attention with Relative Positional Encoding</h2>

    <p>So far, the input is $B \times T \times d_{model}$; this is the output from the first FFN + residual. Now, you project X into queries, keys, and values:</p>
    <p>$Q = XW_q^i$, $K = XW_k^i$, $V = XW_v^i$</p>

    <p>Where the W's are $d_{model}/k \times d_{model}/k$ for $k$ heads. Compute attention:</p>
    $$ \text{Attn}(Q, K, V) = \text{Softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V $$

    <p>Concatenate heads and project back with $W_o$ and add a residual:</p>
    $$ Y = X + \text{MHSA}(X) $$

    <p>It’s almost the same as a vanilla transformer, but instead of sinusoidal encodings, it uses relative positions that generalize better to long sequences and relative timing.</p>

    <p>With absolute positional encodings for each input position:</p>
    $$ x_t \leftarrow x_t + PE(t) $$
    
    <p>The model is given the known positions, and now the query and key themselves get shifted by this absolute index.</p>
    
    <p>With relative positional encoding, we don’t alter the input embeddings directly. Instead, we introduce an extra bias term in the attention score based on relative distance. The model learns biases based on the relative distances between tokens. Speech is shift-invariant; "ba" at 1s and 10s should be treated the same. What matters is how far the tokens are apart, not their absolute index.</p>

    $$ \text{score}(i,j) = Q_i \cdot K_j + Q_i \cdot r_{(i-j)} $$

    <p>Here, $r_{(i-j)}$ is a learned embedding for the relative offset between positions $i$ and $j$. We define a maximum relative distance, and this offset can be anywhere between $-L \dots 0 \dots L$, where each row corresponds to a $d_k$ vector space. For a given pair, we look up the relative embedding and perform a dot product with the query that gets added into the attention scores. The embedding table is $(2L+1) \times d_k$. You can share R across all heads.</p>

    <h2>The Convolution Module</h2>

    <p>Next is the convolution module, which is the big difference from vanilla transformers. This is a depthwise separable convolution to explicitly model local sequential structure.</p>

    <p>The input is now the context-enriched sequences, wherein each frame is mixed with others and long-range dependencies have been learned; the shape is still $B \times T \times d_{model}$. The convolution takes this and applies local convolutional patterns to learn short-term temporal patterns on top of the long-range dependencies already captured by attention.</p>

    <p>The convolution module looks similar to those in EfficientNet and MobileNet. The features are first passed to a Layer Norm to be normalized along the $d_{model}$ dimension.</p>
    
    <p>Then, a pointwise convolution with a gated linear unit (GLU) activation is applied. The pointwise convolution expands the number of channels to $2 \times d_{model}$. The GLU works by halving the channels and applying $A \cdot \text{sigmoid}(B)$. B acts as a learned gate, controlling how much of A to pass. The output dimension would remain at $B \times T \times d_{model}$.</p>

    <p>Then, a depthwise convolution is applied along the time axis. Each channel has its own convolution filter, with no cross-channel mixing, using a 150 ms kernel size. This gives each feature map a local receptive field over time. The input is padded ('same') such that the output from this step remains the same size as the input. In streaming ASR, you’d switch to causal padding (only on the left, so the model doesn’t peek into the future). At this point, the representation is channel-local and time-dependent.</p>

    <h2>Normalization and Activation</h2>
    
    <p>Let’s think, would you use LayerNorm or BatchNorm after this? LayerNorm would normalize per frame across all channels, so it could actually wash away the learned temporal filters. If you use BatchNorm, it normalizes per channel but across time and batch. So each channel keeps its identity, and normalization is consistent across time, which would stabilize the convolution filters. So BatchNorm makes more sense, although it has running inference issues. BN would output zero-mean and unit-variance activation, so an activation function would help here. Swish would let small negative values pass through, which is crucial in audio.</p>

    <p>Let’s briefly discuss BatchNorm during training and inference. During training, BatchNorm computes the mean and variance of the batch and time dimensions for each channel to normalize the values, along with scale and shift learnable parameters for each channel. It additionally keeps a running estimate of both mean and variance, which is not used during the training forward pass but is stored to be used for inference later. During inference, you don’t use the batch statistics of the test data. You use the running stats along with the same learnable parameters to normalize the data.</p>
    $$ x_n = \gamma \cdot \frac{x - \mu_{\text{running}}}{\sqrt{\sigma^2_{\text{running}} + \epsilon}} + \beta $$
    
    <p>Now, with streaming ASR, the issue is that during training, the stats were computed on long sequences and full batches of data. During streaming, you’re normalizing with global averages that don’t match the local chunk you’re currently processing. Additionally, in streaming, you don’t want to wait for future frames to compute stats, so you need a causal/online BN. There are variations of causal BN where the mean and variance are computed over frames $\le t$, so normalization only depends on the past. It’s incrementally updated as new frames arrive. There’s also sliding-window BN that updates only based on the last W frames. In the beginning, statistics can be unreliable, however, so they typically kick in when the state is reliable again. Alternatively, you can use other normalizations that are more real-time friendly, like LN.</p>
    
    <p>Anyways, Conformer then applies a BatchNorm and a Swish activation. Finally, another pointwise convolution with dropout is applied, which does $d_{model} \rightarrow d_{model}$. Lastly, a residual connection from the original input is added from the start of the module. This might seem redundant, but it introduces another cross-channel interaction, which can be helpful for expressiveness, as depthwise convolution is a per-channel local filter.</p>

    <h2>Completing the Conformer Block</h2>

    <p>This is our convolution block. The output from the convolution block remains $B \times T \times d_{model}$. After this, another feed-forward network similar to the first one is applied per frame with a residual to complete the macaron-style design. It expands $d_{model}$ to $d_{ff}$, applies non-linearity, projects back to $d_{model}$ with dropout, and adds a scaled version of that as a residual. The shape remains $B \times T \times d_{model}$. This helps with stacking blocks, and we know ML people love stacking blocks!</p>
    
    <p>The first FFN before attention enriched the frame-level features before they interacted globally. The second FFN enriches the representation after they are mixed globally and locally. This FFN symmetry is what is claimed to make the block more expressive and stable.</p>

    <h2>PyTorch Implementation</h2>
<pre><code class="language-python">
import torch
import torch.nn as nn
import torch.nn.functional as F

# -------------------------------
# 1. Macaron-style FeedForward
# -------------------------------
class ConformerFFN(nn.Module):
    def __init__(self, d_model, d_ff, dropout=0.1):
        super().__init__()
        self.linear1 = nn.Linear(d_model, d_ff)
        self.linear2 = nn.Linear(d_ff, d_model)
        self.dropout = nn.Dropout(dropout)
        self.activation = nn.SiLU()  # Swish

    def forward(self, x):
        out = self.linear1(x)
        out = self.activation(out)
        out = self.dropout(out)
        out = self.linear2(out)
        out = self.dropout(out)
        return x + 0.5 * out   # Macaron: scale by 0.5
        
# -------------------------------
# 2. Multi-Head Self-Attention (vanilla version, can extend with relative PE)
# -------------------------------
class MHSA(nn.Module):
    def __init__(self, d_model, num_heads, dropout=0.1):
        super().__init__()
        assert d_model % num_heads == 0
        self.h = num_heads
        self.d_k = d_model // num_heads

        self.W_q = nn.Linear(d_model, d_model)
        self.W_k = nn.Linear(d_model, d_model)
        self.W_v = nn.Linear(d_model, d_model)
        self.W_o = nn.Linear(d_model, d_model)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x, mask=None):
        B, T, D = x.shape

        Q = self.W_q(x).view(B, T, self.h, self.d_k).transpose(1, 2)
        K = self.W_k(x).view(B, T, self.h, self.d_k).transpose(1, 2)
        V = self.W_v(x).view(B, T, self.h, self.d_k).transpose(1, 2)

        scores = torch.matmul(Q, K.transpose(-2, -1)) / (self.d_k ** 0.5)
        if mask is not None:
            scores = scores.masked_fill(mask == 0, float("-inf"))
        attn = F.softmax(scores, dim=-1)
        out = torch.matmul(attn, V)

        out = out.transpose(1, 2).contiguous().view(B, T, D)
        return x + self.dropout(self.W_o(out))  # residual

# -------------------------------
# 3. Convolution Module
# -------------------------------
class ConformerConvModule(nn.Module):
    def __init__(self, d_model, kernel_size=15, dropout=0.1, causal=False):
        super().__init__()
        self.layer_norm = nn.LayerNorm(d_model)
        self.pointwise_conv1 = nn.Conv1d(d_model, 2*d_model, kernel_size=1)
        padding = (kernel_size - 1) if causal else (kernel_size - 1) // 2
        self.depthwise_conv = nn.Conv1d(
            d_model, d_model, kernel_size,
            groups=d_model, padding=padding
        )
        self.causal = causal
        self.batch_norm = nn.BatchNorm1d(d_model)
        self.pointwise_conv2 = nn.Conv1d(d_model, d_model, kernel_size=1)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x):
        residual = x
        x = self.layer_norm(x)
        x = x.transpose(1, 2)  # (B, d_model, T)

        x = self.pointwise_conv1(x)
        A, B = x.chunk(2, dim=1)
        x = A * torch.sigmoid(B)  # GLU

        x = self.depthwise_conv(x)
        if self.causal:
            x = x[:, :, :residual.size(1)]  # trim if causal

        x = self.batch_norm(x)
        x = F.silu(x)
        x = self.pointwise_conv2(x)

        x = x.transpose(1, 2)  # back to (B, T, d_model)
        return residual + self.dropout(x)  # residual

# -------------------------------
# 4. Full Conformer Block
# -------------------------------
class ConformerBlock(nn.Module):
    def __init__(self, d_model=256, d_ff=1024, num_heads=4, kernel_size=15, dropout=0.1, causal=False):
        super().__init__()
        self.ffn1 = ConformerFFN(d_model, d_ff, dropout)
        self.mhsa = MHSA(d_model, num_heads, dropout)
        self.conv = ConformerConvModule(d_model, kernel_size, dropout, causal)
        self.ffn2 = ConformerFFN(d_model, d_ff, dropout)
        self.final_ln = nn.LayerNorm(d_model)  # often added at block end

    def forward(self, x, mask=None):
        x = self.ffn1(x)
        x = self.mhsa(x, mask)
        x = self.conv(x)
        x = self.ffn2(x)
        return self.final_ln(x)
</code></pre>
    
    <h2>Real-Time and Streaming Applications</h2>

    <p>Now, how do you make the Conformer work in real-time? The attention mechanism, as we mentioned earlier, needs to change to a streaming attention where we restrict how far queries can look back using a fixed attention window with causal masking. Sometimes speech can be ambiguous if you don’t peek a little into the future, so we allow each frame to have a look-ahead by a fixed number of frames. This adds some latency but improves the model, and it can easily be configured in the attention mask. Additionally, you can make your multi-head attention into a multi-scale multi-head attention. Because information varies in length (a phoneme can be shorter, while syllables and words are longer), one head might span more frames than others to capture these relationships.</p>

    <p>The convolution module uses 'same' padding, which will need to change to causal padding, and the normalization, as we discussed in depth, needs to change as well.</p>

    <h2>Output and Loss Function</h2>

    <p>Okay, now we have our Conformer block, and we can stack several Conformer blocks to enrich the acoustic embedding. Now we need to map this to text. What we have are frame-level embeddings that need to be converted back to words or tokens.</p>

    <p>You can add a linear layer after that takes $d_{model} \rightarrow V$ (the number of words/tokens/phonemes in the vocabulary). The output is $Z = B \times T \times V$. Then, take the model outputs (logits, which are unnormalized scores) and pass them through a softmax function per frame token to output probabilities.</p>
    $$ P(t,v) = \frac{\exp(Z_{t,v})}{\sum_{v'} \exp(Z_{t,v'})} $$
    
    <p>Once you have this, you need to use a CTC loss to align the output probabilities with the text.</p>
    
</body>
</html>
