<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Neural Architecture Search (NAS)</title>
    <style>
        body {
            font-family: sans-serif;
            line-height: 1.6;
            margin: 20px;
            max-width: 900px;
            margin-left: auto;
            margin-right: auto;
            color: #333;
        }
        h1, h2, h3 {
            color: #2c3e50;
        }
        h1 {
            border-bottom: 2px solid #3498db;
            padding-bottom: 10px;
        }
        h2 {
            border-bottom: 1px solid #ddd;
            padding-bottom: 5px;
        }
        code {
            background-color: #f4f4f4;
            padding: 2px 6px;
            border-radius: 4px;
            font-family: monospace;
        }
        pre code {
            display: block;
            padding: 15px;
            overflow-x: auto;
            white-space: pre;
        }
        table {
            width: 100%;
            border-collapse: collapse;
            margin-bottom: 20px;
        }
        th, td {
            border: 1px solid #ddd;
            padding: 8px;
            text-align: left;
        }
        th {
            background-color: #f2f2f2;
        }
        ul {
            margin-bottom: 15px;
        }
        li {
            margin-bottom: 5px;
        }
        nav { margin-bottom: 30px; padding: 10px; background: #ecf0f1; border: 1px solid #bdc3c7; border-radius: 4px;}
        nav ul { list-style: none; padding: 0; }
        nav li { display: inline-block; margin-right: 15px; }
        nav a { text-decoration: none; color: #3498db; font-weight: bold;}
        nav a:hover { text-decoration: underline; color: #2980b9;}

    </style>
    <script type="text/javascript" async
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
    </script>
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            tex2jax: {
                inlineMath: [['$','$'], ['\\(','\\)']],
                displayMath: [['$$','$$'], ['\\[','\\]']],
                processEscapes: true
            }
        });
    </script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/monokai.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
</head>
<body>
    <nav>
     <ul>
      <li><a href="index.html">Home</a></li> 
     </ul>
   </nav>
    <h1>Neural Architecture Search (NAS)</h1>

    <p><strong>Neural Architecture Search (NAS)</strong> automates the design of neural networks. The process involves defining a <strong>search space</strong> of possible architectural components, a <strong>search strategy</strong> to explore this space, and an <strong>evaluation strategy</strong> to score candidate models. The primary goal is to discover the optimal architecture considering metrics like accuracy, latency/FLOPs, model size, and hardware constraints.</p>

    <p>A NAS algorithm can manipulate various architectural elements, including:</p>
    <ul>
        <li>Convolutional filter sizes</li>
        <li>Number of channels</li>
        <li>Block types (e.g., MBConv, regular conv)</li>
        <li>Network depth</li>
        <li>Skip connections</li>
        <li>Attention heads</li>
    </ul>

    <hr>

    <h2>Core Components of NAS</h2>

    <h3>Search Strategies</h3>
    <p>Several strategies are employed to navigate the vast search space:</p>
    <ul>
        <li><strong>Reinforcement Learning (RL)</strong>: An RNN controller samples different architecture configurations. A reward signal (e.g., accuracy) is fed back to the controller to guide its search. Google's NASNet utilized this approach.</li>
        <li><strong>Evolutionary Algorithms</strong>: These methods maintain a population of network architectures. Offspring are generated through random mutations, and the fittest individuals are selected for the next generation. AmoebaNet is an example.</li>
        <li><strong>Bayesian Optimization</strong>: This strategy treats architecture search as a black-box optimization problem. It leverages past evaluations to predict which architectures are likely to perform well.</li>
        <li><strong>Gradient-based Search</strong>: A "supernet" is defined, encompassing all possible architectural choices (soft choices of layers). This supernet is trained using gradient descent to identify the best operations.</li>
    </ul>

    <h3>Evaluation Strategies</h3>
    <p>Efficiently evaluating candidate architectures is crucial:</p>
    <ul>
        <li><strong>Proxy Task</strong>: Training candidate models for a reduced number of epochs or on a smaller dataset.</li>
        <li><strong>Weight Sharing</strong>: Candidate architectures share weights from a pre-trained supernet, significantly speeding up evaluation.</li>
        <li><strong>Low-fidelity Scoring</strong>: Estimating performance based on metrics like latency or FLOPs without full training.</li>
    </ul>

    <hr>

    <h2>Notable Applications of NAS</h2>
    <p>NAS has been instrumental in designing state-of-the-art models, including:</p>
    <ul>
        <li><strong>MobileNets</strong>: Searched over block types and network widths.</li>
        <li><strong>EfficientNet</strong>: Employed compound scaling for depth, width, and resolution.</li>
        <li><strong>EdgeTPU/NPU Models</strong>: Utilized latency-aware NAS to optimize for edge devices.</li>
    </ul>

    <hr>

    <h2>NAS for EfficientNet</h2>
    <p>EfficientNet's development leveraged a <strong>reinforcement learning-based NAS</strong>. The search focused on:</p>
    <ul>
        <li>Operator types (MBConv vs. regular convolution)</li>
        <li>Kernel sizes</li>
        <li>Expansion ratios for MobileNet-style inverted residual blocks</li>
        <li>Number of layers per stage</li>
        <li>Inclusion of Squeeze-and-Excitation (SE) blocks</li>
        <li>Stride and width per block</li>
    </ul>
    <p>The search target was a model optimized for both <strong>accuracy and latency on mobile hardware</strong>. Candidate models were trained on ImageNet, and latency was measured on actual devices rather than just relying on FLOP counts. An RL controller guided this search, leading to the <strong>EfficientNet-B0</strong> architecture—the foundational model of the EfficientNet family.</p>
    <p>After identifying B0, NAS was no longer used. Instead, a <strong>compound scaling method</strong> was introduced to systematically scale B0 to generate larger models (EfficientNet B1-B7). The compact search space and on-device latency measurement were key to EfficientNet's success, even compared to contemporary Vision Transformer models.</p>

    <hr>

    <h2>Reinforcement Learning (RL) for NAS</h2>
    <p>In supervised learning, models learn from input/output pairs using a loss function. In <strong>Reinforcement Learning (RL)</strong>, an <strong>agent</strong> interacts with an <strong>environment</strong> by taking <strong>actions</strong>. After each action, it receives a <strong>reward</strong>. The agent's objective is to learn a <strong>policy</strong>—a strategy for choosing actions—that maximizes the cumulative long-term reward.</p>

    <h3>REINFORCE Algorithm</h3>
    <p>A fundamental RL algorithm is <strong>REINFORCE</strong>. The core idea is to increase the probability of actions that lead to high rewards. The REINFORCE loss function is:</p>
    <p>$$ \text{Loss} = -\log(\text{probability of the action}) \times \text{reward} $$</p>
    <p>If the reward is high, minimizing this loss increases the log-probability of the action. This is a type of <strong>policy gradient</strong> method, where the controller's policy is directly adjusted to favor high-reward actions.</p>

    <h3>Mapping RL Concepts to NAS</h3>
    <table>
        <thead>
            <tr>
                <th>RL Concept</th>
                <th>NAS Version</th>
            </tr>
        </thead>
        <tbody>
            <tr>
                <td>Agent</td>
                <td>Controller (e.g., an RNN)</td>
            </tr>
            <tr>
                <td>Action</td>
                <td>Selecting an architectural choice (e.g., kernel size, layer type)</td>
            </tr>
            <tr>
                <td>Environment</td>
                <td>Training and evaluating the candidate neural network</td>
            </tr>
            <tr>
                <td>Reward</td>
                <td>Performance metric (e.g., accuracy, latency-adjusted accuracy)</td>
            </tr>
            <tr>
                <td>Policy</td>
                <td>How the controller samples actions (architectural choices)</td>
            </tr>
        </tbody>
    </table>
    <p>To enable the controller to learn over time, it's often implemented as an RNN. The RNN outputs logits for different architectural choices (actions). The REINFORCE algorithm (policy gradient) then updates the RNN's parameters based on the received reward.</p>

    <h3>Example: Using RL to Discover the Best Kernel Size</h3>
    <p>Let's consider finding the optimal kernel size for a convolutional block using RL:</p>
    <ol>
        <li><strong>Search Space / Action Space</strong>: Define possible kernel sizes, e.g., <code>[3, 5, 7]</code>.</li>
        <li><strong>Controller</strong>: Implement a controller (e.g., an RNN or even a random sampler initially) that selects a kernel size.</li>
        <li><strong>Evaluation Loop</strong>: For each sampled kernel size:
            <ul>
                <li>Build a small model using that kernel.</li>
                <li>Train it briefly or evaluate it using weight-sharing.</li>
                <li>Measure its performance (e.g., validation accuracy).</li>
                <li>Provide this performance measure as a reward to the controller.</li>
            </ul>
        </li>
    </ol>
    <p>If the controller outputs a softmax distribution over kernel sizes, and it samples <code>kernel = 3</code> with a probability of, say, <code>0.62</code>, and the resulting reward is <code>0.85</code>, the REINFORCE loss would be:</p>
    <p>$$ \text{loss} = -\log(0.62) \times 0.85 \approx -(-0.478) \times 0.85 \approx 0.406 $$</p>
    <p>Backpropagation through this loss adjusts the controller's logits to increase the probability of selecting <code>kernel = 3</code> if the reward is high. Conversely, if the reward is low, the probability of choosing that kernel decreases. Often, a <strong>baseline reward</strong> (e.g., the moving average of past rewards) is subtracted from the current reward. This encourages actions that perform better than average.</p>
    <p>The following Python code demonstrates a simplified version of this process. A controller samples kernel sizes, and it's trained using REINFORCE to maximize a simulated reward. We run 50 iterations of this sampling and learning process. The code will also generate a plot showing the controller's probability of choosing each kernel size over the training steps.</p>

    <pre><code class="language-python">
        
import torch
import torch.nn.functional as F
import matplotlib.pyplot as plt

""" 
    Toy architecture choices: kernel sizes 
    This is the policy that chooses which architecture to build. It's a probability distribution over actions. 
    In RL the controller is the agent. 
    The controller trying to learn a probability distribution over architectural choices. 
    This is action taken by the agent, sampling kernel sizes. You sample from the controller's distribution. 
    It starts with choosing each kernel equally at first. These logits will get transformed to probabilities. Then you sample from this categorical distribution 
    Then you measure the reward from taking this action. This is the environment returning a reward based on the action the agent took. 
    The REINFORCE algorithm updates these logits to prefer higher-reward options, it's updating the contoller. 
    And you repeat with the newly updated controller. Sampling, training, evaluating and improving the controller. 
"""
kernel_sizes = [3, 5, 7]
logits = torch.nn.Parameter(torch.zeros(len(kernel_sizes))) # internal parameters of the controller
optimizer = torch.optim.Adam([logits], lr=0.1)

# Simulated reward function (pretend model performance)
# Let's say kernel=5 is the best
""" 
# In practice, this is how you migth get the reward from accuracy.  
def get_reward(kernel):
    model = SimpleCNN(kernel_size=kernel)
    train_model(model, train_loader)
    accuracy = evaluate_model(model, val_loader)
    return accuracy
# Or maybe a multi-objvective: 
        reward = accuracy - 0.01 * latency - 0.005 * model_size
"""
def get_reward(kernel):
    true_perf = {3: 0.7, 5: 0.9, 7: 0.6} # we're assuming these are the accuracy from training the model
    noise = torch.randn(1).item() * 0.02 # small noise
    return true_perf[kernel] + noise

# Tracking
history = {
    "step": [],
    "chosen_kernel": [],
    "reward": [],
    "probs": []
}

for step in range(50):
    probs = F.softmax(logits, dim=0)
    dist = torch.distributions.Categorical(probs)
    action = dist.sample()
    kernel = kernel_sizes[action.item()]
    reward = get_reward(kernel)
    loss = -dist.log_prob(action) * reward # REINFORCE
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    # Log
    history["step"].append(step)
    history["chosen_kernel"].append(kernel)
    history["reward"].append(reward)
    history["probs"].append(probs.detach().numpy())

# Convert logged data for visualization
probs_over_time = list(zip(*history["probs"]))

# Plot probabilities over time
# The following code would generate a plot if run in a Python environment
with matplotlib.pyplot imported as plt.
plt.figure(figsize=(10, 6))
for i, k in enumerate(kernel_sizes):
     plt.plot(history["step"], probs_over_time[i], label=f"kernel={k}")
plt.title("Controller's Probability of Choosing Each Kernel Size")
plt.xlabel("Training Step")
plt.ylabel("Probability")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()
    </code></pre>

</body>
</html>
