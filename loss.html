<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Deep Learning Loss Functions: BCE, CE, and CTC</title>
    <style>
        body {
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif;
            line-height: 1.6;
            color: #333;
            max-width: 800px;
            margin: 0 auto;
            padding: 20px;
        }
        h1 { border-bottom: 2px solid #333; padding-bottom: 10px; }
        h2 { color: #2c3e50; margin-top: 40px; border-bottom: 1px solid #eee; }
        h3 { color: #555; margin-top: 30px; }
        code {
            background-color: #f4f4f4;
            padding: 2px 4px;
            border-radius: 4px;
            font-family: Consolas, Monaco, "Andale Mono", monospace;
            font-size: 0.9em;
        }
        pre {
            background-color: #f8f8f8;
            border: 1px solid #ddd;
            border-radius: 4px;
            padding: 15px;
            overflow-x: auto;
        }
        blockquote {
            border-left: 4px solid #ddd;
            padding-left: 15px;
            color: #666;
            margin: 20px 0;
        }
        .math-block {
            background: #fff;
            padding: 10px;
            text-align: center;
            font-family: "Times New Roman", serif;
            font-style: italic;
        }
    </style>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>

    <h1>Let’s Talk About Loss Functions</h1>
    <p>You know me, I’m gonna talk about loss functions now! Let’s start with binary cross entropy.</p>

    <h2>1. Binary Cross Entropy (BCE)</h2>
    <p>The formula for BCE is defined as:</p>
    
    <div class="math-block">
        $$ L = -\sum_i (y_i \log(p'_i) + (1-y_i)\log(1-p'_i)) $$
    </div>

    <p>Where:</p>
    <ul>
        <li>\( p'_i \) is the \( \text{sigmoid}(\text{logit}_i) \)</li>
        <li>\( y_i \) is whether a class was present (the ground truth).</li>
    </ul>

    <p>First, you squash logits into probabilities using a sigmoid function. Then, you compare that prediction with the ground truth using BCE loss. With BCE, each class is an independent binary prediction. You can even extend this to multiple labels.</p>

    <p>You can pass each logit through a sigmoid, compute binary cross entropy per class, and average over the classes. This lets you model multiple classes co-occurring. With BCE, you can’t compress the label into one integer anymore, so you need to have a multi-hot vector. You must determine the cross-entropy for each class and then sum over them.</p>
    
    <div class="math-block">
        $$ \text{BCE}(y, p) = -\sum_{i=1}^C (y_i \log(p_i) + (1-y_i) \log(1-p_i)) $$
    </div>

    <p>This checks every class independently. BCE needs a multi-hot vector because each class is treated as an independent yes/no problem.</p>

<pre><code class="language-python">def bce_with_logits_loss(logits, targets):
    """
    logits: (N, C) raw outputs
    targets: (N, C) multi-hot labels {0,1}
    """
    # sigmoid
    probs = 1 / (1 + torch.exp(-logits))

    # binary cross entropy per class
    bce = -(targets * torch.log(probs) + (1 - targets) * torch.log(1 - probs))

    # average over classes and batch
    loss = bce.mean()
    return loss
</code></pre>

    <h2>2. Categorical Cross Entropy (CE)</h2>
    <p>Categorical cross entropy is a special case of BCE with a one-hot target. Here, for classes with logits, probabilities \( p_i = \text{sigmoid}(z_i) \), and targets in 0 and 1, only one class is true at a time.</p>

    <p>With categorical cross entropy, you apply a softmax over logits to get a probability distribution over \( C \) classes.</p>

    <div class="math-block">
        $$ P_i = \frac{\exp(z_i)}{\sum_j \exp(z_j)} $$
    </div>

    <p>Then you apply categorical cross-entropy against a one-hot target. If the predicted probability for the correct class \( k \) is \( p \), then the CE is:</p>

    <div class="math-block">
        $$ CE = -\log P(\text{true\_class}) $$
    </div>

    <p>It only looks at the correct class’s probability. CE assumes mutual exclusivity—that exactly one class is correct. That’s why with categorical cross entropy, you can represent the target as a single index. Internally, PyTorch converts that into a one-hot vector (conceptually). The loss just turns into taking the negative log of the probability of the correct class.</p>

    <p>Remember, negative log means lower loss equals higher probability; it’s just like the negative log-likelihood of the true class under the predicted distribution. If the model was super sure (e.g., 0.9), then the negative log loss would be roughly 0.1. If it wasn’t sure (e.g., 0.1), the loss would be around 2.3. CE punishes the model when it assigns low probability to the true class.</p>

<pre><code class="language-python">def cross_entropy_loss(logits, target_index):
    """
    logits: (N, C) raw outputs
    target_index: (N,) integer labels (0 <= y < C)
    """
    # softmax
    exp_logits = torch.exp(logits)
    probs = exp_logits / exp_logits.sum(dim=1, keepdim=True)

    # pick probability of the correct class
    correct_class_probs = probs[torch.arange(len(target_index)), target_index]

    # negative log likelihood
    loss = -torch.log(correct_class_probs).mean()
    return loss
</code></pre>

    <h2>3. The Alignment Problem</h2>
    <p>If we had a sequence of input frames and we knew the label for each frame (which we don't here), then we could just apply CE at each frame and average.</p>
    
    <div class="math-block">
        $$ \text{Loss} = -\frac{1}{T} \sum_{t=1}^T \log(p_{(t, y_t)}) $$
    </div>

    <p>Where \( y_t \) is the ground truth label for frame \( t \). But this assumes a frame-to-label alignment. We don’t have labels for every frame, and the target tokens are much shorter than the input. A speech sequence length might be 1000 frames (10 seconds), which is much longer than the text tokens (maybe 20 words), and we don’t know the frame-to-token alignment.</p>

    <p>We need a loss that doesn’t require explicit alignment and can collapse variable-length frame predictions into a shorter token length. This is where <strong>Connectionist Temporal Classification (CTC)</strong> comes into play.</p>

    <h2>4. Connectionist Temporal Classification (CTC)</h2>
    <p>CTC augments the vocabulary with a <strong>blank token</strong> (no label). This allows frames to correspond to no output symbols. So, now you have \( T \) frames, and each frame gets a probability distribution over \( V+1 \) (tokens + blank).</p>

    <h3>Merging Predictions</h3>
    <p>To get final transcriptions from a frame sequence, you have to merge their predictions. You merge repeated tokens and remove blanks. CTC loss uses dynamic programming to sum the probabilities of all possible alignments between frames and target tokens, and it does this efficiently.</p>

    <h3>Valid Alignments</h3>
    <p>Each frame has a probability mass over many tokens and thus can create multiple alignments. We sum over all valid alignments that lead to the ground truth. However, this creates a massive number of alignments. Over \( T \) frames with \( |V| + 1 \) possible paths, you get \( (|V| + 1)^T \) paths. Even with a small vocabulary of 50, you could get \( 50^{100} \) paths.</p>

    <p>With CTC, the model doesn’t output one label; it outputs a probability distribution over all possible labels at each frame. Alignments are not generated arbitrarily from predictions; they are constrained by the ground truth. An alignment is one possible frame-level sequence that, when collapsed, equals the ground truth.</p>

    <p>The predictions supply the probability at each frame, and the ground truth constrains which sequence of choices we care about. We compute the total probability of the ground truth string by summing over all valid alignment paths.</p>

    <div class="math-block">
        $$ P(y | x ) = \sum_{\pi \in B^{-1}(y)} \prod_{t=1}^T p(\pi_t | x_t) $$
    </div>

    <p>Where:</p>
    <ul>
        <li>\( Y \): Ground truth sequence</li>
        <li>\( X \): Input sequence</li>
        <li>\( p(\pi_t | x_t) \): Model’s softmax probability of symbol \( \pi_t \) at frame \( t \)</li>
        <li>\( \pi = (\pi_1, \dots, \pi_T) \): One alignment path</li>
        <li>\( B(\pi) \): Collapse function (removes blanks and merges repeats)</li>
        <li>\( B^{-1}(y) \): Set of all alignments that collapse to the same ground truth</li>
    </ul>

    <p>Since multiple alignments can collapse to the same label sequence \( y \), the total probability of \( y \) is the sum of all alignment probabilities. The CTC loss is then just negative log-likelihood:</p>

    <div class="math-block">
        $$ L_{ctc} = -\log P(y|x) $$
    </div>

    <p>That’s a lot of math you probably don’t need, but the big idea here is that we need to find the alignments that collapse to the target word and use our model’s probability predictions to score each valid alignment. CTC uses a forward-backward dynamic programming algorithm to efficiently sum these probabilities.</p>

    <h3>The DP Algorithm (Counting Paths)</h3>
    <p>At each input, you can output either a symbol or a blank token. A valid alignment is any sequence that collapses to the ground truth after removing blanks and merging consecutive duplicates. You can think of this as a "LeetCode hard" problem: counting paths. We need to count how many paths collapse to the target sequence, and once we have that, we can replace those counts with probabilities.</p>

    <p>A naive CTC would be a softmax with repeat collapse. You consider the single argmax alignment and collapse repeats accordingly. It’s simple and fast, but it doesn’t work! It assumes each character or phoneme is well-aligned in time or clearly segmented. Misalignment and variable timing will cause errors.</p>

    <h4>Example:</h4>
    <p>Let’s say we have 10 frames and take a small example like <strong>CAT</strong>. Our vocabulary is {C, A, T, blank}. Valid alignments include:</p>
    <ul>
        <li>{C, A, T, -}</li>
        <li>{C, A, -, T}</li>
        <li>{C, -, A, T}</li>
        <li>{-, C, A, T}</li>
        <li>{C, C, A, T}</li>
        <li>{C, A, A, T}</li>
        <li>{C, A, T, T}</li>
    </ul>

    <p>CTC builds an extended target sequence with blanks inserted:</p>
    <p><strong>Y’ = _ C _ A _ T _</strong></p>
    <p>The length is \( 2U + 1 \). Now let’s define a DP table. Our original target of length 3 has been extended to 7. Blanks are guard rails; they allow for variable timing and disambiguating repeats. Now think of aligning the 10 frames to the 7 positions in the extended sequence.</p>
    
    <p>At each step you can:</p>
    <ol>
        <li>Stay at the same position.</li>
        <li>Move forward by 1 (always).</li>
        <li>Move forward by 2 (only if skipping a blank between different symbols).</li>
    </ol>

    <p>This is like a lattice. Each node is \( (t, s) \) at time \( t \) and target position \( s \). The table looks like 10 frames/rows by 7 alignments/columns (\( T \times 2U+1 \)). From \( (t, s) \), the DP calculates the probability of being aligned to position \( s \) in the extended target.</p>

    <h3>CTC Forward Implementation</h3>
    <p>The transition rule: To compute \( dp[t][s] \), you could get a contribution from the same symbol \( (t-1, s) \), advance by 1 \( (t-1, s-1) \), or skip over the blank token if valid \( (t-1, s-2) \).</p>

<pre><code class="language-python">import numpy as np

def ctc_forward(probs, target):
    """
    probs: array (T, V) with softmax probs for each frame
           V includes all symbols + blank
    target: list of symbols, e.g. ['C','A','T']
    """
    # 1. Build extended target
    extended = ['_']
    for ch in target:
        extended += [ch, '_']
    S = len(extended)

    T = probs.shape[0]
    alpha = np.zeros((T, S))

    # 2. Initialize first frame
    alpha[0][0] = probs[0][extended[0]]   # prob of blank at t=0
    alpha[0][1] = probs[0][extended[1]]   # prob of 'C' at t=0

    # 3. Fill DP table
    for t in range(1, T):
        for s in range(S):
            stay = alpha[t-1][s]
            prev = alpha[t-1][s-1] if s-1 >= 0 else 0
            skip = alpha[t-1][s-2] if (s-2 >= 0 and extended[s] != extended[s-2]) else 0
            alpha[t][s] = probs[t][extended[s]] * (stay + prev + skip)

    # 4. Final probability
    return alpha[T-1][S-1] + alpha[T-1][S-2]
</code></pre>

    <p>This is our \( p(y|x) \); we just efficiently computed the sum. The loss function would then be \( -\log(P(y|x)) \).</p>

    <h2>5. Training and Gradients (Forward-Backward)</h2>
    <p>For training, we need gradients w.r.t. the model’s softmax outputs at each frame. We need to know: at frame 1, how much did it contribute to predicting symbols of the target? This is the posterior probability.</p>

    <p><strong>Forward</strong> gives you the <em>prefix</em> probability (probability mass up to \( (t,s) \)).<br>
    <strong>Backward</strong> gives you the <em>suffix</em> probability (probability mass from \( (t,s) \) to the end).</p>

    <p>In plain words: Forward only says "how likely is it that we got here," and Backward says "how likely is it that we can finish from here." When you multiply forward and backward (normalized by total sequence probability), you get the posterior: <strong>How likely is it that frame \( t \) really belongs to target position \( s \)?</strong></p>

    <p>Let’s look at this with a different lens until it sinks in. Loss is \( -\log(P(y|x)) \). The gradient is the derivative of the loss w.r.t. each logit at each frame. We need to ask: for frame \( t \) and symbol \( k \), how much should we increase or decrease its probability?</p>

<pre><code class="language-python">import numpy as np

def ctc_forward_backward(probs, target):
    """
    probs: np.array of shape (T, V) with softmax probabilities
           (V includes all symbols + blank, accessed by dict)
    target: list of symbols, e.g. ['C','A','T']
    """
    # Build extended target with blanks
    extended = ['_']
    for ch in target:
        extended += [ch, '_']
    S = len(extended)   # extended length
    T = probs.shape[0]  # number of frames

    # Map symbol -> index in probs
    vocab = {ch: i for i, ch in enumerate(set(extended))}

    # Forward DP
    alpha = np.zeros((T, S))
    alpha[0][0] = probs[0, vocab[extended[0]]]
    alpha[0][1] = probs[0, vocab[extended[1]]]

    for t in range(1, T):
        for s in range(S):
            stay = alpha[t-1][s]
            prev = alpha[t-1][s-1] if s-1 >= 0 else 0
            skip = alpha[t-1][s-2] if (s-2 >= 0 and extended[s] != extended[s-2]) else 0
            alpha[t][s] = probs[t, vocab[extended[s]]] * (stay + prev + skip)

    # Backward DP
    beta = np.zeros((T, S))
    # initialize last frame
    beta[T-1][S-1] = probs[T-1, vocab[extended[S-1]]]   # last blank
    beta[T-1][S-2] = probs[T-1, vocab[extended[S-2]]]   # last symbol

    for t in reversed(range(T-1)):  # from T-2 down to 0
        for s in range(S):
            same = beta[t+1][s]
            nxt = beta[t+1][s+1] if s+1 < S else 0
            skip = beta[t+1][s+2] if (s+2 < S and extended[s] != extended[s+2]) else 0
            beta[t][s] = probs[t, vocab[extended[s]]] * (same + nxt + skip)

    # Total probability (should match forward and backward)
    total_prob = alpha[T-1][S-1] + alpha[T-1][S-2]
    
    return alpha, beta, total_prob, extended
</code></pre>

    <h2>6. Inference: Beam Search</h2>
    <p>At inference, we need to figure out which output sequence to return. If the model is confident and the output space is small, greedy decoding (picking the argmax at each frame) is okay. But that throws away a lot of probability mass.</p>

    <p><strong>Beam search</strong> recovers some of it by merging multiple paths that collapse to the same candidate sequence. Since we don’t know the target sequence, we just pick the sequence that maximizes \( p(y|x) \).</p>

    <p>At each frame, you pick the top-k candidates and merge paths that collapse to the same output string by summing their probabilities. At the end, you return the candidate with the highest total probability.</p>

<pre><code class="language-python">def simple_ctc_beam_search(probs, beam_size=3, top_k=3):
    """
    probs: (T, V) softmax probabilities over vocab (incl blank "_")
    """
    beams = {"": 1.0}  # start with empty prefix and prob=1

    for t in range(len(probs)):
        new_beams = {}
        # pick top_k symbols at this frame
        top_syms = np.argsort(probs[t])[-top_k:]
        for prefix, prob in beams.items():
            for sym in top_syms:
                p = probs[t][sym]
                if sym == "_":
                    new_prefix = prefix
                else:
                    # collapse rule: if repeat, don't double add
                    if prefix.endswith(sym):
                        new_prefix = prefix  # same seq
                    else:
                        new_prefix = prefix + sym
                # accumulate probability
                new_beams[new_prefix] = new_beams.get(new_prefix, 0) + prob * p
        # prune to top beam_size
        beams = dict(sorted(new_beams.items(), key=lambda x: -x[1])[:beam_size])

    return max(beams.items(), key=lambda x: x[1])
</code></pre>

</body>
</html>
