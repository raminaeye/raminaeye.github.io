<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>A Deep Dive into Convolutional Neural Networks</title>
  <style>
    body {
      font-family: sans-serif;
      max-width: 900px;
      margin: 0 auto;
      padding: 2rem;
      line-height: 1.6;
      color: #333;
    }
    h1, h2, h3, h4 {
      color: #2c3e50; /* Darker shade for better contrast */
      margin-top: 1.5em;
      margin-bottom: 0.5em;
    }
    h1 { font-size: 2.5em; border-bottom: 2px solid #3498db; padding-bottom: 0.3em;}
    h2 { font-size: 2em; border-bottom: 1px solid #bdc3c7; padding-bottom: 0.2em;}
    h3 { font-size: 1.5em; }
    h4 { font-size: 1.2em; color: #555;}

    nav { margin-bottom: 30px; padding: 10px; background: #ecf0f1; border: 1px solid #bdc3c7; border-radius: 4px;}
    nav ul { list-style: none; padding: 0; }
    nav li { display: inline-block; margin-right: 15px; }
    nav a { text-decoration: none; color: #3498db; font-weight: bold;}
    nav a:hover { text-decoration: underline; color: #2980b9;}

    pre {
      background: #f8f9f9; /* Lighter background for code blocks */
      padding: 1rem;
      overflow-x: auto;
      border: 1px solid #e1e4e8; /* Softer border */
      border-left: 4px solid #3498db; /* Accent border */
      border-radius: 4px;
      font-size: 0.9em;
    }
    code {
      font-family: 'SFMono-Regular', Consolas, 'Liberation Mono', Menlo, Courier, monospace;
    }
    /* For inline code */
    p > code, li > code, table td > code {
      background: #e8eaed;
      padding: 0.2em 0.4em;
      border-radius: 3px;
      font-size: 0.85em;
    }
    pre code { /* Reset for code inside pre, already handled by pre styling */
        background: none;
        padding: 0;
        font-size: 1em; /* Ensure pre's font size is inherited */
    }
    ul {
        padding-left: 20px;
    }
    li {
        margin-bottom: 0.5em;
    }
    strong {
        color: #2980b9;
    }
    hr {
      border: 0;
      height: 1px;
      background: #bdc3c7;
      margin-top: 2em;
      margin-bottom: 2em;
    }
  </style>
 
  <script type="text/javascript" async
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
  </script>
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [['$','$'], ['\\(','\\)']],
        displayMath: [['$$','$$'], ['\\[','\\]']],
        processEscapes: true
      }
    });
  </script>

  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
  <script>hljs.highlightAll();</script>
</head>
<body>
    <nav>
        <ul>
            <li><a href="index.html">Home</a></li> 
            <li><a href="#intro">Introduction</a></li>
            <li><a href="#core-concepts">Core Concepts</a></li>
            <li><a href="#receptive-field">Receptive Field</a></li>
            <li><a href="#advanced-techniques">Advanced Techniques</a></li>
            <li><a href="#specialized-convs">Specialized Convolutions</a></li>
            <li><a href="#temporal-modeling">Temporal Modeling (TCNs)</a></li>
            <li><a href="#architectures">Key Architectures</a></li>
        </ul>
    </nav>

    <h1 id="intro">Introduction to Convolutional Neural Networks</h1>
    <p>
        Let’s learn about convolutional neural networks. I’m assuming you’re mostly familiar with what CNNs are, and this tutorial is meant to refresh your memory on concepts you might have missed out on or forgotten.
    </p>

    <hr>

    <h2 id="core-concepts">Core Concepts</h2>
    
    <h3>Kernels and Filters</h3>
    <p>
        In a CNN, <strong>kernels</strong> (or <strong>filters</strong>) are small weight matrices that slide over the input. They compute a dot product between the kernel's weights and the local input patches they cover. They eventually learn to extract things like edges, textures, etc. The same kernel is applied to all spatial locations (this is called <strong>shared weights</strong>), which makes CNNs more parameter-efficient than feedforward networks. The larger the kernel, the more expensive it is computationally. You can apply multiple kernels per layer to learn multiple features and channels.
    </p>

    <h3>Stride</h3>
    <p>
        <strong>Stride</strong> controls how far the kernel moves at each step. A larger stride results in fewer steps and acts as a form of downsampling.
    </p>

    <h3>Padding</h3>
    <p>
        <strong>Padding</strong> inserts zeros around the input to preserve its size. Common types are:
    </p>
    <ul>
        <li><strong>Valid</strong>: No padding.</li>
        <li><strong>Constant</strong>: Adding a fixed number of zeros around the input.</li>
        <li><strong>Same</strong>: Adding enough padding to keep the output size the same as the input size.</li>
    </ul>
    <p>The formula for the output size is:</p>
    $$ \text{Output Size} = \bigg\lfloor \frac{(\text{Input Size} - \text{Kernel Size} + 2 \times \text{Padding})}{\text{Stride}} \bigg\rfloor + 1 $$

    <hr>

    <h2 id="receptive-field">Receptive Field</h2>
    <p>
        The <strong>receptive field</strong> of a neuron is the region of the input image that can influence that neuron’s value. How much of the original input does this neuron get to see? It determines how much context the model has. A small receptive field sees local details, like edges, while a large receptive field sees global structures, like shapes and objects.
    </p>
    <p>
        With <strong>stride</strong>, you increase the spacing between the receptive fields of adjacent neurons, not the size of the receptive field itself; each step just jumps farther. Stride controls the resolution and indirectly spreads the influence of each neuron over a wider input area.
    </p>
    <p>
        <strong>Padding</strong> lets the kernel reach the edge of the input. It doesn’t directly impact the receptive field of a single layer. However, without padding the input shrinks. Padding preserves the size, so when more layers are stacked, the receptive field grows. It allows for deeper models without the input shrinking too fast.
    </p>

    <h3>Calculating the Receptive Field</h3>
    <p>Here’s how you can calculate the receptive field. Let’s define for layer <code>i</code>:</p>
    <ul>
        <li><code>RF[i]</code>: Receptive field size at layer <code>i</code>.</li>
        <li><code>K[i]</code>: Kernel size at layer <code>i</code>.</li>
        <li><code>S[i]</code>: Stride at layer <code>i</code>.</li>
        <li><code>J[i]</code>: Jump (the step size in terms of input pixels).</li>
    </ul>
    <p>
        For the input layer, <code>RF[0]=1</code> (each pixel sees itself) and <code>J[0]=1</code> (the input step size). Think of the <strong>jump</strong> as how many input pixels separate two adjacent outputs in the current layer. When a layer aggregates across <code>k</code> inputs, and each of those inputs is already spread apart, the receptive field grows faster than just <code>k</code>.
    </p>
    <p>At each layer <code>i</code>, we update the receptive field and jump as follows:</p>
    $$ \text{RF}[i] = \text{RF}[i-1] + (K[i]-1) \times J[i-1] $$
    $$ J[i] = J[i-1] \times S[i] $$
    <p>The term <code>K[i]-1</code> is how many new "jumps" you’re adding, and <code>J[i-1]</code> is how far apart those inputs are in the input space.</p>
    
    <h4>Example:</h4>
    <p>Assume for a layer:</p>
    <ul>
        <li><code>RF[1] = 3</code> (from a previous convolution)</li>
        <li><code>J[1] = 2</code> (Layer 1 outputs are 2 input pixels apart)</li>
        <li><code>K = 3</code> (the next layer's kernel)</li>
    </ul>
    <p>Then the receptive field for the next layer is:</p>
    $$ \text{RF}[2] = 3 + (3-1) \times 2 = 7 $$
    <p>
        You're adding 2 jumps of 2 pixels each because the kernel sees 3 inputs. The jump tells us how stretched the receptive field becomes. The more stride you use, the more your output is coarsely spread across the input. So, when a new kernel sees <code>k</code> inputs, the total receptive field includes the jumps between those inputs.
    </p>
    <p>
        In vanilla CNNs with small kernels, the receptive field grows linearly, and you might need dozens of layers to reach a large enough receptive field. Even if theory says the receptive field is large, in practice, most of the signal comes from a much smaller region. This is called the <strong>effective receptive field</strong>, and it’s usually smaller and Gaussian-shaped. There are a number of techniques we can apply to make sure the receptive field grows fast, which will be covered over the course of this tutorial.
    </p>

    <hr>
    
    <h2 id="advanced-techniques">Advanced Techniques &amp; Implementations</h2>

    <h3>Convolution in Practice</h3>
    <p>Let’s take a look at how kernels are applied in a convolution layer using NumPy. We’re sliding the kernel over the input, where <code>i</code> goes down the rows and <code>j</code> goes across the columns.</p>
    <pre><code class="language-python">
import numpy as np

def conv2d(input, kernel, stride=1, padding=0):
    H_in, W_in = input.shape
    K = kernel.shape[0]  # Assume square kernel for simplicity

    # Pad input
    if padding > 0:
        input = np.pad(input, ((padding, padding), (padding, padding)), mode='constant')

    H_out = (input.shape[0] - K) // stride + 1
    W_out = (input.shape[1] - K) // stride + 1

    output = np.zeros((H_out, W_out))

    for i in range(H_out):  # Move down the rows
        for j in range(W_out):  # Move across the columns
            i_start = i * stride
            j_start = j * stride
            patch = input[i_start:i_start + K, j_start:j_start + K]
            output[i, j] = np.sum(patch * kernel)

    return output
    </code></pre>
    <p>
        Now, consider this example: let’s say you want to get a receptive field of 5. You can get that with one <code>5x5</code> kernel layer. Or, you can get it with two <code>3x3</code> kernels with a stride and padding of 1. The two <code>3x3</code> kernels would actually have fewer parameters and can add more non-linearity, increasing expressiveness. For shallow models, a <code>5x5</code> might benefit from wide kernels, but even then, a dilated <code>3x3</code> can do the same task.
    </p>
    
    <h3>Dilation</h3>
    <p>
        <strong>Dilation</strong> means inserting spaces between kernel elements when sliding over the input, so the kernel reaches further without increasing its size or parameter counts. Dilation is like using a "zoomed-out" kernel. A regular <code>3x3</code> kernel covers a <code>3x3</code> patch. If the dilation is 2, we’re inserting 1 zero between each kernel element, so now it covers a <code>5x5</code> region even though the kernel is still <code>3x3</code>.
    </p>
    <p>The effective kernel size is:</p>
    $$ \text{Effective Kernel Size} = K + (K - 1) \times (\text{Dilation} - 1) $$
    <p>
        This method skips over input positions, increasing the receptive field without adding parameters. The receptive field with dilation (<code>D[i]</code>) is calculated as:
    </p>
    $$ \text{RF}[i] = \text{RF}[i-1] + (K[i]-1) \times J[i-1] \times D[i] $$
    <p>
        So, if dilation seems great, why not use it all the time? There’s a limit to how far you can push it. This is called the <strong>gridding effect</strong> or <strong>blind spots</strong>. A large dilation causes the kernel to skip over much of the input. Some input positions are never touched by the neuron. The same is true for stride. Stride widens the spacing between sampled input positions, especially when combined with dilation, and can cause blind spots. It’s important to use a stride of 1 in early layers to avoid this and periodically reset to a standard convolution to restore coverage.
    </p>
    <p>An example of effective kernel coverage with a dilation of 4:</p>
    <pre><code>[ x &nbsp; 0 &nbsp; 0 &nbsp; 0 &nbsp; x &nbsp; 0 &nbsp; 0 &nbsp; 0 &nbsp; x ]</code></pre>
    <p>
        We’re only looking at inputs 0, 4, and 8. If you stack multiple layers with the same dilation, each new layer sees a sparse subset of the previous sparse subset. So, if layer 1 sees inputs 0, 4, 8, 12, then layer 2 sees the same spots at multiples of 4, and its receptive field might cover 0, 16, 32, completely missing inputs like 1, 2, 3... They will never influence the output. Hence, you should always <strong>vary dilation across layers</strong>.
    </p>
    <p>
        Use smaller dilation in early layers so it densely covers the input. Mix with non-dilated convolutions and use parallel branches with different dilation rates, such as in <strong>Atrous Spatial Pyramid Pooling (ASPP)</strong>. ASPP is a CNN module designed to capture multi-scale context using parallel dilated convolutions. Small dilation captures fine and local detail, while larger dilation captures global and semantic context. Together, they create richer features without increasing depth.
    </p>

    <h3>Pooling and Downsampling</h3>
    <p>
        <strong>Pooling</strong> is a form of downsampling achieved by sliding a window over the input and taking a statistic of that window. It doesn’t learn; it’s hard-coded.
    </p>
    <ul>
        <li><strong>Max-Pooling</strong>: Keeps only the maximum value in the window. It is translationally invariant, keeps strong activations, ignores noise, and highlights dominant signals. However, max-pooling can lose spatial detail.</li>
        <li><strong>Average Pooling</strong>: Takes the average of the values. It smooths features, reduces noise, preserves low-frequency info, and keeps more information from the input. For audio and time-series signals where signal energy is more meaningful, average pooling may be more useful.</li>
        <li><strong>Global Pooling</strong>: Reduces the entire spatial or temporal dimension to one value per channel. You get one average or max per channel. Global pooling removes dimensions, reduces parameters, can replace fully-connected layers, and helps reduce overfitting.</li>
        <li><strong>Adaptive Pooling</strong>: Adjusts the window size and stride automatically so that the output has a target shape, no matter the input size.</li>
    </ul>
    <p>
        A regular convolution with a <strong>stride > 1</strong> also causes downsampling, just like pooling. It’s a learnable kernel versus the hardcoded pooling, although it is not as translation-invariant. Strided convolution may lose spatial sharpness and can cause <strong>aliasing</strong> if not tuned carefully. Aliasing happens when you downsample a signal without removing high-frequency components first, causing those components to be misrepresented as incorrect lower-frequency artifacts.
    </p>
    <p>
        There are anti-aliasing kernels like <strong>BlurPool</strong> that apply a fixed blur kernel before downsampling. This gives the convolution a chance to extract features before reducing resolution and also helps the layer be more <strong>shift-invariant</strong>. A model is shift or translation invariant if shifting the input slightly does not change the output much. CNNs can lose this property due to operations like strided convolution, max-pooling, and zero-padding, which adds artificial borders and causes asymmetry. Sometimes using <code>reflect</code> or <code>replicate</code> padding is better than zero-padding.
    </p>

    <hr>
    
    <h2 id="specialized-convs">Specialized Convolution Types</h2>
    
    <h3>1x1 Convolution</h3>
    <p>
        A <strong>1x1 convolution</strong> is similar to a linear layer. It applies a single learned weight to each spatial position but across all input channels. It sees one pixel location at a time across all <code>C_in</code> input channels and applies <code>C_out</code> different linear combinations (dot products) to the vector of channels. It’s like applying a linear layer with a weight shape of <code>[C_out, C_in]</code> to each pixel independently.
    </p>
    <p>
        A 1x1 convolution preserves the spatial grid, as weights are shared across all locations. It mixes channel features at each pixel, whereas a linear layer would collapse the feature maps to 1D. Neither expands the receptive field. A 1x1 convolution can often be more efficient via convolution engines than matrix multiplication over flattened inputs.
    </p>
    <p>
        Let’s talk about this weight-sharing impact. For each pixel (h,w), the input is <code>x[:, h, w] = [R, G, B]</code>. Then a 1x1 convolution output is:
    </p>
    $$ \text{output}[:, h, w] = W \cdot x[:, h, w] $$
    <p>It applies the same weight matrix <code>W</code> to each pixel's <code>[R, G, B]</code> vector to create new channels:</p>
    $$ \text{out}_1(h,w) = w_{11}R + w_{12}G + w_{13}B $$
    $$ \text{out}_2(h,w) = w_{21}R + w_{22}G + w_{23}B $$
    <p>
        It mixes those three channels linearly using the same weights at every location. It’s not a convolution in the spatial sense; it’s applying a linear layer to the channel vector for each pixel, which is why it’s called a <strong>pointwise operation</strong>. In contrast, a linear layer flattens the input (e.g., to <code>[1, 3 x H x W]</code>) and applies a large weight matrix to get the output channels. It mixes all pixels and all channels together into a single vector. This is typically okay for the final classification layer, but for tasks requiring spatial awareness, pointwise convolution is preferred.
    </p>
    <p>
        In summary, a 1x1 convolution performs channel mixing at each spatial location, reduces or expands the dimensionality of channels without touching <code>H x W</code>, and acts as a learned, spatially-aware MLP over the channels.
    </p>

    <h3>Depthwise Separable Convolution</h3>
    <p>
        A regular convolution mixes spatial information across channels. Each output channel is a weighted sum of all input channels. <strong>Depthwise convolution</strong> decouples this: it applies one filter per input channel independently, with no channel mixing.
    </p>
    <p>
        In PyTorch, if you set <code>groups = in_channels</code>, you get one filter per channel. Each channel gets its own kernel, and there is no mixing.
    </p>
    <pre><code class="language-python">
def depthwise_conv2d(x, kernels, padding=1):
    """
    x: shape [C, H, W]
    kernels: shape [C, kH, kW]
    returns: shape [C, H, W]
    """
    C, H, W = x.shape
    kH, kW = kernels.shape[1:]
    padded = np.pad(x, ((0, 0), (padding, padding), (padding, padding)))
    out = np.zeros_like(x)

    for c in range(C):
        for i in range(H):
            for j in range(W):
                patch = padded[c, i:i + kH, j:j + kW]
                out[c, i, j] = np.sum(patch * kernels[c])

    return out
    </code></pre>
    <p>
        If depthwise convolution doesn’t do channel mixing, what does? A pointwise convolution makes a great pair. And that’s called <strong>depthwise separable convolution</strong>. The depthwise convolution outputs <code>[B, C_in, H, W]</code>, and the pointwise convolution takes that and applies a <code>1x1</code> kernel across all channels to produce <code>[B, C_out, H, W]</code>.
    </p>
    <p>The total number of parameters for <code>C_in=32</code>, <code>C_out=64</code>, and <code>kernel_size=3x3</code>:</p>
    <ul>
        <li><strong>Standard Convolution</strong>: <code>32 × 64 × 3 × 3 = 18,432</code></li>
        <li><strong>Depthwise Separable</strong>: <code>(32 × 3 × 3) + (32 × 64 × 1 × 1) = 288 + 2,048 = 2,336</code> (about 9x fewer parameters!)</li>
    </ul>
    <pre><code class="language-python">
import torch.nn as nn

class SeparableConv2D(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size, padding):
        super().__init__()
        self.depthwise = nn.Conv2d(in_channels, in_channels, kernel_size,
                                   padding=padding, groups=in_channels, bias=False)
        self.pointwise = nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False)

    def forward(self, x):
        x = self.depthwise(x)
        x = self.pointwise(x)
        return x
    </code></pre>

    <h3>Batch Normalization</h3>
    <p>
        We haven’t talked about a really important block that convolution layers tend to employ: <strong>batch normalization</strong>. BatchNorm normalizes the activation for each feature/channel across a batch, keeping the distribution stable during training. This typically helps with speeding up convergence, training deeper networks, and allowing higher learning rates. It computes one mean and variance over the batch and spatial dimensions for each channel. It then scales and shifts the normalized activations with learnable parameters.
    </p>
    <pre><code class="language-python">
class BatchNorm2d:
    def __init__(self, num_channels, eps=1e-5, momentum=0.1):
        self.eps = eps
        self.momentum = momentum
        self.num_channels = num_channels
        self.gamma = np.ones((num_channels,), dtype=np.float32) # Learnable scale
        self.beta = np.zeros((num_channels,), dtype=np.float32)  # Learnable shift
        self.running_mean = np.zeros((num_channels,), dtype=np.float32) # For inference
        self.running_var = np.ones((num_channels,), dtype=np.float32)  # For inference
        self.training = True

    def __call__(self, x):
        return self.forward(x)

    def forward(self, x):
        # x shape: [B, C, H, W]
        B, C, H, W = x.shape

        if self.training:
            # Compute mean and var over (B, H, W) for each channel
            x_reshaped = x.transpose(1, 0, 2, 3).reshape(C, -1)  # [C, B*H*W]
            mean = x_reshaped.mean(axis=1)
            var = x_reshaped.var(axis=1)

            # Update running stats
            self.running_mean = (self.momentum * mean + (1 - self.momentum) * self.running_mean)
            self.running_var = (self.momentum * var + (1 - self.momentum) * self.running_var)
        else:
            # Use running stats in eval mode
            mean = self.running_mean
            var = self.running_var

        # Normalize
        x_norm = (x - mean[None, :, None, None]) / np.sqrt(var[None, :, None, None] + self.eps)
        out = self.gamma[None, :, None, None] * x_norm + self.beta[None, :, None, None]
        return out
    </code></pre>
    <p>
        CNNs pass activations through many layers. Without normalization, the distribution of activations drifts (<strong>internal covariate shift</strong>), and later layers get unstable input. BatchNorm keeps the activations centered and scaled, making training smoother and more predictable. A typical convolution block has BatchNorm right after the convolution layer and before the activation function.
    </p>
    <pre><code class="language-python">
self.block = nn.Sequential(
    nn.Conv2d(in_ch, out_ch, kernel_size=kernel_size, padding=padding, bias=False),
    nn.BatchNorm2d(out_ch),
    nn.ReLU(inplace=True),
    nn.Dropout2d(p=dropout_prob)
)
    </code></pre>
    
    <hr>

    <h2 id="temporal-modeling">Temporal Modeling (TCNs)</h2>
    <p>
        Now let’s talk about applying CNNs to temporal modeling. You can use <strong>Conv1D</strong> over time to model sequential dependencies. By applying a filter over the time dimension, it learns temporal patterns like motion, trends, and signal rhythms. Each output time step sees a window of size <code>K</code> from the past or future.
    </p>
    <p>
        In a <strong>causal task</strong>, you can’t use future inputs to predict the current output. A standard Conv1D is non-causal. To fix this, you must control padding carefully. For a kernel of size 3 where you want the output at time <code>t</code> to depend on <code>[t-2, t-1, t]</code>, you pad only on the left by <code>kernel_size - 1</code>.
    </p>
    <pre><code class="language-python">
import torch.nn.functional as F

class CausalConv1d(nn.Module):
    def __init__(self, in_ch, out_ch, kernel_size, dilation=1):
        super().__init__()
        self.pad = (kernel_size - 1) * dilation
        self.conv = nn.Conv1d(
            in_ch, out_ch, kernel_size=kernel_size,
            padding=0, dilation=dilation
        )

    def forward(self, x):
        # x: [B, C, T]
        x = F.pad(x, (self.pad, 0))  # Pad left only
        return self.conv(x)
    </code></pre>
    <p>The next step is to use <strong>dilation</strong> to allow for exponentially larger receptive fields with fewer layers, letting you capture long-term dependencies. With <code>L</code> layers, kernel size <code>k</code>, and dilation doubling at each layer, the receptive field becomes:</p>
    $$ \text{Receptive Field} = 2^L \times (k-1) + 1 $$
    <p>If dilation increases too fast, some intermediate time steps are skipped, and the TCN may miss finer-grained dependencies. Stacking blocks with repeated dilation cycles can help.</p>
    <p>TCNs also need <strong>residual connections</strong> to work reliably. With a deep stack of dilated convolutions, gradients can vanish. With a residual connection, the model learns modifications on top of the input. It’s a shortcut that lets the input skip one or more layers.</p>
    <pre><code class="language-python">
class TCNBlock(nn.Module):
    def __init__(self, in_ch, out_ch, kernel_size, dilation, dropout=0.2):
        super().__init__()
        self.conv1 = CausalConv1d(in_ch, out_ch, kernel_size, dilation)
        self.relu1 = nn.ReLU()
        self.drop1 = nn.Dropout(dropout)
        self.conv2 = CausalConv1d(out_ch, out_ch, kernel_size, dilation)
        self.relu2 = nn.ReLU()
        self.drop2 = nn.Dropout(dropout)
        # 1x1 conv to match dimensions if needed
        self.downsample = nn.Conv1d(in_ch, out_ch, kernel_size=1) if in_ch != out_ch else nn.Identity()

    def forward(self, x):
        out = self.conv1(x)
        out = self.relu1(out)
        out = self.drop1(out)
        out = self.conv2(out)
        out = self.relu2(out)
        out = self.drop2(out)
        res = self.downsample(x)
        return out + res
    </code></pre>
    <p>
        For multi-sensor data (like EEG), you can first apply a <strong>Conv2D</strong> to extract temporal patterns shared across all channels. This is done by treating the channels <code>C</code> as height, so you get an input of <code>x: [B, 1, C, T]</code>. A kernel of size <code>(C, k)</code> covers all channels and a temporal window.
    </p>
    <p>
        A <strong>2D TCN</strong> is a <code>Conv2D</code> with dilation and causal padding applied across the temporal axis. The convolution layer slides across <code>[C, T]</code> causally. Stacking these layers with increasing dilation expands the receptive field.
    </p>
    <pre><code class="language-python">
class Conv2DTemporalTCN(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size=3, dilation=1, dropout=0.2):
        super().__init__()
        self.conv = nn.Conv2d(
            in_channels=in_channels,
            out_channels=out_channels,
            kernel_size=(1, kernel_size),  # (no spatial span, only temporal)
            dilation=(1, dilation),
            padding=(0, (kernel_size - 1) * dilation),  # causal padding only in time
        )
        self.bn = nn.BatchNorm2d(out_channels)
        self.elu = nn.ELU()
        self.dropout = nn.Dropout(dropout)

    def forward(self, x):
        """
        x: [B, in_channels, C, T]
        returns: [B, out_channels, C, T]
        """
        x = self.conv(x)
        x = x[..., :x.shape[-1]]  # Trim future steps for causality
        x = self.bn(x)
        x = self.elu(x)
        x = self.dropout(x)
        return x
    </code></pre>
    
    <hr>
    
    <h2 id="architectures">Key CNN Architectures</h2>
    <p>
        Now, with this summary, let’s try to understand a couple of very well-known CNN architectures.
    </p>

    <h3>AlexNet</h3>
    <p>
        AlexNet made CNNs famous. It introduced <strong>ReLU</strong>, <strong>overlapping max-pooling</strong>, and made heavy use of strided convolution and pooling. With overlapping max-pooling, the goal was to reduce spatial resolution to make training feasible while keeping more information than non-overlapping pooling.
    </p>

    <h3>VGG</h3>
    <p>
        VGG is an architecture that stacks many <strong>3x3 convolution layers</strong> with max-pooling to keep the receptive field large while extracting key features. This is a form of smart compression and a signal booster for important patterns.
    </p>

    <h3>ResNet</h3>
    <p>
        ResNet (Residual Network) uses <strong>skip connections</strong> to allow gradients to flow through very deep networks. Instead of learning <code>H(x)</code>, the model learns the change from the input <code>x</code>, <code>F(x) = H(x) - x</code>, so <code>H(x) = F(x) + x</code>. It’s often easier for the model to learn what needs to be added to <code>x</code> than a full transformation of <code>x</code>. Some variants use a <strong>bottleneck block</strong> to reduce the number of channels before a <code>3x3</code> convolution and then expand it back to the original size with a <code>1x1</code> convolution.
    </p>

    <h3>MobileNet</h3>
    <p>
        MobileNet was designed for mobile and embedded devices. It introduced <strong>depthwise separable convolutions</strong> to drastically reduce parameters and compute. <strong>MobileNetV2</strong> uses a clever twist on ResNet's skip connection called <strong>inverted residuals</strong>. Instead of shrinking, processing, and expanding, it expands, processes, and then shrinks. It also uses global average pooling instead of a fully-connected layer for classification.
    </p>
    
    <h3>EfficientNet</h3>
    <p>
        EfficientNet is an extremely powerful CNN built on the ideas of ResNet and MobileNet, using neural architecture search and <strong>compound scaling</strong> to achieve state-of-the-art accuracy with an order of magnitude fewer parameters and FLOPs. Instead of scaling depth, width, or resolution individually, EfficientNet scales them together with a fixed ratio. It uses a block similar to MobileNetV2's but adds a <strong>Squeeze-and-Excitation (SE) block</strong>. The SE block provides channel-wise attention, allowing the network to reweigh each feature map based on its importance.
    </p>
    <pre><code class="language-python">
class SEBlock(nn.Module):
    def __init__(self, channels, reduction=16):
        super().__init__()
        self.pool = nn.AdaptiveAvgPool2d(1)  # -> [B, C, 1, 1]
        self.fc = nn.Sequential(
            nn.Linear(channels, channels // reduction),
            nn.ReLU(),
            nn.Linear(channels // reduction, channels),
            nn.Sigmoid()
        )

    def forward(self, x):
        B, C, H, W = x.shape
        s = self.pool(x).view(B, C)      # [B, C]
        w = self.fc(s).view(B, C, 1, 1)  # [B, C, 1, 1]
        return x * w                      # Channel-wise scale
    </code></pre>
    <p>
        The SE block is placed inside the <strong>MBConv</strong> block, right after the depthwise convolution and before the final projection layer.
    </p>
    <pre><code class="language-python">
class MBConv(nn.Module):
    def __init__(self, in_ch, out_ch, expansion=6, kernel_size=3, stride=1, se_reduction=4):
        super().__init__()
        mid_ch = in_ch * expansion
        self.use_residual = (in_ch == out_ch) and (stride == 1)

        self.expand = nn.Sequential(
            nn.Conv2d(in_ch, mid_ch, kernel_size=1, bias=False),
            nn.BatchNorm2d(mid_ch),
            nn.SiLU()
        ) if expansion != 1 else nn.Identity()

        self.depthwise = nn.Sequential(
            nn.Conv2d(mid_ch, mid_ch, kernel_size, stride, padding=kernel_size // 2,
                      groups=mid_ch, bias=False),
            nn.BatchNorm2d(mid_ch),
            nn.SiLU()
        )

        self.se = SEBlock(mid_ch, reduction=se_reduction)

        self.project = nn.Sequential(
            nn.Conv2d(mid_ch, out_ch, kernel_size=1, bias=False),
            nn.BatchNorm2d(out_ch)
        )

    def forward(self, x):
        identity = x
        x = self.expand(x)
        x = self.depthwise(x)
        x = self.se(x)
        x = self.project(x)
        if self.use_residual:
            x = x + identity
        return x
    </code></pre>

    <h3>Inception</h3>
    <p>
        The design of CNNs can feel like trial and error. The Inception architecture's answer is: instead of picking one kernel size, let’s compute multiple receptive fields in parallel and let the network figure out what matters. It has four branches:
    </p>
    <ul>
        <li><strong>Branch 1</strong>: A <code>1x1</code> convolution.</li>
        <li><strong>Branch 2</strong>: A <code>1x1</code> convolution followed by a <code>3x3</code> convolution.</li>
        <li><strong>Branch 3</strong>: A <code>1x1</code> convolution followed by a <code>5x5</code> convolution.</li>
        <li><strong>Branch 4</strong>: A <code>3x3</code> max-pooling layer followed by a <code>1x1</code> convolution.</li>
    </ul>
    <p>
        The channels from all branches are then concatenated. Inception also uses <strong>factorized convolutions</strong> (e.g., using two <code>3x3</code>s instead of one <code>5x5</code>). Newer versions combine Inception blocks with residual connections.
    </p>
    <h4>Inception Architecture Layout:</h4>
    <pre><code>
[Input: 224x224 RGB]
      ↓
[Stem Block]
      ↓
[Inception(3a-3b)] → MaxPool
      ↓
[Inception(4a-4e)] → MaxPool
      ↓
[Inception(5a-5b)]
      ↓
[GlobalAvgPool + Dropout + FC]
      ↓
[Output: 1000 classes]
    </code></pre>
    <p>
     We have learned about the basics of CNNs: kernels, dilation, and more. We learned about the receptive field and how to calculate it. We learned about depthwise, pointwise, and separable convolutions. We learned about pooling and strided convolution. We learned about batch normalization and temporal/causal convolution blocks. And we looked at a number of popular CNN architectures and how they employ these techniques.
    </p>
</body>
</html>
