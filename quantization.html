<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Quantization</title>
  <style>
    body {
      font-family: sans-serif;
      max-width: 900px;
      margin: 0 auto;
      padding: 2rem;
      line-height: 1.6;
    }
    h1, h2, h3 {
      color: #333;
    }
    nav { margin-bottom: 30px; padding: 10px; background: #f9f9f9; border: 1px solid #ddd; }
    nav ul { list-style: none; padding: 0; }
    nav li { display: inline-block; margin-right: 15px; }
    nav a { text-decoration: none; color: #007BFF; }
    nav a:hover { text-decoration: underline; }
    pre {
      background: #f4f4f4;
      padding: 1rem;
      overflow-x: auto;
      border-left: 4px solid #ccc;
      border-radius: 4px; /* Added for consistency */
    }
    code {
      font-family: monospace;
      background: #f4f4f4; /* For inline code */
      padding: 0.2rem 0.4rem;
      border-radius: 4px; /* Added for consistency */
    }
    pre code { /* Reset for code inside pre */
        background: none;
        padding: 0;
    }
    section {
        margin-bottom: 2rem; /* Added for spacing between sections */
        padding-bottom: 1rem; /* Added for spacing */
        border-bottom: 1px solid #eee; /* Added for visual separation */
    }
    section:last-of-type {
        border-bottom: none; /* Remove border for the last section */
    }
    ul {
        padding-left: 20px; /* Indent lists slightly */
    }
  </style>
</head>
<body>
   <nav>
     <ul>
       <li><a href="index.html">Home</a></li>
       <li><a href="https://github.com/raminaeye/ML-Concepts/blob/main/notebooks/quantization/qunatization.ipynb">My notebook on applying quantization in Pytorch</a></li>
     </ul>
   </nav>
   <h1>Quantization</h1>

   <p>Smaller model means less memory bandwidth. Faster inference because fewer bits and fewer multiplies. Lower power because integer ops are cheaper than float on most hardware. You get the idea.</p>

   <p>There are a couple types of quantization: <strong>Post-Training Quantization (PTQ)</strong> and <strong>Quantization-Aware Training (QAT)</strong>. PTQ comes in two flavors: <strong>Dynamic</strong> and <strong>Static</strong>. Let’s break them down.</p>

   <section>
     <h2>Dynamic Quantization</h2>
     <p>This is the lazy version of quantization—and that’s not a bad thing. Super simple. You quantize the weights after training. Activations stay in float32 during inference but get quantized on the fly. No calibration. No fuss.</p>

     <pre><code>import torch.quantization

model = YourModel()
model.eval()
quantized_model = torch.quantization.quantize_dynamic(
    model, {torch.nn.Linear}, dtype=torch.qint8
)</code></pre>

     <ul>
       <li>Weights → int8</li>
       <li>Activations → float32</li>
       <li>Quantization of activations happens at runtime, per batch</li>
       <li>Dequantization happens right after matmul</li>
     </ul>

     <p>So why use it? It’s stupid easy and gives you a fast win on CPUs—especially for LSTMs and Linear-heavy models.</p>

     <p>But why doesn’t it work well for CNNs? Because Conv2d is not matmul. Conv layers have gnarly memory access patterns and need pre-quantized activations to be efficient. Dynamic quantization doesn’t do that. You’d be quantizing every sliding window patch per inference. Not fun. Not fast.</p>

     <p>Also, dynamic quantization doesn’t touch things like BatchNorm or ReLU. It just swaps in dynamic versions of Linear and LSTM layers. No fusion here.</p>
   </section>

   <section>
     <h2>Static Quantization</h2>
     <p>Now we’re doing it properly. Both weights and activations go to int8. But to quantize activations, you first need to know their range. That’s where <strong>calibration</strong> comes in.</p>

     <pre><code># Fuse layers
def fuse_model(self):
    torch.quantization.fuse_modules(self.conv, [['0', '1', '2']], inplace=True)

model = QuantCNN()
model.eval()
model.fuse_model()

# Set qconfig
model.qconfig = torch.quantization.get_default_qconfig('fbgemm')

# Insert observers
torch.quantization.prepare(model, inplace=True)

# Calibration
with torch.no_grad():
    for images in calibration_loader:
        model(images)

# Convert to int8
torch.quantization.convert(model, inplace=True)</code></pre>

     <h3>Fusion</h3>
     <p>Fusing Conv + BatchNorm + ReLU reduces memory overhead, speeds things up, and minimizes numerical errors.</p>

     <h3>QConfig</h3>
     <p>QConfig tells PyTorch <em>how</em> to quantize:</p>

     <pre><code>from torch.quantization import default_per_channel_weight_observer, MinMaxObserver

my_qconfig = torch.quantization.QConfig(
    activation=MinMaxObserver.with_args(dtype=torch.quint8),
    weight=default_per_channel_weight_observer
)
model.qconfig = my_qconfig</code></pre>

     <h3>Observers</h3>
     <p>Observers record stats during calibration:</p>

     <pre><code>class MinMaxObserver(ObserverBase):
    def forward(self, x):
        self.min_val = min(self.min_val, x.min())
        self.max_val = max(self.max_val, x.max())</code></pre>
   </section>

   <section>
     <h2>Histogram Observers</h2>
     <p>MinMax gets clobbered by outliers. Histogram observers are smarter. They simulate many clipping thresholds and pick the one with lowest quant error.</p>

     <pre><code>import torch.quantization as tq

qconfig = tq.QConfig(
    activation=tq.observer.HistogramObserver.with_args(
        dtype=torch.quint8, qscheme=torch.per_tensor_affine
    ),
    weight=tq.default_per_channel_weight_observer
)
model.qconfig = qconfig</code></pre>

     <p>This builds a histogram of activations and simulates quant-dequant at different thresholds. It finds the one with the smallest L2 loss between original and reconstructed signal.</p>
   </section>

   <section>
     <h2>Scale and Zero-Point</h2>

     <p>Quantize:</p>
     <code>Q = round(X / scale) + zero_point</code><br>
     <p>Dequantize:</p>
     <code>X = scale * (Q - zero_point)</code>

     <h3>Symmetric</h3>
     <ul>
       <li><code>zero_point = 0</code></li>
       <li><code>scale = max(abs(min), abs(max)) / 127</code></li>
     </ul>

     <h3>Asymmetric</h3>
     <ul>
       <li><code>zero_point ≠ 0</code></li>
       <li><code>scale = (max - min) / 255</code></li>
       <li><code>zero_point = round(-min / scale)</code></li>
     </ul>
   </section>

   <section>
     <h2>Calibration Tips</h2>
     <ul>
       <li>Bad calibration = bad quantization</li>
       <li>Make sure your calibration data reflects actual inference data</li>
       <li>Calibration is a one-time deal in PTQ—no backprop, no fine-tuning</li>
     </ul>
   </section>

   <section>
     <h2>Quantization Boundaries</h2>
     <p>Be careful when mixing float and quantized layers. Dequantizing and re-quantizing between layers kills performance and breaks fusion.</p>
     <p>Once you quantize, try to stay quantized.</p>
   </section>

   <section>
     <h2>Model Visualization with FX</h2>

     <pre><code>from torch.fx import symbolic_trace
graph = symbolic_trace(quantized_model)
print(graph.graph)</code></pre>

     <p>FX gives you a model graph. You can inspect, transform, and trace your quantization process. It’s the modern way. Use it.</p>

     <pre><code>qconfig_dict = {"": torch.quantization.get_default_qconfig('fbgemm')}</code></pre>
   </section>

   <section>
    <h2>PTQ Example: CNN with Manual Observers</h2>
    <p>Let’s do an example with a CNN model to wrap up PTQ. We manually add <code>PerChannelMinMaxObserver</code> for weights and <code>MinMaxObserver</code> for activation.</p>
<pre><code>import torch
import torch.nn as nn
import torch.nn.functional as F

from torch.ao.quantization.observer import (
    MinMaxObserver,
    PerChannelMinMaxObserver
)

class ObserverCNN(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(3, 16, 3, stride=1, padding=1)
        self.relu1 = nn.ReLU()
        self.conv2 = nn.Conv2d(16, 32, 3, stride=1, padding=1)
        self.relu2 = nn.ReLU()
        self.pool = nn.AdaptiveAvgPool2d((1, 1))
        self.fc = nn.Linear(32, 10)

        # Manual observers
        self.obs_act1 = MinMaxObserver()
        self.obs_act2 = MinMaxObserver()
        self.obs_fc_in = MinMaxObserver()

        self.obs_weight1 = PerChannelMinMaxObserver(ch_axis=0)
        self.obs_weight2 = PerChannelMinMaxObserver(ch_axis=0)
        self.obs_weight_fc = PerChannelMinMaxObserver(ch_axis=0)

    def forward(self, x):
        # Observe conv1 weight
        self.obs_weight1(self.conv1.weight)
        x = self.relu1(self.conv1(x))
        self.obs_act1(x)
        
        # Observe conv2 weight
        self.obs_weight2(self.conv2.weight)
        x = self.relu2(self.conv2(x))
        self.obs_act2(x)
        
        x = self.pool(x)
        x = x.view(x.size(0), -1)

        self.obs_weight_fc(self.fc.weight)
        self.obs_fc_in(x)
        return self.fc(x)

model = ObserverCNN().eval()

# Fake calibration pass
with torch.no_grad():
    for _ in range(10):  # simulate 10 batches
        x = torch.randn(8, 3, 32, 32)
        _ = model(x)

print("Conv1 activation range:", model.obs_act1.min_val.item(), model.obs_act1.max_val.item())
print("Conv2 activation range:", model.obs_act2.min_val.item(), model.obs_act2.max_val.item())
print("FC input range:", model.obs_fc_in.min_val.item(), model.obs_fc_in.max_val.item())

print("Conv1 weight qparams:", model.obs_weight1.calculate_qparams())
print("Conv2 weight qparams:", model.obs_weight2.calculate_qparams())
print("FC weight qparams:", model.obs_weight_fc.calculate_qparams())
</code></pre>
    <p><code>calculate_qparams()</code> is a method used by observers in PyTorch to compute scale and zero-point. That's the observer's job to begin with.</p>
    <p>The general quantization formula is:</p>
    <pre><code>quantized_value = round(clamp(float_value / scale + zero_point, qmin, qmax))</code></pre>
    
    <h3>For Symmetric Quantization:</h3>
    <ul>
        <li><code>scale = max(abs(min_val), abs(max_val)) / 127</code> (or 128, depending on the exact range, e.g., [-127, 127] or [-128, 127] for int8)</li>
        <li><code>zero_point = 0</code></li>
    </ul>

    <h3>For Asymmetric Quantization:</h3>
    <ul>
        <li><code>scale = (max_val - min_val) / (qmax - qmin)</code> (e.g., (max_val - min_val) / 255 for quint8)</li>
        <li><code>zero_point = round(qmin - min_val / scale)</code></li>
    </ul>
   </section>
   <section>
    <h2>Quantization-Aware Training (QAT)</h2>
    <p>If PTQ drops too much accuracy, QAT is how you can recover it. QAT requires retraining; it’s slower but more accurate. It learns to suppress outliers and it works well with tuning. You start with inserting observers just like PTQ. These observers are actually <code>FakeQuantize</code> modules that are differentiable. The model learns to adapt to quantization noise.</p>
    <pre><code>from torch.ao.quantization import get_default_qat_qconfig

qconfig = get_default_qat_qconfig('fbgemm')
qconfig_dict = {"": qconfig}

from torch.ao.quantization.quantize_fx import prepare_qat_fx, convert_fx

model.train() # Ensure model is in training mode for QAT
model_prepared = prepare_qat_fx(model, qconfig_dict)</code></pre>
    <p>Then you train the model (<code>model_prepared</code>) and convert it to int8 using <code>convert_fx(model_prepared.eval())</code>. Inside the <code>FakeQuantize</code> module, it takes input <code>x</code>, computes scale and zero-point, and simulates quantization like:</p>
    <pre><code>q = round(clamp(x / scale + zero_point))
x_hat = (q - zero_point) * scale</code></pre>
    <p>QAT can learn activation clipping, push small weights out of dead zones, and learn quantization-friendly distributions.</p>
    <p>QAT simulates quantization in the forward pass. Rounding and clamping are, however, both non-differentiable, so you can't directly backpropagate through them. QAT uses a <strong>Straight-Through Estimator (STE)</strong>, which basically means this operation has no gradient for the non-differentiable part. It pretends the output equals the input during backpropagation for that specific part.</p>
    <pre><code>class RoundSTE(torch.autograd.Function):
    @staticmethod
    def forward(ctx, x):
        return torch.round(x)

    @staticmethod
    def backward(ctx, grad_output):
        return grad_output  # straight-through (acts like identity for the rounding part)</code></pre>
    <p>STE allows the model to learn parameters and "survive" quantization during training.</p>
    <p>Now let’s go back to <code>MovingAverageMinMaxObserver</code> during QAT. It uses an exponential moving average of min/max values observed during training. Similar to PTQ, observers (which are <code>FakeQuantize</code> modules in QAT) are inserted during <code>prepare_qat_fx()</code> and are active during the training phase. After training, during <code>convert_fx()</code>, they’re replaced with real quantization operations.</p>
    <p>The moving average update rules are typically:</p>
    <pre><code>min_val = (1 - α) * min_val + α * current_batch_min_val
max_val = (1 - α) * max_val + α * current_batch_max_val</code></pre>
    <p>Where α (alpha) is a momentum term.</p>
   </section>
   <section>
     <h2>Final Notes</h2>
     <ul>
       <li>Dynamic quantization: quick win for linear/LSTM-heavy models</li>
       <li>Static quantization: proper path for CNNs and edge deployment</li>
       <li>Fusion improves performance and reduces error</li>
       <li>Choose the right observer for your data distribution</li>
       <li>Watch out for quant-dequant boundaries</li>
       <li>Use FX for quantization in modern PyTorch</li>
       <li><code>MovingAverageObserver</code> (or <code>MovingAverageMinMaxObserver</code>) is typically used for QAT, not PTQ. PTQ observers like <code>MinMaxObserver</code> or <code>HistogramObserver</code> collect stats on a calibration dataset.</li>
     </ul>
   </section>

</body>
</html>
