<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Quantization</title>
  <style>
    body {
      font-family: sans-serif;
      max-width: 900px;
      margin: 0 auto;
      padding: 2rem;
      line-height: 1.6;
    }
    h1, h2, h3 {
      color: #333;
    }
    nav { margin-bottom: 30px; padding: 10px; background: #f9f9f9; border: 1px solid #ddd; }
    nav ul { list-style: none; padding: 0; }
    nav li { display: inline-block; margin-right: 15px; }
    nav a { text-decoration: none; color: #007BFF; }
    nav a:hover { text-decoration: underline; }
    pre {
      background: #f4f4f4;
      padding: 1rem;
      overflow-x: auto;
      border-left: 4px solid #ccc;
      border-radius: 4px; /* Added for consistency */
    }
    code {
      font-family: monospace;
      background: #f4f4f4; /* For inline code */
      padding: 0.2rem 0.4rem;
      border-radius: 4px; /* Added for consistency */
    }
    pre code { /* Reset for code inside pre */
        background: none;
        padding: 0;
    }
    section {
        margin-bottom: 2rem; /* Added for spacing between sections */
        padding-bottom: 1rem; /* Added for spacing */
        border-bottom: 1px solid #eee; /* Added for visual separation */
    }
    section:last-of-type {
        border-bottom: none; /* Remove border for the last section */
    }
    ul {
        padding-left: 20px; /* Indent lists slightly */
    }
    .subsection-title {
        font-size: 1.25em; /* h4 like */
        font-weight: bold;
        margin-top: 1.5rem;
        margin-bottom: 0.5rem;
        color: #444;
    }
  </style>
</head>
<body>
   <nav>
     <ul>
       <li><a href="index.html">Home</a></li>
       <li><a href="https://github.com/raminaeye/ML-Concepts/blob/main/notebooks/quantization/qunatization.ipynb">My notebook on applying quantization in Pytorch</a></li>
     </ul>
   </nav>
   <h1>Quantization</h1>

   <p>Smaller model means less memory bandwidth. Faster inference because fewer bits and fewer multiplies. Lower power because integer ops are cheaper than float on most hardware. You get the idea.</p>

   <p>There are a couple types of quantization: <strong>Post-Training Quantization (PTQ)</strong> and <strong>Quantization-Aware Training (QAT)</strong>. PTQ comes in two flavors: <strong>Dynamic</strong> and <strong>Static</strong>. Let’s break them down.</p>

   <section>
     <h2>Dynamic Quantization</h2>
     <p>This is the lazy version of quantization—and that’s not a bad thing. Super simple. You quantize the weights after training. Activations stay in float32 during inference but get quantized on the fly. No calibration. No fuss.</p>

     <pre><code>import torch.quantization

model = YourModel()
model.eval()
quantized_model = torch.quantization.quantize_dynamic(
    model, {torch.nn.Linear}, dtype=torch.qint8
)</code></pre>

     <ul>
       <li>Weights → int8</li>
       <li>Activations → float32</li>
       <li>Quantization of activations happens at runtime, per batch</li>
       <li>Dequantization happens right after matmul</li>
     </ul>

     <p>So why use it? It’s stupid easy and gives you a fast win on CPUs—especially for LSTMs and Linear-heavy models.</p>

     <p>But why doesn’t it work well for CNNs? Because Conv2d is not matmul. Conv layers have gnarly memory access patterns and need pre-quantized activations to be efficient. Dynamic quantization doesn’t do that. You’d be quantizing every sliding window patch per inference. Not fun. Not fast.</p>

     <p>Also, dynamic quantization doesn’t touch things like BatchNorm or ReLU. It just swaps in dynamic versions of Linear and LSTM layers. No fusion here.</p>
   </section>

   <section>
     <h2>Static Quantization</h2>
     <p>Now we’re doing it properly. Both weights and activations go to int8. But to quantize activations, you first need to know their range. That’s where <strong>calibration</strong> comes in.</p>

     <pre><code># Fuse layers
def fuse_model(self):
    torch.quantization.fuse_modules(self.conv, [['0', '1', '2']], inplace=True)

model = QuantCNN()
model.eval()
model.fuse_model()

# Set qconfig
model.qconfig = torch.quantization.get_default_qconfig('fbgemm')

# Insert observers
torch.quantization.prepare(model, inplace=True)

# Calibration
with torch.no_grad():
    for images in calibration_loader:
        model(images)

# Convert to int8
torch.quantization.convert(model, inplace=True)</code></pre>

     <h3>Fusion</h3>
     <p>Fusing Conv + BatchNorm + ReLU reduces memory overhead, speeds things up, and minimizes numerical errors.</p>

     <h3>QConfig</h3>
     <p>QConfig tells PyTorch <em>how</em> to quantize:</p>

     <pre><code>from torch.quantization import default_per_channel_weight_observer, MinMaxObserver

my_qconfig = torch.quantization.QConfig(
    activation=MinMaxObserver.with_args(dtype=torch.quint8),
    weight=default_per_channel_weight_observer
)
model.qconfig = my_qconfig</code></pre>

     <h3>Observers</h3>
     <p>Observers record stats during calibration:</p>

     <pre><code>class MinMaxObserver(ObserverBase):
    def forward(self, x):
        self.min_val = min(self.min_val, x.min())
        self.max_val = max(self.max_val, x.max())</code></pre>
   </section>

   <section>
     <h2>Histogram Observers</h2>
     <p>MinMax gets clobbered by outliers. Histogram observers are smarter. They simulate many clipping thresholds and pick the one with lowest quant error.</p>

     <pre><code>import torch.quantization as tq

qconfig = tq.QConfig(
    activation=tq.observer.HistogramObserver.with_args(
        dtype=torch.quint8, qscheme=torch.per_tensor_affine
    ),
    weight=tq.default_per_channel_weight_observer
)
model.qconfig = qconfig</code></pre>

     <p>This builds a histogram of activations and simulates quant-dequant at different thresholds. It finds the one with the smallest L2 loss between original and reconstructed signal.</p>
   </section>

   <section>
     <h2>Scale and Zero-Point</h2>

     <p>Quantize:</p>
     <code>Q = round(X / scale) + zero_point</code><br>
     <p>Dequantize:</p>
     <code>X = scale * (Q - zero_point)</code>

     <h3>Symmetric</h3>
     <ul>
       <li><code>zero_point = 0</code></li>
       <li><code>scale = max(abs(min), abs(max)) / 127</code></li>
     </ul>

     <h3>Asymmetric</h3>
     <ul>
       <li><code>zero_point ≠ 0</code></li>
       <li><code>scale = (max - min) / 255</code></li>
       <li><code>zero_point = round(-min / scale)</code></li>
     </ul>
   </section>

   <section>
     <h2>Calibration Tips</h2>
     <ul>
       <li>Bad calibration = bad quantization</li>
       <li>Make sure your calibration data reflects actual inference data</li>
       <li>Calibration is a one-time deal in PTQ—no backprop, no fine-tuning</li>
     </ul>
   </section>

   <section>
     <h2>Quantization Boundaries</h2>
     <p>Be careful when mixing float and quantized layers. Dequantizing and re-quantizing between layers kills performance and breaks fusion.</p>
     <p>Once you quantize, try to stay quantized.</p>
   </section>

   <section>
     <h2>Model Visualization with FX</h2>

     <pre><code>from torch.fx import symbolic_trace
graph = symbolic_trace(quantized_model)
print(graph.graph)</code></pre>

     <p>FX gives you a model graph. You can inspect, transform, and trace your quantization process. It’s the modern way. Use it.</p>

     <pre><code>qconfig_dict = {"": torch.quantization.get_default_qconfig('fbgemm')}</code></pre>
   </section>

   <section>
    <h2>PTQ Example: CNN with Manual Observers</h2>
    <p>Let’s do an example with a CNN model to wrap up PTQ. We manually add <code>PerChannelMinMaxObserver</code> for weights and <code>MinMaxObserver</code> for activation.</p>
<pre><code>import torch
import torch.nn as nn
import torch.nn.functional as F

from torch.ao.quantization.observer import (
    MinMaxObserver,
    PerChannelMinMaxObserver
)

class ObserverCNN(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(3, 16, 3, stride=1, padding=1)
        self.relu1 = nn.ReLU()
        self.conv2 = nn.Conv2d(16, 32, 3, stride=1, padding=1)
        self.relu2 = nn.ReLU()
        self.pool = nn.AdaptiveAvgPool2d((1, 1))
        self.fc = nn.Linear(32, 10)

        # Manual observers
        self.obs_act1 = MinMaxObserver()
        self.obs_act2 = MinMaxObserver()
        self.obs_fc_in = MinMaxObserver()

        self.obs_weight1 = PerChannelMinMaxObserver(ch_axis=0)
        self.obs_weight2 = PerChannelMinMaxObserver(ch_axis=0)
        self.obs_weight_fc = PerChannelMinMaxObserver(ch_axis=0)

    def forward(self, x):
        # Observe conv1 weight
        self.obs_weight1(self.conv1.weight)
        x = self.relu1(self.conv1(x))
        self.obs_act1(x)
        
        # Observe conv2 weight
        self.obs_weight2(self.conv2.weight)
        x = self.relu2(self.conv2(x))
        self.obs_act2(x)
        
        x = self.pool(x)
        x = x.view(x.size(0), -1)

        self.obs_weight_fc(self.fc.weight)
        self.obs_fc_in(x)
        return self.fc(x)

model = ObserverCNN().eval()

# Fake calibration pass
with torch.no_grad():
    for _ in range(10):  # simulate 10 batches
        x = torch.randn(8, 3, 32, 32)
        _ = model(x)

print("Conv1 activation range:", model.obs_act1.min_val.item(), model.obs_act1.max_val.item())
print("Conv2 activation range:", model.obs_act2.min_val.item(), model.obs_act2.max_val.item())
print("FC input range:", model.obs_fc_in.min_val.item(), model.obs_fc_in.max_val.item())

print("Conv1 weight qparams:", model.obs_weight1.calculate_qparams())
print("Conv2 weight qparams:", model.obs_weight2.calculate_qparams())
print("FC weight qparams:", model.obs_weight_fc.calculate_qparams())
</code></pre>
    <p><code>calculate_qparams()</code> is a method used by observers in PyTorch to compute scale and zero-point. That's the observer's job to begin with.</p>
    <p>The general quantization formula is:</p>
    <pre><code>quantized_value = round(clamp(float_value / scale + zero_point, qmin, qmax))</code></pre>
    
    <h3>For Symmetric Quantization:</h3>
    <ul>
        <li><code>scale = max(abs(min_val), abs(max_val)) / 127</code> (or 128, depending on the exact range, e.g., [-127, 127] or [-128, 127] for int8)</li>
        <li><code>zero_point = 0</code></li>
    </ul>

    <h3>For Asymmetric Quantization:</h3>
    <ul>
        <li><code>scale = (max_val - min_val) / (qmax - qmin)</code> (e.g., (max_val - min_val) / 255 for quint8)</li>
        <li><code>zero_point = round(qmin - min_val / scale)</code></li>
    </ul>
   </section>
   
   <section>
    <h2>Quantization-Aware Training (QAT)</h2>
    <p>If PTQ drops too much accuracy, QAT is how you can recover it. QAT requires retraining; it’s slower but more accurate. It learns to suppress outliers and it works well with tuning. You start with inserting observers just like PTQ. These observers are actually <code>FakeQuantize</code> modules that are differentiable. The model learns to adapt to quantization noise.</p>
    <pre><code>from torch.ao.quantization import get_default_qat_qconfig

qconfig = get_default_qat_qconfig('fbgemm')
qconfig_dict = {"": qconfig}

from torch.ao.quantization.quantize_fx import prepare_qat_fx, convert_fx

model.train() # Ensure model is in training mode for QAT
model_prepared = prepare_qat_fx(model, qconfig_dict)</code></pre>
    <p>Then you train the model (<code>model_prepared</code>) and convert it to int8 using <code>convert_fx(model_prepared.eval())</code>. Inside the <code>FakeQuantize</code> module, it takes input <code>x</code>, computes scale and zero-point, and simulates quantization like:</p>
    <pre><code>q = round(clamp(x / scale + zero_point))
x_hat = (q - zero_point) * scale</code></pre>
    <p>QAT can learn activation clipping, push small weights out of dead zones, and learn quantization-friendly distributions.</p>
    <p>QAT simulates quantization in the forward pass. Rounding and clamping are, however, both non-differentiable, so you can't directly backpropagate through them. QAT uses a <strong>Straight-Through Estimator (STE)</strong>, which basically means this operation has no gradient for the non-differentiable part. It pretends the output equals the input during backpropagation for that specific part.</p>
    <pre><code>class RoundSTE(torch.autograd.Function):
    @staticmethod
    def forward(ctx, x):
        return torch.round(x)

    @staticmethod
    def backward(ctx, grad_output):
        return grad_output  # straight-through (acts like identity for the rounding part)</code></pre>
    <p>STE allows the model to learn parameters and "survive" quantization during training.</p>
    <p>Now let’s go back to <code>MovingAverageMinMaxObserver</code> during QAT. It uses an exponential moving average of min/max values observed during training. Similar to PTQ, observers (which are <code>FakeQuantize</code> modules in QAT) are inserted during <code>prepare_qat_fx()</code> and are active during the training phase. After training, during <code>convert_fx()</code>, they’re replaced with real quantization operations.</p>
    <p>The moving average update rules are typically:</p>
    <pre><code>min_val = (1 - α) * min_val + α * current_batch_min_val
max_val = (1 - α) * max_val + α * current_batch_max_val</code></pre>
    <p>Where α (alpha) is a momentum term.</p>
   </section>

   <section>
    <h2>QAT Training Details & FakeQuantize Modules</h2>
    <p>Training a QAT model often requires lower learning rates and more epochs to converge. It also needs careful initialization, for example, starting from a pretrained FP32 model. Similar to static quantization, layers such as BatchNorm (BN) are fused with preceding convolution or linear layers during the conversion step (<code>convert_fx</code>).</p>
    <p>After inserting observers (which are <code>FakeQuantize</code> modules in QAT) and monitoring ranges, it helps to fine-tune the model to the "frozen" quantization noise. This means that after an initial "warm-up" phase where quantization parameters (scale and zero-point) are being learned, these parameters are then fixed, and the model continues training to adapt to these specific, now constant, quantization effects.</p>
    
    <div class="subsection-title">A Note on Observers and FakeQuantize Modules</div>
    <p>Inside the <code>FakeQuantize</code> modules, there is an observer instance and a quantize-dequantize function. You can inspect them:</p>
    <pre><code># Assuming 'model_prepared' is your QAT model and 'layer_name' is a quantized layer
fq_module = model_prepared.layer_name.activation_post_process 
# For some model structures, it might be directly on the layer if it's a leaf module
# or within a sub-module if it's a sequential block. The exact path depends on your model structure.

# If fq_module is indeed the FakeQuantize module:
# print(fq_module.activation_post_process) # This would be the observer *within* the FakeQuantize module
# print(fq_module.calculate_qparams())     # This gets the current scale/zp from the FakeQuantize module
</code></pre>
    <p><strong>Correction/Clarification:</strong> The <code>activation_post_process</code> attribute on a layer (e.g., <code>model.conv1.activation_post_process</code>) *is* the <code>FakeQuantize</code> module itself after <code>prepare_qat_fx</code>. The observer is an attribute *of* this <code>FakeQuantize</code> module. For example:</p>
    <pre><code># Assuming model_prepared is your QAT model from prepare_qat_fx
# and 'conv1' is a layer in your model
if hasattr(model_prepared.conv1, 'activation_post_process'):
    fq_module = model_prepared.conv1.activation_post_process # This IS the FakeQuantize module
    
    # The observer is an attribute of the FakeQuantize module.
    # The actual name of the observer attribute within FakeQuantize might vary based on PyTorch version
    # or specific FakeQuantize class, but it's often 'observer' or similar, or it might be
    # the FakeQuantize module itself acting as its own observer for some parameters.
    # For example, MinMaxObserver, MovingAverageMinMaxObserver, etc. are set as fq_module.activation_post_process
    
    # To get the observer instance from the FakeQuantize module:
    # (This depends on the specific type of FakeQuantize module,
    #  often it's the module itself or an internal attribute like 'activation_post_process' again if nested)
    # For a typical FakeQuantize module, it holds the observer type it was configured with.
    # Let's assume fq_module is the FakeQuantize instance:
    print(type(fq_module)) # This will tell you it's a FakeQuantize module
    
    # To get its calculated scale and zero-point:
    current_scale, current_zero_point = fq_module.calculate_qparams()
    print(f"Scale: {current_scale}, Zero-point: {current_zero_point}")

    # If you want to see the min/max values the observer within FakeQuantize has recorded (if applicable):
    if hasattr(fq_module, 'min_val') and hasattr(fq_module, 'max_val'):
         print(f"Observed min: {fq_module.min_val.item()}, Observed max: {fq_module.max_val.item()}")
else:
    print("conv1 does not have activation_post_process, check layer name or model structure.")
</code></pre>

    <p>During the forward pass, observers are enabled to calculate the scale and zero-point, typically using <code>MovingAverageMinMaxObserver</code> for activations and <code>PerChannelMinMaxObserver</code> (or their moving average variants) for weights. The fake quantizer takes the output from these observers (or uses their statistics) and applies the quantize-dequantize simulation, using STE during backpropagation.</p>
    <p>During QAT training, observers are active, updating the range statistics, simulating quantization, and the model learns under these evolving quantization conditions. This is often called the "warm-up" phase. Once that’s done (e.g., after a certain number of epochs), the scale and zero-point are often "frozen" (i.e., observers stop updating these parameters). Training then continues with this consistent quantization noise to improve stability and allow the model to fully adapt and lock into the quantized regime.</p>
   </section>

   <section>
    <h2>Debugging Quantization Issues</h2>
    <p>Quantization can sometimes lead to unexpected behavior or performance drops. Here are common issues and how to trace them.</p>

    <div class="subsection-title">Common Quantization Bugs</div>
    <ul>
        <li><strong>Accuracy drops significantly:</strong>
            <ul>
                <li>Bad scale/zero-point due to poor calibration (PTQ) or observer setup (QAT).</li>
                <li>Layers not fused properly, or unnecessary quantize-dequantize (qd_q) boundaries introduced.</li>
                <li>ReLU ranges are too wide, skewing observers (consider ReLU6 or clipping).</li>
                <li>Using per-tensor quantization when per-channel might be more appropriate (especially for weights).</li>
            </ul>
        </li>
        <li><strong>Layers behave abnormally post-quantization:</strong>
            <ul>
                <li>Observer didn’t see representative data during calibration/training.</li>
                <li>Quantize-dequantize boundary in the wrong place, disrupting data flow.</li>
                <li>Rounding and clamping causing excessive clipping of values.</li>
            </ul>
        </li>
        <li><strong>FakeQuant not simulating correctly, or model not converging during QAT:</strong>
            <ul>
                <li>Observers are not updating their statistics.</li>
                <li>Observers are frozen too soon in the training process.</li>
                <li>FakeQuant module is clipping values too aggressively.</li>
            </ul>
        </li>
        <li><strong><code>convert_fx()</code> (or <code>torch.quantization.convert()</code>) fails:</strong>
            <ul>
                <li>Model wasn’t fused correctly before preparing for quantization.</li>
                <li>A QConfig was missing for a submodule that needs to be quantized.</li>
                <li>The FX graph tracer couldn’t correctly trace the model (more common with complex control flow).</li>
            </ul>
        </li>
    </ul>

    <div class="subsection-title">How to Trace the Bugs</div>
    
    <p><strong>1. Print Quantization Parameters:</strong></p>
    <p>Check the scale and zero-point for each quantized layer. If scale is zero or NaN, the observers aren’t seeing any data or the range is problematic. If scale is too large or too small, the range might be skewed by outliers.</p>
    <pre><code>for name, module in model_prepared.named_modules(): # Use your QAT-prepared model
    if isinstance(module, torch.ao.quantization.FakeQuantize):
        # For FakeQuantize modules, scale and zero_point are attributes
        print(f"{name} -- scale: {module.scale.item() if torch.is_tensor(module.scale) else module.scale}, zero_point: {module.zero_point.item() if torch.is_tensor(module.zero_point) else module.zero_point}")
    elif hasattr(module, 'weight_fake_quant'): # For layers with separate weight fake_quant
         print(f"{name}.weight_fake_quant -- scale: {module.weight_fake_quant.scale.item()}, zero_point: {module.weight_fake_quant.zero_point.item()}")
</code></pre>

    <p><strong>2. Visualize Activation Ranges:</strong></p>
    <p>It’s important to see what range of values a layer is producing. Ensure observers capture this range properly and avoid being skewed by outliers. If the range is off, you’ll get clipping, dead activations, or poor resolution. Hook into layers to see float ranges and compare them with observer reports.</p>
    <pre><code>import matplotlib.pyplot as plt

activation_histograms = {}

def capture_activations_hook(name):
    def hook(module, input, output):
        # Ensure output is a tensor; input can be a tuple
        data_to_log = output
        if isinstance(output, tuple): # Some layers might return tuples
            data_to_log = output[0] 
        if torch.is_tensor(data_to_log):
            activation_histograms[name] = data_to_log.detach().cpu().flatten().numpy()
        else:
            print(f"Warning: Output of {name} is not a tensor, it's {type(data_to_log)}")
    return hook

# Assuming 'model' is your float model or QAT model before conversion
# Attach hooks to relevant layers
# Ensure layer names like 'model.conv1' are correct for your model structure
# For example:
# hook_conv1 = model.conv1.register_forward_hook(capture_activations_hook("conv1_output"))
# hook_relu1 = model.relu1.register_forward_hook(capture_activations_hook("relu1_output"))

# Example: Attaching to a specific layer if your model is 'model_to_debug'
# model_to_debug = ObserverCNN() # Or your specific model
# hook_conv1 = model_to_debug.conv1.register_forward_hook(capture_activations_hook("conv1_output"))

# Run a forward pass with sample data
# with torch.no_grad():
#     _ = model_to_debug(torch.randn(1, 3, 32, 32)) # Adjust input shape as needed

# Plot histograms
# for name, data in activation_histograms.items():
#     plt.figure()
#     plt.hist(data, bins=100)
#     plt.title(f"Activation Histogram: {name}")
#     plt.xlabel("Value")
#     plt.ylabel("Frequency")
#     plt.grid(True)
#     plt.show()

# hook_conv1.remove() # Don't forget to remove hooks when done
# hook_relu1.remove()
</code></pre>
    <p><em>Note: The plotting code above is commented out to prevent immediate execution errors if matplotlib is not available or if run in a non-GUI environment. Uncomment and adapt as needed.</em></p>

    <p><strong>3. Compare Float vs. Quantized Outputs:</strong></p>
    <p>If a single layer is causing issues, compare its output before and after quantization to pinpoint discrepancies.</p>

    <p><strong>4. Disable Quantization from Layers One by One (for QAT models):</strong></p>
    <p>If you suspect a specific layer in a QAT-prepared model, you can temporarily disable its fake quantization to see if accuracy improves.</p>
    <pre><code># Assuming 'model_prepared' is your QAT model
# and 'conv1' is a layer with an activation_post_process (FakeQuantize module)
if hasattr(model_prepared.conv1, 'activation_post_process') and \
   hasattr(model_prepared.conv1.activation_post_process, 'disable_fake_quant'):
    model_prepared.conv1.activation_post_process.disable_fake_quant()

# Similarly for weight fake quant if it exists and is separate
if hasattr(model_prepared.conv1, 'weight_fake_quant') and \
   hasattr(model_prepared.conv1.weight_fake_quant, 'disable_fake_quant'):
    model_prepared.conv1.weight_fake_quant.disable_fake_quant()
</code></pre>
    <p>Then, re-evaluate the model. Remember to re-enable it (<code>.enable_fake_quant()</code>) or re-prepare the model for further debugging.</p>

    <p><strong>5. Run FX <code>print_tabular()</code>:</strong></p>
    <p>For models prepared with FX (<code>prepare_qat_fx</code> or <code>prepare_fx</code>), this shows which quantization observers/FakeQuant modules are attached and where scale and zero-points are applied. Look for missing <code>activation_post_process</code> attributes or unexpected quant/dequant pairs, which might indicate bad layer fusion or incorrect QConfig application.</p>
    <pre><code># Assuming 'model_prepared' is your FX-prepared QAT model
model_prepared.graph.print_tabular()</code></pre>

    <p><strong>6. Inspect <code>activation_post_process</code>:</strong></p>
    <p><code>activation_post_process</code> is the name of the attribute (holding a <code>FakeQuantize</code> module in QAT, or an observer in PTQ) that PyTorch uses to handle quantization of activations. It’s inserted when you call <code>prepare_qat_fx()</code> or <code>prepare()</code>.</p>
    <pre><code># Example:
# self.conv = nn.Conv2d(...)
# After prepare_qat_fx, self.conv will have an 'activation_post_process' attribute:
# self.conv.activation_post_process = FakeQuantize(...) 

# You can use it to compare with observer states:
# Assuming 'model_prepared' is your QAT model
if hasattr(model_prepared.conv1, 'activation_post_process'):
    fq_module = model_prepared.conv1.activation_post_process  # This is the FakeQuantize module
    
    current_scale, current_zp = fq_module.calculate_qparams()
    print(f"FakeQuant scale: {current_scale}, zero_point: {current_zp}")

    # The observer is part of the FakeQuantize module's configuration.
    # To see the min/max values the observer logic within FakeQuantize is using:
    if hasattr(fq_module, 'min_val') and hasattr(fq_module, 'max_val'):
        print(f"Observed min/max by FakeQuant's internal observer logic: {fq_module.min_val.item()}, {fq_module.max_val.item()}")
</code></pre>
    <p>If the actual input values to this layer are frequently outside the [min_val, max_val] range used by the <code>FakeQuantize</code> module, they’ll be clipped during the simulated quantization, potentially harming accuracy.</p>

    <p><strong>7. Use <code>torch.ao.quantization.quant_debug</code>:</strong></p>
    <p>This utility can help record and compare float vs. quantized activations, show layer-by-layer differences, and provide histograms of these differences. (Note: API and availability might vary across PyTorch versions).</p>
    <pre><code>from torch.ao.quantization.quant_debug import add_debug_observers_qat # For QAT
# Or from torch.ao.quantization.quant_debug import add_debug_observers # For PTQ

# Assuming 'model_prepared' is your QAT model and qconfig_dict is defined
# debug_model = add_debug_observers_qat(model_prepared, qconfig_dict) 
# Then run calibration/training epochs with debug_model
# Access stats via: debug_model.activation_debug_stats (or similar attribute)
</code></pre>
    <p><em>Consult the PyTorch documentation for the exact usage of <code>quant_debug</code> for your version.</em></p>

    <p><strong>8. Look for Outliers and Dead Activations Manually:</strong></p>
    <p>Plot histograms of weights and activations for various layers to spot outliers that might be skewing quantization ranges, or "dead" activations (always zero) that indicate issues.</p>
    <pre><code># Example for a generic tensor (e.g., weights or activations)
# import matplotlib.pyplot as plt
# my_tensor = model.conv1.weight.detach().cpu() # Or an activation tensor
# plt.hist(my_tensor.flatten().numpy(), bins=100)
# plt.title("Histogram of Tensor Values")
# plt.show()
</code></pre>
   </section>
   <section>
     <h2>Final Notes</h2>
     <ul>
       <li>Dynamic quantization: quick win for linear/LSTM-heavy models</li>
       <li>Static quantization: proper path for CNNs and edge deployment</li>
       <li>Fusion improves performance and reduces error</li>
       <li>Choose the right observer for your data distribution</li>
       <li>Watch out for quant-dequant boundaries</li>
       <li>Use FX for quantization in modern PyTorch</li>
       <li><code>MovingAverageObserver</code> (or <code>MovingAverageMinMaxObserver</code>) is typically used for QAT, not PTQ. PTQ observers like <code>MinMaxObserver</code> or <code>HistogramObserver</code> collect stats on a calibration dataset.</li>
     </ul>
   </section>

</body>
</html>
