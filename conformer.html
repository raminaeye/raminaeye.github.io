<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>The Conformer Model: Fusing CNNs and Transformers for ASR</title>
  <style>
    body {
      font-family: sans-serif;
      max-width: 900px;
      margin: 0 auto;
      padding: 2rem;
      line-height: 1.6;
      color: #333;
    }
    h1, h2, h3, h4 {
      color: #2c3e50; /* Darker shade for better contrast */
      margin-top: 1.5em;
      margin-bottom: 0.5em;
    }
    h1 { font-size: 2.5em; border-bottom: 2px solid #3498db; padding-bottom: 0.3em;}
    h2 { font-size: 2em; border-bottom: 1px solid #bdc3c7; padding-bottom: 0.2em;}
    h3 { font-size: 1.5em; }
    h4 { font-size: 1.2em; color: #555;}

    nav { margin-bottom: 30px; padding: 10px; background: #ecf0f1; border: 1px solid #bdc3c7; border-radius: 4px;}
    nav ul { list-style: none; padding: 0; }
    nav li { display: inline-block; margin-right: 15px; }
    nav a { text-decoration: none; color: #3498db; font-weight: bold;}
    nav a:hover { text-decoration: underline; color: #2980b9;}

    pre {
      background: #f8f9f9; /* Lighter background for code blocks */
      padding: 1rem;
      overflow-x: auto;
      border: 1px solid #e1e4e8; /* Softer border */
      border-left: 4px solid #3498db; /* Accent border */
      border-radius: 4px;
      font-size: 0.9em;
    }
    code {
      font-family: 'SFMono-Regular', Consolas, 'Liberation Mono', Menlo, Courier, monospace;
    }
    /* For inline code */
    p > code, li > code, table td > code {
      background: #e8eaed;
      padding: 0.2em 0.4em;
      border-radius: 3px;
      font-size: 0.85em;
    }
    pre code { /* Reset for code inside pre, already handled by pre styling */
        background: none;
        padding: 0;
        font-size: 1em; /* Ensure pre's font size is inherited */
    }
    ul, ol {
        padding-left: 20px;
    }
    li {
        margin-bottom: 0.5em;
    }
    strong {
        color: #2980b9;
    }
    hr {
      border: 0;
      height: 1px;
      background: #bdc3c7;
      margin-top: 2em;
      margin-bottom: 2em;
    }
  </style>
 
  <script type="text/javascript" async
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
  </script>
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [['$','$'], ['\\(','\\)']],
        displayMath: [['$$','$$'], ['\\[','\\]']],
        processEscapes: true
      }
    });
  </script>

  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
  <script>hljs.highlightAll();</script>
</head>
<body>
    <nav>
        <ul>
            <li><a href="#intro">Introduction</a></li>
            <li><a href="#block-structure">Block Structure</a></li>
            <li><a href="#attention">Self-Attention</a></li>
            <li><a href="#convolution">Convolution Module</a></li>
            <li><a href="#implementation">Implementation</a></li>
            <li><a href="#real-time">Real-Time Use</a></li>
        </ul>
    </nav>

    <h1 id="intro">The Conformer Model for Automatic Speech Recognition (ASR)</h1>
    <p>
        Let’s take a look at an ASR model that blends CNNs with Transformers for speech. Vanilla Transformers are great for global context but can be weak at capturing local details like phoneme edges and formants. CNNs are excellent for local structure but they miss global dependencies. The <strong>Conformer</strong>, a "convolution-augmented transformer," has both.
    </p>
    <p>
        Unlike text, ASR takes continuous audio as input. Because audio data has a high sampling rate (e.g., 16kHz), we first need to extract acoustic features. Common choices are log-mel spectrograms or filterbank features. For 1 second of speech, you might get 100 frames with 80 dimensions each. Just like the embedding layer that projects 1D tokens into a vector space, in speech there is a linear layer or convolutional layer that projects the acoustic features into the model's dimension, $d_{model}$. For example, this can project an $80$-dimensional feature vector to a $256$-dimensional one, so the input to the model becomes a tensor of shape $T \times d_{model}$ (e.g., $100 \times 256$).
    </p>

    <hr>
    
    <h2 id="block-structure">The Conformer Block Structure</h2>
    <p>A standard Transformer encoder block looks like this:</p>
    <pre><code>x → [MHSA] → +residual → [FFN] → +residual → out</code></pre>
    <p>The Conformer changes this to a "macaron-style" structure with two feed-forward networks sandwiching the attention and convolution modules:</p>
    <pre><code>x → [FFN(0.5)] → [MHSA] → [Conv] → [FFN(0.5)] → out</code></pre>
    <p>
        The <strong>macaron-style feed-forward network</strong> adds capacity and non-linearity but doesn’t mix information across time steps. The FFN outputs are scaled by half before the residual connection is added, to match the contribution of the single FFN in a vanilla Transformer. The first FFN applies a 2-layer MLP to each frame independently.
    </p>
    
    <hr>

    <h2 id="attention">Multi-Head Self-Attention in Conformer</h2>
    <p>
        After the first FFN, the input (now shape <code>B x T x d_model</code>) is projected into queries, keys, and values for the Multi-Head Self-Attention (MHSA) module.
    </p>
    <p>
        It’s almost the same as a vanilla Transformer, but instead of sinusoidal encodings, it uses <strong>relative positional encoding</strong>, which generalizes better to long sequences and relative timing. With absolute positional encodings, the input embedding is modified: $x_t \leftarrow x_t + PE(t)$.
    </p>
    <p>
        With relative positional encoding, we don’t alter the input embeddings directly. Instead, we introduce an extra bias term in the attention score based on the relative distance. The model learns biases based on how far tokens are apart, not their absolute index. This is useful for speech, which is shift-invariant.
    </p>
    $$ \text{score}(i,j) = Q_i \cdot K_j + Q_i \cdot r_{i-j} $$
    <p>
        Where $r_{i-j}$ is a learned embedding for the relative offset between positions $i$ and $j$. We define a maximum relative distance, and this offset can be anywhere between $-L \dots 0 \dots L$. The embedding table for these relative positions has a shape of $(2L+1) \times d_k$.
    </p>

    <hr>

    <h2 id="convolution">The Convolution Module</h2>
    <p>
        Next is the convolution module, which is the big difference from vanilla Transformers. This is a <strong>depthwise separable convolution</strong> used to explicitly model local sequential structure. The input is the context-enriched sequence from the attention layer.
    </p>
    <p>The convolution module looks similar to those in EfficientNet and MobileNet:</p>
    <ol>
        <li>The features are first passed to a LayerNorm to be normalized along the $d_{model}$ dimension.</li>
        <li>A pointwise convolution with a <strong>Gated Linear Unit (GLU)</strong> activation is applied. The pointwise convolution expands the number of channels to $2 \times d_{model}$. The GLU works by splitting the channels in half and applying $A \cdot \sigma(B)$, where $B$ acts as a learned gate controlling how much of $A$ to pass through. The output dimension returns to $d_{model}$.</li>
        <li>A <strong>depthwise convolution</strong> is applied along the time axis. Each channel gets its own convolution filter, with a kernel size that might correspond to 150 ms. In streaming ASR, you’d switch from `same` to causal padding.</li>
        <li>A <strong>BatchNorm</strong> is applied. Let’s think, would you use LayerNorm or BatchNorm here? LayerNorm would normalize per frame across all channels, which could wash away the learned temporal filters. BatchNorm normalizes per channel across time and batch, so each channel keeps its identity and the normalization is consistent across time, which helps stabilize the convolution filters. So, BatchNorm makes more sense here, although it has issues with streaming inference. It is followed by a **Swish** activation.</li>
        <li>Finally, another pointwise convolution with dropout projects back to $d_{model}$, and a residual connection from the module's input is added.</li>
    </ol>
    
    <h3>BatchNorm for Streaming</h3>
    <p>
        Let’s briefly discuss BatchNorm during training and inference. During training, it computes batch statistics and keeps a running estimate of the mean and variance. During inference, you use these running stats:
    </p>
    $$ x_{norm} = \gamma \cdot \frac{x - \mu_{running}}{\sqrt{\sigma^2_{running} + \epsilon}} + \beta $$
    <p>
        With streaming ASR, the issue is that the running stats were computed on long sequences, but at inference you are processing a small, local chunk. To handle this, you need a causal/online BN where the mean and variance are computed only over past frames (e.g., frames $\le t$).
    </p>

    <hr>

    <h2 id="implementation">Full Conformer Block Implementation</h2>
    <p>
        The output from the convolution block remains <code>B x T x d_model</code>. After this, another feed-forward network, similar to the first, is applied to complete the macaron-style design. We know ML people love stacking blocks!
    </p>
    <pre><code class="language-python">
import torch
import torch.nn as nn
import torch.nn.functional as F

# -------------------------------
# 1. Macaron-style FeedForward
# -------------------------------
class ConformerFFN(nn.Module):
    def __init__(self, d_model, d_ff, dropout=0.1):
        super().__init__()
        self.linear1 = nn.Linear(d_model, d_ff)
        self.linear2 = nn.Linear(d_ff, d_model)
        self.dropout = nn.Dropout(dropout)
        self.activation = nn.SiLU()  # Swish

    def forward(self, x):
        out = self.linear1(x)
        out = self.activation(out)
        out = self.dropout(out)
        out = self.linear2(out)
        out = self.dropout(out)
        return x + 0.5 * out  # Macaron: scale by 0.5

# -------------------------------
# 2. Multi-Head Self-Attention (vanilla version, can extend with relative PE)
# -------------------------------
class MHSA(nn.Module):
    def __init__(self, d_model, num_heads, dropout=0.1):
        super().__init__()
        assert d_model % num_heads == 0
        self.h = num_heads
        self.d_k = d_model // num_heads
        self.W_q = nn.Linear(d_model, d_model)
        self.W_k = nn.Linear(d_model, d_model)
        self.W_v = nn.Linear(d_model, d_model)
        self.W_o = nn.Linear(d_model, d_model)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x, mask=None):
        B, T, D = x.shape
        Q = self.W_q(x).view(B, T, self.h, self.d_k).transpose(1, 2)
        K = self.W_k(x).view(B, T, self.h, self.d_k).transpose(1, 2)
        V = self.W_v(x).view(B, T, self.h, self.d_k).transpose(1, 2)

        scores = torch.matmul(Q, K.transpose(-2, -1)) / (self.d_k ** 0.5)
        if mask is not None:
            scores = scores.masked_fill(mask == 0, float("-inf"))
        attn = F.softmax(scores, dim=-1)
        out = torch.matmul(attn, V)

        out = out.transpose(1, 2).contiguous().view(B, T, D)
        return x + self.dropout(self.W_o(out))  # residual

# -------------------------------
# 3. Convolution Module
# -------------------------------
class ConformerConvModule(nn.Module):
    def __init__(self, d_model, kernel_size=15, dropout=0.1, causal=False):
        super().__init__()
        self.layer_norm = nn.LayerNorm(d_model)
        self.pointwise_conv1 = nn.Conv1d(d_model, 2 * d_model, kernel_size=1)
        padding = (kernel_size - 1) if causal else (kernel_size - 1) // 2
        self.depthwise_conv = nn.Conv1d(
            d_model, d_model, kernel_size,
            groups=d_model, padding=padding
        )
        self.causal = causal
        self.batch_norm = nn.BatchNorm1d(d_model)
        self.pointwise_conv2 = nn.Conv1d(d_model, d_model, kernel_size=1)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x):
        residual = x
        x = self.layer_norm(x)
        x = x.transpose(1, 2)  # (B, d_model, T)

        x = self.pointwise_conv1(x)
        A, B = x.chunk(2, dim=1)
        x = A * torch.sigmoid(B)  # GLU

        x = self.depthwise_conv(x)
        if self.causal:
            x = x[:, :, :residual.size(1)]  # trim if causal

        x = self.batch_norm(x)
        x = F.silu(x)
        x = self.pointwise_conv2(x)
        x = x.transpose(1, 2)  # back to (B, T, d_model)
        return residual + self.dropout(x)  # residual

# -------------------------------
# 4. Full Conformer Block
# -------------------------------
class ConformerBlock(nn.Module):
    def __init__(self, d_model=256, d_ff=1024, num_heads=4, kernel_size=15, dropout=0.1, causal=False):
        super().__init__()
        self.ffn1 = ConformerFFN(d_model, d_ff, dropout)
        self.mhsa = MHSA(d_model, num_heads, dropout)
        self.conv = ConformerConvModule(d_model, kernel_size, dropout, causal)
        self.ffn2 = ConformerFFN(d_model, d_ff, dropout)
        self.final_ln = nn.LayerNorm(d_model)

    def forward(self, x, mask=None):
        x = self.ffn1(x)
        x = self.mhsa(x, mask)
        x = self.conv(x)
        x = self.ffn2(x)
        return self.final_ln(x)
    </code></pre>
    
    <hr>
    
    <h2 id="real-time">Real-Time Adaptations and Final Output</h2>
    
    <h3>Making the Conformer Work in Real-Time</h3>
    <p>
        Now, how do you make the Conformer work in real-time? The attention mechanism, as we mentioned earlier, needs to change to a <strong>streaming attention</strong> where we restrict how far queries can look back using a fixed attention window with causal masking. Sometimes speech can be ambiguous if you don’t peek a little into the future, so we can allow each frame to have a <strong>look ahead</strong> of a fixed number of frames. This adds some latency but improves the model, and it can be easily configured in the attention mask.
    </p>
    <p>
        Additionally, you can make your multi-head attention into a <strong>multiscale multihead attention</strong>. Because information varies in length—phonemes can be shorter, while syllables and words are longer—one head might span more frames than another to capture these different relationships.
    </p>
    <p>
        The convolution module, which uses <code>same</code> padding, will also need to be changed to use <strong>causal padding</strong>, and the normalization, as we discussed in depth, needs to change as well to a streaming-friendly version.
    </p>

    <h3>Mapping to Text and Using CTC Loss</h3>
    <p>
        Okay, now we have our Conformer block, and we can stack several Conformer blocks to enrich the acoustic embeddings. Now we need to map this to text. What we have are frame-level embeddings that need to be converted back to words or tokens.
    </p>
    <p>
        You can add a linear layer after the stack of Conformer blocks that takes the final feature dimension, $d_{model}$, and projects it to $|V|$, the number of words, tokens, or phonemes in the vocabulary. The output is a logit matrix $Z$ of shape $B \times T \times |V|$ (Batch, Time, Vocabulary).
    </p>
    <p>
        Then, you take the model outputs (the logits, which are unnormalized scores) and pass them through a softmax function for each frame to get token probabilities:
    </p>
    $$ P(t,v) = \frac{\exp(Z_{t,v})}{\sum_{v'} \exp(Z_{t,v'})} $$
    <p>
        Once you have these frame-level probabilities, you need to use a loss function like the <strong>Connectionist Temporal Classification (CTC) loss</strong> to align the output probabilities with the ground-truth text sequence.
    </p>

</body>
</html>
