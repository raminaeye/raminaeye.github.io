<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>What I'm Reading | Ramin Anushiravani</title>
  <style>
    body { 
      font-family: Arial, sans-serif; 
      margin: 0; 
      padding: 0; 
      background-color: #f4f4f4; 
      line-height: 1.6;
      color: #333;
    }
    .container { 
      max-width: 900px; 
      margin: 20px auto; 
      background: #fff; 
      padding: 20px; 
      border-radius: 8px; 
      box-shadow: 0px 0px 10px rgba(0,0,0,0.1);
    }
    header { text-align: center; padding-bottom: 20px; border-bottom: 1px solid #ddd; }
    nav { text-align: center; margin-top: 10px; }
    nav a { margin: 0 10px; text-decoration: none; color: #0073e6; font-weight: bold; }
    h2 { border-bottom: 2px solid #0073e6; padding-bottom: 5px; }
    ul { list-style-type: none; padding: 0; }
    ul li { margin: 10px 0; }
    .paper { margin-bottom: 20px; }
    footer { text-align: center; margin-top: 40px; font-size: 0.9em; color: #777; }
  </style>
</head>
<body>
  <div class="container">
    <header>
      <h1>What I'm Reading</h1>
      <nav>
        <a href="index.html">Home</a> | 
        <a href=pubs.html>Publications</a> | 
        <a href="Papers.html">What I'm Reading</a> | 
        <a href="work.html">What I'm Working on</a> | 
        <a href=tutorial.html>Tutorials</a>
      </nav>
    </header>

    <section>
      <h2><a href = "https://github.com/raminaeye/AI_Papers">Recent Papers & Summaries</a></h2>
      <div class="paper">
        <h3>NVIDIA Flamingo</h3>
        <p><strong></strong>.</p>
      </div>
      <div class="paper">
        <h3>Open-Set Sound Event Classification using Self-Supervised Learning</h3>
        <p><strong>Analogy:</strong> A security system that recognizes known sounds but flags unknown ones.</p>
        <p><strong>Novelty & Interest:</strong> Uses deep learning to form a compact “map” of known sounds while detecting new ones.</p>
        <p><strong>Conclusion:</strong> Successfully enhances recognition accuracy for both familiar and novel sound events.</p>
      </div>
      <div class="paper">
        <h3>How Should We Extract Discrete Audio Tokens from Self-Supervised Models?</h3>
      </div>
      <div class="paper">
        <h3>Show and Tell: A Neural Image Caption Generator</h3>
        <p><strong>Analogy:</strong> A bilingual tour guide describing landmarks in vivid detail.</p>
        <p><strong>Novelty & Interest:</strong> Combines CNN and RNN to bridge vision and language.</p>
        <p><strong>Conclusion:</strong> Generates accurate, fluent captions, advancing image description techniques.</p>
      </div>
      <div class="paper">
        <h3>VideoBERT: A Joint Model for Video and Language Representation Learning</h3>
        <p><strong>Analogy:</strong> A film editor aligning visual frames with textual context.</p>
        <p><strong>Novelty & Interest:</strong> Adapts BERT to video, training with masked prediction tasks.</p>
        <p><strong>Conclusion:</strong> Enhances video representation learning, supporting video captioning and forecasting.</p>
      </div>
      <div class="paper">
        <h3>LLaMA: Open and Efficient Foundation Language Models</h3>
        <p><strong>Analogy:</strong> Building a library with publicly available books instead of rare manuscripts.</p>
        <p><strong>Novelty & Interest:</strong> Uses massive public datasets, challenging reliance on proprietary data.</p>
        <p><strong>Conclusion:</strong> Democratizes language modeling while achieving state-of-the-art results.</p>
      </div>
      <div class="paper">
        <h3>Conformer: Convolution-augmented Transformer for Speech Recognition</h3>
        <p><strong>Analogy:</strong> A musician reading both the overall score and the fine details of each note.</p>
        <p><strong>Novelty & Interest:</strong> Interleaves self-attention with convolutional layers for speech processing.</p>
        <p><strong>Conclusion:</strong> Achieves state-of-the-art accuracy on LibriSpeech.</p>
      </div>
      <div class="paper">
        <h3>AudioPaLM: Speech and Text Multimodal Language Model</h3>
        <p><strong>Analogy:</strong> A multilingual storyteller seamlessly switching between speech and text.</p>
        <p><strong>Novelty & Interest:</strong> Combines pre-trained text models with innovative audio tokenization.</p>
        <p><strong>Conclusion:</strong> Achieves strong results in speech recognition and translation while preserving speaker identity.</p>
      </div>
      <div class="paper">
        <h3>Wav2vec 2.0: Self-Supervised Speech Representation Learning Framework</h3>
        <p><strong>Analogy:</strong> A detective recognizing conversation patterns without labeled transcripts.</p>
        <p><strong>Novelty & Interest:</strong> Learns speech representations by masking parts of the signal.</p>
        <p><strong>Conclusion:</strong> Sets state-of-the-art performance in speech recognition with limited labeled data.</p>
      </div>
      <div class="paper">
        <h3>IMAGEBIND: A Joint Embedding Across Six Modalities</h3>
        <p><strong>Analogy:</strong> A universal translator aligning six different data types.</p>
        <p><strong>Novelty & Interest:</strong> Trains on image-paired data to enable cross-modal retrieval and generation.</p>
        <p><strong>Conclusion:</strong> Advances cross-modal AI by unifying diverse data sources into a single embedding space.</p>
      </div>
      <div class="paper">
        <h3>SoundStream: An End-to-End Neural Audio Codec</h3>
        <p><strong>Analogy:</strong> A high-performance audio compression system maintaining sound quality.</p>
        <p><strong>Novelty & Interest:</strong> Uses a residual vector quantizer for scalable bitrate compression.</p>
        <p><strong>Conclusion:</strong> Outperforms traditional codecs, enabling real-time audio applications.</p>
      </div>
      <div class="paper">
        <h3>Llama 3: Open Foundation Model for Responsible AGI Development</h3>
        <p><strong>Analogy:</strong> A next-gen supercomputer with built-in safeguards.</p>
        <p><strong>Novelty & Interest:</strong> Expands multimodal capabilities with strong ethical considerations.</p>
        <p><strong>Conclusion:</strong> Sets new benchmarks for safe and responsible AI development.</p>
      </div>
      <div class="paper">
        <h3>ViViT: A Video Vision Transformer</h3>
        <p><strong>Analogy:</strong> Reading a graphic novel where each panel contributes to the narrative.</p>
        <p><strong>Novelty & Interest:</strong> Uses self-attention for spatiotemporal modeling in video.</p>
        <p><strong>Conclusion:</strong> Improves video classification and understanding tasks.</p>
      </div>
      <div class="paper">
        <h3>AudioLM: Language Modeling for High-Quality Audio Generation</h3>
        <p><strong>Analogy:</strong> An artist first sketches a broad outline before adding details.</p>
        <p><strong>Novelty & Interest:</strong> Hierarchical prediction of both semantic and acoustic features.</p>
        <p><strong>Conclusion:</strong> Generates long, coherent, high-quality audio sequences.</p>
      </div>
    </section>
    
    <footer>
      <p>Back to <a href="index.html">Home</a></p>
    </footer>
  </div>
</body>
</html>
