<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>1D Convolutions for Time-Series Data Explained</title>
  <style>
    body {
      font-family: sans-serif;
      max-width: 900px;
      margin: 0 auto;
      padding: 2rem;
      line-height: 1.6;
      color: #333;
    }
    h1, h2, h3, h4 {
      color: #2c3e50; /* Darker shade for better contrast */
      margin-top: 1.5em;
      margin-bottom: 0.5em;
    }
    h1 { font-size: 2.5em; border-bottom: 2px solid #3498db; padding-bottom: 0.3em;}
    h2 { font-size: 2em; border-bottom: 1px solid #bdc3c7; padding-bottom: 0.2em;}
    h3 { font-size: 1.5em; }
    h4 { font-size: 1.2em; color: #555;}

    nav { margin-bottom: 30px; padding: 10px; background: #ecf0f1; border: 1px solid #bdc3c7; border-radius: 4px;}
    nav ul { list-style: none; padding: 0; }
    nav li { display: inline-block; margin-right: 15px; }
    nav a { text-decoration: none; color: #3498db; font-weight: bold;}
    nav a:hover { text-decoration: underline; color: #2980b9;}

    pre {
      background: #f8f9f9; /* Lighter background for code blocks */
      padding: 1rem;
      overflow-x: auto;
      border: 1px solid #e1e4e8; /* Softer border */
      border-left: 4px solid #3498db; /* Accent border */
      border-radius: 4px;
      font-size: 0.9em;
    }
    code {
      font-family: 'SFMono-Regular', Consolas, 'Liberation Mono', Menlo, Courier, monospace;
    }
    /* For inline code */
    p > code, li > code, table td > code, ul ul li > code {
      background: #e8eaed;
      padding: 0.2em 0.4em;
      border-radius: 3px;
      font-size: 0.85em;
    }
    pre code { /* Reset for code inside pre, already handled by pre styling */
        background: none;
        padding: 0;
        font-size: 1em; /* Ensure pre's font size is inherited */
    }
    ul, ol {
        padding-left: 20px;
    }
    li {
        margin-bottom: 0.5em;
    }
    strong {
        color: #2980b9;
    }
    hr {
      border: 0;
      height: 1px;
      background: #bdc3c7;
      margin-top: 2em;
      margin-bottom: 2em;
    }
  </style>
 
  <script type="text/javascript" async
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
  </script>
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [['$','$'], ['\\(','\\)']],
        displayMath: [['$$','$$'], ['\\[','\\]']],
        processEscapes: true
      }
    });
  </script>

  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
  <script>hljs.highlightAll();</script>
</head>
<body>
    <nav>
        <ul>
            <li><a href="#fundamentals">Fundamentals</a></li>
            <li><a href="#padding">Padding & Causality</a></li>
            <li><a href="#dilation">Dilation & Receptive Field</a></li>
            <li><a href="#design">Design Considerations</a></li>
            <li><a href="#normalization">Normalization</a></li>
            <li><a href="#separable-convs">Separable Convolutions</a></li>
            <li><a href="#advanced-arch">Advanced Architectures</a></li>
        </ul>
    </nav>

    <h1 id="fundamentals">1D Convolutions for Time-Series Data</h1>

    <h2>Stride, Receptive Field, and Aliasing</h2>
    <p>
        With a 1D conv layer, <strong>stride</strong> is how many steps you move the filter at each slide along the sequence. A stride of 1 is max resolution; every neighboring point contributes. Anything greater than 1 produces fewer output points, performing downsampling. The <strong>receptive field</strong> is how much of the input each output sees. A larger stride doesn’t change the receptive field of a single filter—that’s set by the kernel size—but it changes the spacing of the outputs. As you stack layers, striding effectively makes the receptive field grow faster because each deeper unit corresponds to bigger chunks of the original signal.
    </p>
    <p>
        A stride > 1 reduces the output points, so there are fewer multiplications, leading to lower latency and memory cost. That’s why striding is often used instead of pooling for downsampling. However, with any downsampling, it risks aliasing.
    </p>
    <p>
        At a sampling rate $f_s$, any frequency content above $f_s/2$ (the Nyquist frequency) folds back into the lower band; that’s aliasing. We typically low-pass filter to attenuate those high frequencies before downsampling.
    </p>
    <p>
        If you stride by $s$, you downsample by $s$. If your original sampling rate is 1kHz, then after a stride of 2 the effective sampling rate is 500 Hz ($f_s/2$), with the new Nyquist frequency being 250 Hz ($f_s/4$). Now, if the input content is above 250 Hz, it will alias unless the convolution filter itself is low-passed before the downsampling. In a CNN, the convolutional filter can act as a crude low-pass filter if it has averaging-like weights, but this is not guaranteed.
    </p>
    <p>
        In the early layers of a CNN, we use a stride of 1 to preserve fine timing. As we go deeper, we can stride more to capture a larger context without exploding the compute cost. If you’re working with sensory data, you use 1D conv layers to extract temporal filters.
    </p>

    <hr>

    <h2 id="padding">Padding and Causality</h2>
    <p>
        When we apply a convolution, the filter slides across the input, and the filter would hang off at the end unless you pad it. If you pass a filter of size $k$, then $L_{out} = L_{in} - k + 1$.
    </p>
    <p>
        With <strong>zero-padding</strong>, typically 'same' padding, you pad both sides with zeros so the output length matches the input length at a stride of 1. Without it, the output shrinks and you might lose information at the edge.
    </p>
    <p>Putting it together:</p>
    $$L_{out} = \frac{(L_{in} + 2P - k)}{s} + 1$$
    
    <h3>Causal Padding</h3>
    <p>
        Typical 'same' padding looks at both backward and forward in time. With <strong>causal padding</strong>, the filter looks only at the current and past inputs. Causal padding pads only the left side. With real-time inference, you can’t peek into the future; causal convolution ensures that predictions at time $t$ depend only on data up to time $t$. If you use future sampling, you might get better performance on an offline batch than you would in real-time.
    </p>
    <p>
        In PyTorch/TensorFlow, when you use `padding = ‘same’`, it pads the kernel to be centered at each output position, so it’s already peeking at the input. When you put $k-1$ zeros at the left of the input, now you’re only looking back.
    </p>
    
    <hr>
    
    <h2 id="dilation">Dilated Causal Convolutions and Receptive Field</h2>
    <p>
        Now, to make causal convolution more powerful, we can use <strong>dilation</strong>, which means skipping points at a rate $d$. When dilation is 1, the RF is just the kernel size. Now if we introduce a dilation > 1, then we can look farther back in time. The total span of the kernel becomes $(k-1)d + 1$.
    </p>
    <p>
        Now let’s stack two causal dilated conv layers. In the first layer, you usually don’t want to dilate, as you don’t want to miss samples in the input by introducing blind spots. When you stack two layers, the new layer sees the entire receptive field of the layer below.
    </p>
    <p>
        Let’s define <strong>jump</strong>: the effective stride in raw input units. Jump is how far apart two adjacent outputs at layer $l$ are in terms of raw input indices. At the input, each step is one sample. At each layer:
    </p>
    $$ \text{jump}_l = \text{jump}_{l-1} \cdot s_l \cdot d_l $$
    <p>
        Because stride skips inputs in layer $l-1$, it multiplies the step size in the raw input. Dilation spaces out taps inside the kernel, which also multiplies the step size in the raw input.
    </p>
    $$ RF_l = RF_{l-1} + (k_l - 1) \cdot \text{jump}_{l-1} $$
    
    <h4>Example: Designing for a 300ms Receptive Field</h4>
    <p>
        Let’s consider sensory data coming in at a <strong>1kHz sampling rate</strong>. Let’s say we know that in our trial window, the activity corresponding to our label is about <strong>300 ms</strong> long. So, we would want our model’s receptive field to see the entire structure.
    </p>
    <p>
        We want to design a CNN model that has a receptive field (RF) greater than 300. We will mix standard convolutions, striding, and dilations. We start with an input receptive field of 1 and a jump of 1.
    </p>
    <ul>
        <li>
            <strong>Layer 1 (Input):</strong>
            <ul>
                <li><code>RF = 1</code></li>
                <li><code>Jump = 1</code></li>
            </ul>
        </li>
        <li>
            <strong>Layer 2 (Conv Block 1):</strong>
            <ul>
                <li>Parameters: <code>Kernel (k)=7</code>, <code>Stride (s)=1</code>, <code>Dilation (d)=1</code></li>
                <li>New Jump: $1 \times 1 \times 1 = \mathbf{1}$</li>
                <li>New RF: $1 + (7-1) \times 1 = \mathbf{7}$</li>
            </ul>
        </li>
        <li>
            <strong>Layer 3 (Conv Block 2):</strong>
            <ul>
                <li>Parameters: <code>k=5</code>, <code>s=2</code>, <code>d=1</code></li>
                <li>New Jump: $1 \times 2 \times 1 = \mathbf{2}$</li>
                <li>New RF: $7 + (5-1) \times 1 = \mathbf{11}$</li>
            </ul>
        </li>
        <li>
            <strong>Layer 4 (Conv Block 3):</strong>
            <ul>
                <li>Parameters: <code>k=3</code>, <code>s=1</code>, <code>d=2</code></li>
                <li>New Jump: $2 \times 1 \times 2 = \mathbf{4}$</li>
                <li>New RF: $11 + (3-1) \times 2 = \mathbf{15}$</li>
            </ul>
        </li>
        <li>
            <strong>Layer 5 (Conv Block 4):</strong>
            <ul>
                <li>Parameters: <code>k=3</code>, <code>s=1</code>, <code>d=4</code></li>
                <li>New Jump: $4 \times 1 \times 4 = \mathbf{16}$</li>
                <li>New RF: $15 + (3-1) \times 4 = \mathbf{23}$</li>
            </ul>
        </li>
        <li>
            <strong>Layer 6 (Conv Block 5):</strong>
            <ul>
                <li>Parameters: <code>k=3</code>, <code>s=1</code>, <code>d=8</code></li>
                <li>New Jump: $16 \times 1 \times 8 = \mathbf{128}$</li>
                <li>New RF: $23 + (3-1) \times 16 = \mathbf{55}$</li>
            </ul>
        </li>
         <li>
            <strong>Layer 7 (Conv Block 6):</strong>
            <ul>
                <li>Parameters: <code>k=3</code>, <code>s=1</code>, <code>d=16</code></li>
                <li>New Jump: $128 \times 1 \times 16 = \mathbf{2048}$</li>
                <li>New RF: $55 + (3-1) \times 128 = \mathbf{311}$</li>
            </ul>
        </li>
    </ul>
    
    <hr>
    
    <h2 id="design">Design Considerations for Sensory Data CNNs</h2>
    <p>
        We have to make some model decisions.
    </p>
    <ul>
        <li><strong>Do we pool early or late?</strong> If we pool early, the sequence length reduces quickly, leading to faster compute and smaller memory, but we could be throwing away fine details. Early pooling can also reduce latency. If we pool late, there’s a good chance to capture those fine details, but it would be more expensive. For noisy, high-frequency signals, you want to do pooling early to tame the sequence length; for low-rate signals, you can postpone it.</li>
        <li><strong>Dilation can help too,</strong> but too much dilation in the early layers can risk holes in the signal and missing local details. This comes at no extra cost, but you want to apply it once features are stabilized.</li>
        <li><strong>Small kernels are typically standard,</strong> as they stack well to approximate large filters. Sometimes, large kernels can be used in the first layer to capture spectral features too.</li>
    </ul>
    
    <h3>Pooling and Final Layers</h3>
    <p>
        Pooling is also a downsampling operation. It summarizes a local region into a single value. Each pooled output corresponds to a wider chunk of the input, increasing the receptive field of later layers. Max pooling is robust to timing jitter, and average pooling smooths out noise.
    </p>
    <p>
        Both pooling and stride downsample time resolution, but stride is a learnable filter, and pooling is a fixed function. With early layers, you want to keep stride 1 and no pooling. With mid-layers, you can start to compress time.
    </p>
    <p>
        The final layers of a CNN model often use <strong>Global Average Pooling (GAP)</strong> across time before classification tasks. This squeezes the sequence dimension and gives you one feature per channel. GAP produces a fixed-size representation regardless of input length. This provides translation invariance in time, where exactly in the window a class occurs matters less than the fact that its pattern exists somewhere. Instead of flattening a $C \times T$ tensor into a huge vector, pooling collapses it to just $C$, which prevents the head from exploding in size.
    </p>
    <p>
        Pooling is not the only way; you can learn a weight per time step and let the model focus on the most informative parts of the signal.
    </p>
    $$ Z = \sum_t(\alpha_t h_t) \quad \text{and} \quad \alpha_t = \text{softmax}(W h_t) $$
    <p>
        We can also feed the CNN features into an LSTM or Transformer to model temporal dependencies. This can either take the last hidden state or perform attention pooling. This can be stronger if the label dynamics matter. Instead of averaging across all channels, you can use Squeeze-and-Excite to learn channel weights before pooling.
    </p>
    
    <hr>
    
    <h2 id="normalization">Normalization for Time-Series Data</h2>
    <p>
        Now let’s talk about normalization. <strong>BatchNorm (BN)</strong> is typically used for images and CNN models; it reduces internal covariate shift and stabilizes training. For time-series signals with long windows, GPU memory is tight, so the batch size might be small. If the signals are non-stationary and drift over time, then BatchNorm can wash out useful information. Let’s talk about this a bit.
    </p>
    <p>
        In images (<code>N, C, H, W</code>), BN computes mean/variance per channel across the batch <code>N</code> and spatial dimensions <code>H, W</code>. For a 1D conv layer and a time-series signal (<code>N, C, T</code>), BN computes per channel across the batch and time. If you normalize a signal with weak and strong amplitudes using BN, both would get mapped to an extreme range of the normalized space; the absolute difference in amplitude is gone. It loses scale cues that indicate a burst was stronger. For images, we don’t care if an image is brighter or darker. For time-series signals, however, amplitude carries information, usually. And at test time, BatchNorm needs batch statistics. If input comes one trial at a time, the running stats may not match the training distribution.
    </p>
    <p>There are other forms of normalization.</p>
    <ul>
        <li><strong>InstanceNorm (IN)</strong>: Normalizes per sample, per channel, across time only. That is, each trial and channel will have its own mean and standard deviation. This removes per-trial baseline shifts. But it also loses absolute amplitude information within a trial. This can be a problem. If you have a non-active trial and an active one, InstanceNorm would center both around zero, making them more similar than they actually are. It really depends on the label here. If amplitude information is important, then these norms will hurt the model.</li>
        <li><strong>LayerNorm (LN)</strong>: Normalizes per sample, across channels at each time step. This works for RNNs and Transformers, small batches, and preserves differences between trials. But if one channel is dominant, normalization would suppress that cue. It’s more helpful for later layers after fusion across channels and is good for stacked LSTMs on top of CNN features.</li>
        <li><strong>GroupNorm (GN)</strong>: Splits channels into groups, then normalizes each group. If you set the number of groups to 1, you get LayerNorm, and if you set it to the total number of channels, you get InstanceNorm. It’s a middle ground between the two.</li>
    </ul>
    
    <hr>

    <h2 id="separable-convs">Depthwise Separable Convolutions in 1D</h2>
    <p>
        The number of parameters in a standard convolution is $k \times C_{in} \times C_{out}$. Each filter mixes time and channels jointly.
    </p>
    <p>
        <strong>Depthwise separable convolution</strong> breaks it into two cheaper steps by setting <code>groups</code> to the number of input channels.
    </p>
    <ol>
        <li>A <strong>depthwise convolution</strong> applies one filter per input channel independently. Each filter has a size of $k \times 1$. The number of parameters is $k \times C_{in}$. It captures temporal patterns within each channel but not cross-channel mixing.</li>
        <li>It is followed by a <strong>pointwise convolution</strong> (a 1x1 convolution). It mixes across channels and captures cross-channel correlations. The number of parameters is $C_{in} \times C_{out}$.</li>
    </ol>
    <p>
        The total number of parameters is $k \times C_{in} + C_{in} \times C_{out}$. The depthwise part learns temporal filters per channel, and then the pointwise part learns how channels interact.
    </p>
    <pre><code class="language-python">
import torch
import torch.nn as nn

class SeparableConv1d(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0):
        super().__init__()
        
        # Depthwise: one conv per channel (groups=in_channels)
        self.depthwise = nn.Conv1d(
            in_channels, 
            in_channels, 
            kernel_size=kernel_size, 
            stride=stride, 
            padding=padding, 
            groups=in_channels,  # <- key!
            bias=False
        )
        
        # Pointwise: 1x1 conv to mix channels
        self.pointwise = nn.Conv1d(
            in_channels, 
            out_channels, 
            kernel_size=1, 
            bias=False
        )

    def forward(self, x):
        x = self.depthwise(x)  # per-channel temporal filtering
        x = self.pointwise(x)  # cross-channel mixing
        return x
    </code></pre>

    <hr>
    
    <h2 id="advanced-arch">Advanced Architectures for Temporal Signals</h2>
    
    <h3>Time-Depth Separable (TDS) Block</h3>
    <p>
        A <strong>Time-Depth Separable (TDS) block</strong> is inspired by depthwise separable convolution and is tailored for temporal signals. It has three main steps:
    </p>
    <ol>
        <li>A time-channel separable convolution (a large convolution along time applied independently to each channel).</li>
        <li>A pointwise convolution that mixes across channels, expanding and then projecting the feature dimensionality back down.</li>
        <li>A residual connection with normalization and dropout to avoid vanishing gradients.</li>
    </ol>
    <p>
        It’s basically a temporal pattern extractor + channel mixer + residual. LayerNorm is used because it normalizes per sample across channels, not across the batch; it respects the time-series structure and doesn’t erase trial-level amplitude differences like BN would.
    </p>
    <pre><code class="language-python">
import torch.nn.functional as F

class TDSBlock(nn.Module):
    """
    Time-Depth Separable (TDS) block for temporal signals (EMG/IMU/Audio).
    """
    def __init__(self, channels, kernel_size=5, dropout=0.1):
        super().__init__()
        
        self.depthwise = nn.Conv1d(
            in_channels=channels,
            out_channels=channels,
            kernel_size=kernel_size,
            groups=channels,
            padding=kernel_size // 2,
            bias=False
        )
        
        self.pointwise1 = nn.Conv1d(channels, channels, kernel_size=1, bias=False)
        self.pointwise2 = nn.Conv1d(channels, channels, kernel_size=1, bias=False)
        
        self.norm = nn.LayerNorm(channels)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x):
        # x: (batch, channels, time)
        residual = x
        
        out = self.depthwise(x)
        out = F.relu(out)
        
        out = self.pointwise1(out)
        out = F.relu(out)
        out = self.pointwise2(out)
        
        out = self.dropout(out)
        out = out + residual
        
        # LayerNorm expects (batch, time, channels)
        out = out.transpose(1, 2)
        out = self.norm(out)
        out = out.transpose(1, 2)
        
        return out
    </code></pre>
    
    <h3>Multi-Scale TDS</h3>
    <p>
        You can use <strong>Multi-Scale TDS blocks</strong>, which are TDS blocks in parallel with increasing temporal scales achieved through downsampling. The network can capture both fine-grained bursts and long-range temporal structure. This gives the network multiple receptive field sizes. Using powers of 2 for downsampling gives exponentially increasing context. We use average pooling, which acts as a low-pass filter before downsampling to prevent aliasing. Max-pooling would throw away too many fine details. We then upsample the output from each block to the original time so it can be used with residual connections.
    </p>
    <pre><code class="language-python">
class MultiScaleTDS(nn.Module):
    """
    Multiscale TDS design (s = 0..5).
    """
    def __init__(self, channels, kernel_size=5, num_scales=6, dropout=0.1):
        super().__init__()
        
        self.scales = nn.ModuleList()
        for s in range(num_scales):
            block = TDSBlock(channels, kernel_size=kernel_size, dropout=dropout)
            self.scales.append(block)
        
        self.num_scales = num_scales

    def forward(self, x):
        # x: (batch, channels, time)
        outputs = []
        T = x.size(-1)
        
        for s, block in enumerate(self.scales):
            # Downsample by 2^s
            pooled = F.avg_pool1d(x, kernel_size=2**s, stride=2**s, ceil_mode=True)
            
            # Apply TDS block
            out = block(pooled)
            
            # Upsample back to original time length
            out = F.interpolate(out, size=T, mode="linear", align_corners=False)
            
            outputs.append(out)
        
        # Fuse multiscale outputs
        return sum(outputs) / self.num_scales
    </code></pre>

</body>
</html>
