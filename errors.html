<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Advanced ML Concepts: Errors, Optimizers, and Scaling Laws</title>
  <style>
    body {
      font-family: sans-serif;
      max-width: 900px;
      margin: 0 auto;
      padding: 2rem;
      line-height: 1.6;
      color: #333;
    }
    h1, h2, h3, h4 {
      color: #2c3e50; /* Darker shade for better contrast */
      margin-top: 1.5em;
      margin-bottom: 0.5em;
    }
    h1 { font-size: 2.5em; border-bottom: 2px solid #3498db; padding-bottom: 0.3em;}
    h2 { font-size: 2em; border-bottom: 1px solid #bdc3c7; padding-bottom: 0.2em;}
    h3 { font-size: 1.5em; }
    h4 { font-size: 1.2em; color: #555;}

    nav { margin-bottom: 30px; padding: 10px; background: #ecf0f1; border: 1px solid #bdc3c7; border-radius: 4px;}
    nav ul { list-style: none; padding: 0; }
    nav li { display: inline-block; margin-right: 15px; }
    nav a { text-decoration: none; color: #3498db; font-weight: bold;}
    nav a:hover { text-decoration: underline; color: #2980b9;}

    pre {
      background: #f8f9f9; /* Lighter background for code blocks */
      padding: 1rem;
      overflow-x: auto;
      border: 1px solid #e1e4e8; /* Softer border */
      border-left: 4px solid #3498db; /* Accent border */
      border-radius: 4px;
      font-size: 0.9em;
    }
    code {
      font-family: 'SFMono-Regular', Consolas, 'Liberation Mono', Menlo, Courier, monospace;
    }
    /* For inline code */
    p > code, li > code, table td > code {
      background: #e8eaed;
      padding: 0.2em 0.4em;
      border-radius: 3px;
      font-size: 0.85em;
    }
    pre code { /* Reset for code inside pre, already handled by pre styling */
        background: none;
        padding: 0;
        font-size: 1em; /* Ensure pre's font size is inherited */
    }
    ul, ol {
        padding-left: 20px;
    }
    li {
        margin-bottom: 0.5em;
    }
    strong {
        color: #2980b9;
    }
    hr {
      border: 0;
      height: 1px;
      background: #bdc3c7;
      margin-top: 2em;
      margin-bottom: 2em;
    }
  </style>
 
  <script type="text/javascript" async
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
  </script>
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [['$','$'], ['\\(','\\)']],
        displayMath: [['$$','$$'], ['\\[','\\]']],
        processEscapes: true
      }
    });
  </script>

  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
  <script>hljs.highlightAll();</script>
</head>
<body>
    <nav>
        <ul>
            <li><a href="#error-analysis">Error Analysis</a></li>
            <li><a href="#evaluation">Evaluation</a></li>
            <li><a href="#generalization">Generalization</a></li>
            <li><a href="#preprocessing">Preprocessing</a></li>
            <li><a href="#regularization">Regularization & Augmentation</a></li>
            <li><a href="#optimizers">Optimizers</a></li>
            <li><a href="#schedulers">Schedulers</a></li>
            <li><a href="#scaling-laws">Scaling Laws</a></li>
        </ul>
    </nav>

    <h1 id="error-analysis">Understanding and Mitigating Errors in Machine Learning</h1>
    <p>
        Even with lots of training data, models don’t perform perfectly. There are three main sources of error that contribute to the total <strong>True Error</strong>.
    </p>
    <p><strong>True Error = Approximation Error + Estimation Error + Optimization Error</strong></p>

    <h3>1. Approximation Error (Bias from Model Class)</h3>
    <p>
        This error is due to the chosen model class not being expressive enough to represent the true underlying function. The model is too weak.
    </p>
    <ul>
        <li><strong>Symptom</strong>: The model is <strong>underfitting</strong>. Both training error and test error are high. The test error plateaus even when more data is added, or the training error doesn't drop significantly even with a more expressive model.</li>
        <li><strong>Solution</strong>: You need a stronger, more complex model.</li>
    </ul>

    <h3>2. Estimation Error (Generalization Gap, Variance)</h3>
    <p>
        This is error that arises from having insufficient data. With a small dataset, the model may overfit to noise and random fluctuations, failing to find the right parameters that generalize.
    </p>
    <ul>
        <li><strong>Symptom</strong>: The model is <strong>overfitting</strong>. The training error is low, but the test error is high, creating a large "generalization gap." The test error improves when more data is added.</li>
        <li><strong>Solution</strong>: You need more data, regularization, or data augmentation. A simpler model can also help.</li>
    </ul>

    <h3>3. Optimization Error</h3>
    <p>
        This error is due to the training algorithm failing to find the best possible parameters within the chosen model class. This can be caused by non-convex loss functions, getting stuck in bad local minima, or navigating saddle points.
    </p>
    <ul>
        <li><strong>Symptom</strong>: The training error decreases too slowly or plateaus early, even with an expressive model.</li>
        <li><strong>Solution</strong>: You need better optimizers or well-tuned learning rate schedules.</li>
    </ul>

    <h3>Irreducible Error (Bayes Error)</h3>
    <p>
        There is one more type of error that you cannot remove, no matter the model, data, or optimizer. Suppose the true relationship between an input $x$ and a target $y$ is:
    </p>
    $$ Y = f(x) + \epsilon $$
    <p>
        The term $\epsilon$ represents noise from natural randomness in the data-generating process. This is the <strong>Bayes error</strong> or <strong>irreducible error</strong>. This error sets the lower bound on the test error. If you keep improving your model and the test error plateaus, you might be hitting this floor.
    </p>
    <p>You know you've likely hit this limit when:</p>
    <ul>
        <li>Training error is low (optimization succeeded).</li>
        <li>Test error is close to training error (low variance).</li>
        <li>Increasing model size or data volume doesn’t dramatically reduce the error.</li>
    </ul>
    <p>
        At this point, the model has learned the signal, and what remains is inherent noise. The only way to potentially reduce this error floor is to introduce data from another modality that can provide complementary signals about the true latent state.
    </p>

    <hr>

    <h2 id="evaluation">Model Evaluation Strategies</h2>
    <p>When it comes to evaluating a model, what comes to mind?</p>
    <ul>
        <li><strong>Accuracy</strong>: Does it classify correctly?</li>
        <li><strong>Latency</strong>: Is it fast enough for real-time use?</li>
        <li><strong>Robustness</strong>: Does it handle new users or noisy sensors?</li>
        <li><strong>Usability</strong>: Does it work for daily scenarios, not just on lab data?</li>
    </ul>
    <p>There are three main testing strategies:</p>
    <ul>
        <li><strong>Offline Testing</strong>: Using cross-validation (within-subject and cross-subject) and measuring standard metrics on a static dataset.</li>
        <li><strong>Online Testing</strong>: Testing the system in a live, streaming mode to measure real-world latency, stability, and false positive rates. What is the throughput? How many errors occur per hour?</li>
        <li><strong>Stress Testing</strong>: Intentionally varying conditions like sensor placement or noise levels. Augmenting data with synthetic perturbations. Is the system able to recover quickly from an error?</li>
    </ul>

    <hr>

    <h2 id="generalization">Generalization, Overfitting, and Adaptation</h2>
    <p>
        <strong>Overfitting</strong> happens when a model learns the training data too well, including its noise and irrelevant patterns, causing it to perform poorly on new, unseen data.
    </p>
    
    <h3>Generalization in the Context of Wearables</h3>
    <p>
        When can you claim a model generalizes? For wearables, training and testing on the <em>same</em> subjects is a much easier problem because the model can memorize a subject's idiosyncrasies. The true test is generalization to new people.
    </p>
    <ul>
        <li><strong>Cross-Subject Generalization</strong>: Train on a set of people and test on an entirely new set.</li>
        <li><strong>Temporal Generalization</strong>: Does a model trained on a subject's data from Day 1 still work on their data from Day 2? Is it robust to time variance?</li>
        <li><strong>Domain Shift</strong>: Does varying the sensor placement or recording conditions cause the model's performance to collapse?</li>
    </ul>
    <p>
        If your model works for some users but not for new users, it has likely overfit to subject-specific patterns. The solution involves data, the model, and adaptation strategies.
    </p>
    <ul>
        <li><strong>Data</strong>: Collect more diverse training data.</li>
        <li><strong>Adaptation</strong>: Efficiently adapt the model to new subjects.
            <ul>
                <li><strong>Fine-tuning</strong>: Freeze most of the model and fine-tune only the final layers.</li>
                <li><strong>Adapter Modules</strong>: Introduce lightweight modules like LoRA or bottleneck layers for efficient fine-tuning.</li>
                <li><strong>Layer-wise Unfreezing</strong>: Start by fine-tuning only the head, and if performance plateaus, unfreeze deeper layers one by one to prevent catastrophic forgetting.</li>
                <li><strong>Regularization</strong>: Use weight regularization to keep personalized weights close to the pre-trained ones.</li>
            </ul>
        </li>
    </ul>

    <hr>
    
    <h2 id="preprocessing">Preprocessing and Data Handling for Sensory Data</h2>
    <p>
        Sensory data from wearables can be noisy due to powerline interference, baseline drifts, or sensor impedance issues. Signal processing can improve the signal-to-noise ratio.
    </p>
    <ul>
        <li><strong>Bandpass filtering</strong> to remove low-frequency drift and high-frequency noise.</li>
        <li><strong>Notch filtering</strong> to suppress powerline interference.</li>
        <li><strong>Rectification and enveloping</strong> by taking the absolute value and then low-pass filtering.</li>
        <li><strong>Z-scoring</strong> per channel or scaling signals to reduce inter-subject amplitude variability.</li>
    </ul>

    <h3>Handling Missing Signals</h3>
    <p>Signals can be missing due to Bluetooth packet loss or sweat increasing sensor impedance.</p>
    <ul>
        <li><strong>Interpolation</strong>: Detect flatlined or saturated channels and interpolate their values from spatial or temporal neighbors.</li>
        <li><strong>Masking</strong>: Make the model aware of noise by providing a binary mask of which sensors are valid, so it learns to ignore missing ones.</li>
        <li><strong>Attention</strong>: Use an attention mechanism to allow the model to learn to weight more reliable sensors higher.</li>
    </ul>
    
    <h3>Fusing Modalities</h3>
    <p>
        If one modality is missing, another may still provide information.
    </p>
    <ul>
        <li><strong>Separate Encoders</strong>: Use separate encoders for each modality and concatenate their latent features before feeding them to a joint decoder.</li>
        <li><strong>Cross-Modal Attention</strong>: Let features from one sensor form the query and features from other sensors form the keys and values. Attention can then learn to synchronize and weight the modalities based on context and reliability.</li>
    </ul>
    
    <hr>

    <h2 id="regularization">Regularization and Data Augmentation</h2>
    
    <h3>Regularization: Dropout</h3>
    <p>
        <strong>Dropout</strong> randomly sets some activations to zero during training with a probability $p$. This prevents the network from relying too heavily on any single neuron. Each forward pass effectively trains a slightly different, "thinned" sub-network. To keep the expected activation value the same during training and testing, the remaining activations are scaled up by a factor of $1/(1-p)$. At inference time, the full network is used without dropping any neurons.
    </p>

    <h3>Data Augmentation for Wearables</h3>
    <p>Beyond adding noise or scaling, we can use realistic augmentations for wearable data:</p>
    <ul>
        <li><strong>Channel Dropout</strong>: Randomly drop entire sensor channels during training to mimic loss of contact.</li>
        <li><strong>Channel Shuffling</strong>: Permute sensors within a local neighborhood to mimic the band shifting along an arm.</li>
        <li><strong>Spatial Mixing</strong>: Replace a sensor's signal with a weighted average of its neighbors to simulate displacement blur.</li>
        <li><strong>Rotation Augmentation</strong>: Apply 3D rotations to sensor data to mimic the device being worn at different angles. A general rotation matrix $R(\alpha, \beta, \gamma) = R_x R_y R_z$ can be applied to each sensor vector $v$ as $v' = v \cdot R^T$.</li>
        <li><strong>Random Scaling</strong>: Multiply signals by random factors to mimic changes in strap tightness or skin impedance.</li>
        <li><strong>Temporal Jitter</strong>: Slightly shift or stretch signals in time.</li>
        <li><strong>Cross-Modal Masking</strong>: Temporarily drop one modality to encourage robustness to partial signal availability.</li>
    </ul>

    <h3>Handling Class Imbalance</h3>
    <p>Class imbalance can cause overfitting to majority classes.</p>
    <ul>
        <li><strong>Resampling</strong>: Oversample minority classes or undersample majority classes. <strong>SMOTE</strong> is a classic technique that oversamples by interpolating between existing minority samples.</li>
        <li><strong>Loss-level Techniques</strong>: Use a <strong>class-weighted loss</strong> to give more importance to minority classes, or a <strong>focal loss</strong> to focus training on hard-to-classify examples.</li>
        <li><strong>Balanced Batch Sampling</strong>: Ensure each batch has a roughly equal representation of all classes.</li>
    </ul>
    
    <hr>

    <h2 id="optimizers">Optimizers</h2>
    
    <h3>Gradient Descent and SGD</h3>
    <p>
        <strong>Gradient Descent</strong> updates the model parameters in the direction that reduces the loss function. <strong>Stochastic Gradient Descent (SGD)</strong> takes a random subset of data (a mini-batch) instead of the entire dataset and performs the update rule based on that gradient.
    </p>
    $$ \theta_{t+1} = \theta_t - \text{learning\_rate} \cdot \nabla_{\theta_t} L(\theta_t) $$
    <p>
        This makes the process noisy but much faster and allows it to escape poor local minima.
    </p>
    <p>
        SGD can be sensitive to the learning rate; if it's too large, the optimization path can zig-zag or diverge, and if it's too small, it can be very slow. Its learning rate is the same across all parameters, so there’s no adaptation. With a constant learning rate, SGD will bounce around near a local minimum. The stochasticity helps it escape <strong>saddle points</strong>, which are points where the gradient is zero but are not a true minimum. High-dimensional loss surfaces are full of saddle points. SGD's noise can shake it loose from these saddles, while full-batch Gradient Descent might get stuck.
    </p>

    <h3>SGD with Momentum</h3>
    <p>
        <strong>Momentum</strong> can smooth out the zig-zag path of SGD by accumulating a running average of past gradients, so updates have a more consistent direction. The update rule with momentum introduces a velocity term, $v_t$:
    </p>
    $$ v_{t+1} = \mu \cdot v_t + \text{learning\_rate} \cdot \nabla_{\theta_t} L $$
    $$ \theta_{t+1} = \theta_t - v_{t+1} $$
    <p>
        Here, $v_t$ is the running average of the gradients and $\mu$ is the momentum coefficient (between 0 and 1, typically 0.9). A higher $\mu$ gives more weight to past gradients. This means you carry more velocity from the past, like rolling downhill with less friction. The velocity builds up, and the magnitude of the step can actually be bigger than with plain SGD. It’s slower to react but converges faster in a consistent direction.
    </p>

    <h3>Nesterov Accelerated Gradient (NAG)</h3>
    <p>
        With <strong>NAG</strong>, you "look ahead" in the direction of the current velocity <em>before</em> you compute the gradient. This helps prevent overshooting the minimum. You pretend to take a step based on your current velocity and then compute the gradient from that future-leaning position.
    </p>
    $$ \theta_{hat} = \theta_t - \mu \cdot v_t $$
    $$ g_t = \nabla L(\theta_{hat}) $$
    <p>The update rule then becomes:</p>
    $$ v_{t+1} = \mu \cdot v_t + \text{learning\_rate} \cdot g_t $$
    $$ \theta_{t+1} = \theta_t - v_{t+1} $$
    <p>
        It basically glances in the direction you’re already moving and then adjusts based on the slope there. It’s like an early warning if the slope is flattening or turning, so you can correct sooner. With vanilla momentum, you say, “I’m here, what’s the slope?” With Nesterov, you say, “I’m about to be over there, what’s the slope? Let me adjust my push so I don’t overshoot.”
    </p>

    <h3>AdaGrad</h3>
    <p>
        The following optimizers introduce <strong>adaptive learning rates</strong>, where each parameter can have its own step size. AdaGrad adapts the learning rate of each parameter based on the history of its gradients. For each parameter $\theta_i$, it maintains an accumulator, $G_t$, which is the cumulative sum of its squared gradients.
    </p>
    $$ G_{t,i} = G_{t-1,i} + (\nabla L_t(\theta_i))^2 $$
    <p>The update then scales the learning rate by this accumulated value.</p>
    $$ \theta_{t+1, i} = \theta_{t, i} - \frac{\text{learning\_rate}}{\sqrt{G_{t,i}} + \epsilon} \cdot \nabla L_t(\theta_i) $$
    <p>
        Parameters that consistently see large gradients will have their effective learning rate shrink, preventing runaway updates. Parameters that see small gradients will have a larger effective learning rate so they can still learn. However, the denominator grows monotonically, so the learning rate shrinks forever and can eventually become infinitesimally small.
    </p>

    <h3>RMSProp</h3>
    <p>
        Instead of a running sum, <strong>RMSProp</strong> uses an exponentially decaying average of squared gradients.
    </p>
    $$ E[g^2]_{t,i} = \rho \cdot E[g^2]_{t-1,i} + (1-\rho) \cdot (\nabla L_t(\theta_i))^2 $$
    $$ \theta_{t+1, i} = \theta_{t,i} - \frac{\text{learning\_rate}}{\sqrt{E[g^2]_{t,i}} + \epsilon} \cdot \nabla L_t(\theta_i) $$
    <p>
        Where $\rho$ is the decay rate (commonly 0.9). Older gradients decay away exponentially, so they don’t dominate forever. Now the denominator tracks the more recent scale of the gradients, not the entire history, allowing the optimizer to adapt more quickly.
    </p>

    <h3>Adam (Adaptive Moment Estimation)</h3>
    <p>
        <strong>Adam</strong> combines the momentum from SGD with the adaptive learning rates from RMSProp and also adds a bias-correction step.
    </p>
    <p>For each parameter $\theta$ at step $t$, we first get the gradient $g_t = \nabla L_t$.</p>
    <p>Then we compute the first moment (the mean of gradients, like momentum):</p>
    $$ m_t = \beta_1 m_{t-1} + (1-\beta_1)g_t $$
    <p>And the second moment (the mean of squared gradients, like RMSProp):</p>
    $$ v_t = \beta_2 v_{t-1} + (1-\beta_2)g_t^2 $$
    <p>Because these moving averages are initialized at zero, they are biased towards zero at the beginning of training. Adam corrects for this bias:</p>
    $$ \hat{m}_t = \frac{m_t}{1 - \beta_1^t} $$
    $$ \hat{v}_t = \frac{v_t}{1 - \beta_2^t} $$
    <p>The update rule then uses these corrected moments:</p>
    $$ \theta_{t+1} = \theta_t - \text{learning\_rate} \cdot \frac{\hat{m}_t}{\sqrt{\hat{v}_t} + \epsilon} $$

    <h3>A Detour on L1 and L2 Regularization</h3>
    <p>To prevent overfitting, we can add a penalty to the loss function to discourage very large weights.</p>
    <ul>
        <li><strong>L2 Regularization</strong>: Adds a penalty proportional to the squared magnitude of the weights.
            $$ L_{reg}(\theta) = L(\theta) + \frac{\lambda}{2} ||\theta||^2 $$
            <p>The optimizer is pushed to keep weights small and spread out. The constraint $||\theta||^2 < C$ is a ball or circle. L2 produces <strong>shrinkage</strong>, where weights are spread out but generally non-zero.</p>
        </li>
        <li><strong>L1 Regularization</strong>: Uses the sum of the absolute values of the weights.
            $$ L_{reg}(\theta) = L(\theta) + \lambda ||\theta||_1 $$
            <p>Its constraint $|\theta_1| + |\theta_2| < C$ looks like a diamond shape. This encourages the optimizer to find solutions at the "corners" where some weights are exactly zero. That’s why L1 produces <strong>sparsity</strong>.</p>
        </li>
    </ul>

    <h3>AdamW (Adam with Weight Decay)</h3>
    <p>
        In standard implementations, L2 regularization is added <em>inside</em> the gradient calculation.
    </p>
    $$ g_t = \nabla L_t + \lambda \theta_t $$
    <p>
        With adaptive optimizers like Adam, this means the L2 penalty term gets included in the second moment estimate ($v_t$), distorting the adaptive scaling.
    </p>
    <p>
        <strong>AdamW</strong> decouples the weight decay from the gradient update. It keeps a moving average of only the true gradients (without the L2 term) and applies the regularization penalty to the update separately at the end.
    </p>
    $$ \theta_{t+1} = \theta_t - \text{learning\_rate} \cdot \frac{\hat{m}_t}{\sqrt{\hat{v}_t} + \epsilon} - \text{learning\_rate} \cdot \lambda \cdot \theta_t $$
    
    <h3>Nadam (Nesterov-accelerated Adam)</h3>
    <p>
        <strong>Nadam</strong> blends Nesterov's look-ahead momentum with the Adam optimizer. The update becomes:
    </p>
    $$ \theta_{t+1} = \theta_t - \frac{\text{learning\_rate}}{\sqrt{\hat{v}_t} + \epsilon} \cdot \left(\beta_1 \hat{m}_t + \frac{(1-\beta_1)g_t}{1-\beta_1^t}\right) $$
    
    <h3>SAM (Sharpness-Aware Minimization)</h3>
    <p>
        <strong>SAM</strong> is a wrapper that can be used with another optimizer (like AdamW). Instead of finding the point of lowest loss, it finds parameters that lie in a "flat" minima, where the loss remains low even if the weights are slightly perturbed. It seeks to solve the following min-max problem:
    </p>
    $$ \min_{\theta} \max_{||\epsilon|| \le \rho} L(\theta + \epsilon) $$
    <p>
        It first finds the worst-case perturbation $\epsilon$ in a small neighborhood and then updates the weights $\theta$ to reduce that worst-case loss.
    </p>

    <hr>
    
    <h2 id="schedulers">Learning Rate Schedulers</h2>
    <p>A constant learning rate is rarely optimal. Schedulers adapt the learning rate over time.</p>
    <ul>
        <li><strong>Step Decay</strong>: Drops the learning rate by a fixed factor every few epochs.</li>
        <li><strong>Exponential Decay</strong>: Smoothly shrinks the learning rate at every step.</li>
        <li><strong>Cosine Annealing</strong>: Makes the learning rate follow a cosine curve, starting high, decreasing smoothly to a minimum, and potentially restarting in cycles. This keeps the learning rate higher for longer, encouraging exploration.</li>
        <li><strong>One-Cycle Policy</strong>: Increases the learning rate for the first part of training and then decreases it for the second part. This can help the model escape bad minima early and settle into a good solution later.</li>
    </ul>
    <p>
        <strong>Gradient Clipping</strong> is another technique that prevents exploding gradients by capping their magnitude (norm) at a certain threshold.
    </p>

    <hr>

    <h2 id="scaling-laws">Scaling Laws</h2>
    <p>
        This brings us to the conclusion, <strong>Scaling Laws</strong>. Scaling laws describe how performance (usually test loss) improves as you increase data size, model size, or compute. You want to know if you should invest in more data collection, a bigger model, or maybe if the bottleneck is the optimization error. Scaling laws help you decide where the marginal gain is coming from.
    </p>
    <p>
        The following equation is a common one known as the <strong>power law</strong>. This has been found empirically across many domains.
    </p>
    $$ L(N) = L_{\infty} + K \cdot N^{-\alpha} $$
    <p>Where:</p>
    <ul>
        <li><code>L(N)</code> is the loss when trained on a dataset of size N.</li>
        <li><code>L_inf</code> is the irreducible error.</li>
        <li><code>K</code> and <code>alpha</code> are constants. <code>alpha</code> is a scaling component. These you estimate from fitting a line.</li>
    </ul>
    <p>
        But, wait, how do you estimate $L_{\infty}$? You can train your model on multiple dataset sizes and record the test loss. Then plot $L(N) - L_{\infty}$ vs $N$ in a log-log space. You adjust $L_{\infty}$ until the curve is a straight line, then fit the slope, which will be $-\alpha$. If adding more data improves test loss, you haven’t hit $L_{\infty}$ yet. If the curve starts to flatten, you’re approaching $L_{\infty}$.
    </p>
    <p>
        Once you have fit the line correctly, you know three things: the irreducible error, alpha (which tells you how fast error falls when you add more data/parameters), and a constant factor. If the irreducible error is high, that means your modality has a limit. If alpha is large (e.g., 0.5), then doubling the data will give you a noticeable gain, and if it’s small, not so much. You can also predict performance as you scale data.
    </p>
    <p>The gap above the floor is $G(N) = L(N) - L_{\infty} = K \cdot N^{-\alpha}$. After doubling the data, the remaining gap is:</p>
    $$ G(2N) = G(N) \cdot 2^{-\alpha} $$
    <p>
        You can apply the same equation to model size (#params) or compute. When you plot loss vs. dataset size on a log-log plot, you often get a straight line. That line’s slope tells you how much benefit you would get from scaling that resource.
    </p>
    <p>There are also joint scaling laws:</p>
    $$ L(N,P) \approx L_{\infty} + K \cdot N^{-\alpha} + C \cdot P^{-\beta} $$
    <p>Where $P$ is for the number of parameters. This will let you trade off between being data-limited and model-limited.</p>
    
    <h3>A Detour into Line Fitting</h3>
    <p>
        This might be obvious, but how do you fit a line or a surface? This is actually a nice gateway to optimizers. Say you have a list of data points and you want to fit a line $y = ax+b$ with parameters $\theta = (a, b)$.
    </p>
    <p>
        The residual error is $r_i = y_i - f(x_i, \theta)$. <strong>Least Squares</strong> chooses parameters to minimize the sum of squared errors:
    </p>
    $$ \min_{\theta} \sum_{i=1}^{n} (y_i - f(x_i, \theta))^2 $$
    <p>For a linear model, we can solve this with a closed-form solution by taking the partial derivatives with respect to $a$ and $b$, setting them to zero, and solving the normal equations.</p>
    $$ \frac{\partial J}{\partial a} = -2 \sum_{i=1}^{n} (x_i(y_i - ax_i - b)) = 0 $$
    $$ \frac{\partial J}{\partial b} = -2 \sum_{i=1}^{n} (y_i - (ax_i + b)) = 0 $$
    <p>Solving for $a$ and $b$ gives:</p>
    $$ a = \frac{\sum(x_i - \bar{x})(y_i - \bar{y})}{\sum(x_i - \bar{x})^2} = \frac{\text{cov}(x,y)}{\text{var}(x)} $$
    $$ b = \bar{y} - a\bar{x} $$
    <p>Where $\bar{y}$ is the mean of all y's and $\bar{x}$ is the mean of all x's.</p>

</body>
</html>
