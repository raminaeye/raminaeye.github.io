<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Knowledge Distillation Explained with PyTorch</title>
  <style>
    body {
      font-family: sans-serif;
      max-width: 900px;
      margin: 0 auto;
      padding: 2rem;
      line-height: 1.6;
      color: #333;
    }
    h1, h2, h3, h4 {
      color: #2c3e50; /* Darker shade for better contrast */
      margin-top: 1.5em;
      margin-bottom: 0.5em;
    }
    h1 { font-size: 2.5em; border-bottom: 2px solid #3498db; padding-bottom: 0.3em;}
    h2 { font-size: 2em; border-bottom: 1px solid #bdc3c7; padding-bottom: 0.2em;}
    h3 { font-size: 1.5em; }
    h4 { font-size: 1.2em; color: #555;}

    nav { margin-bottom: 30px; padding: 10px; background: #ecf0f1; border: 1px solid #bdc3c7; border-radius: 4px;}
    nav ul { list-style: none; padding: 0; }
    nav li { display: inline-block; margin-right: 15px; }
    nav a { text-decoration: none; color: #3498db; font-weight: bold;}
    nav a:hover { text-decoration: underline; color: #2980b9;}

    pre {
      background: #f8f9f9; /* Lighter background for code blocks */
      padding: 1rem;
      overflow-x: auto;
      border: 1px solid #e1e4e8; /* Softer border */
      border-left: 4px solid #3498db; /* Accent border */
      border-radius: 4px;
      font-size: 0.9em;
    }
    code {
      font-family: 'SFMono-Regular', Consolas, 'Liberation Mono', Menlo, Courier, monospace;
    }
    /* For inline code */
    p > code, li > code, table td > code {
      background: #e8eaed;
      padding: 0.2em 0.4em;
      border-radius: 3px;
      font-size: 0.85em;
    }
    pre code { /* Reset for code inside pre, already handled by pre styling */
        background: none;
        padding: 0;
        font-size: 1em; /* Ensure pre's font size is inherited */
    }
    ul {
        padding-left: 20px;
    }
    li {
        margin-bottom: 0.5em;
    }
    strong {
        color: #2980b9;
    }
  </style>
  
  <script type="text/javascript" async
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
  </script>
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [['$','$'], ['\\(','\\)']],
        displayMath: [['$$','$$'], ['\\[','\\]']],
        processEscapes: true
      }
    });
  </script>

  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/monokai.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
  <script>hljs.highlightAll();</script>
</head>
<body>
    <nav>
     <ul>
       <li><a href="index.html">Home</a></li> 
     </ul>
   </nav>
    <h1>Transformers Explained</h1>

    <h3><strong>Self-Attention</strong></h3>
    <p>
        At their core, transformers are powered by a self-attention mechanism. Self-attention allows each token in a sequence to look at other tokens and decide what is important. For instance, when parsing "ramin writes the nicest tutorials," a model might need to pay more attention to "nicest" when processing "tutorials," depending on the task.
    </p>
    <p>
        This is achieved using three vectors for each token: a <strong>Query (Q)</strong>, a <strong>Key (K)</strong>, and a <strong>Value (V)</strong>.
    </p>
    <ul>
        <li><strong>Query (Q)</strong>: What the current token is looking for.</li>
        <li><strong>Key (K)</strong>: What the token is offering.</li>
        <li><strong>Value (V)</strong>: What the token will pass on if selected.</li>
    </ul>
    <p>
        These vectors are computed by multiplying the input matrix, $X$, by learnable weight matrices: $W_q$, $W_k$, and $W_v$.
    </p>
    <pre><code>Q = X @ W_q  # Shape: (seq_len, d_k)
K = X @ W_k  # Shape: (seq_len, d_k)
V = X @ W_v  # Shape: (seq_len, d_v)</code></pre>
    <p>
        The dot product between a query vector $Q_i$ and a key vector $K_j$ produces a score indicating how much token <em>i</em> should attend to token <em>j</em>.
    </p>
    <p>$$ \text{Score} = QK^T $$</p>
    <p>
        These scores are then scaled to prevent large dot products, and a softmax function is applied to normalize them into attention weights that sum to 1.
    </p>
    <p>$$ \text{Attention Score} = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right) $$</p>
    <p>
        Finally, the output for each token is a weighted average of all other tokens' Value vectors, based on the calculated attention scores.
    </p>
    <p>$$ \text{Output} = \text{Attention Score} \cdot V $$</p>
    <p>
        For example, with the input "Cats Are Bossy":
    </p>
    <ol>
        <li>Each token ("Cats", "Are", "Bossy") gets its own Q, K, and V vector.</li>
        <li>The attention weights for "Cats" are calculated: <code>Attention weights_1 = softmax(Q_1 @ [K1, K2, K3])</code></li>
        <li>The final output for "Cats" is a blend of all token values, weighted by attention: <code>Output_1 = Attention weights_1 @ [V1, V2, V3]</code></li>
    </ol>
    
    <hr>
    
    <h3><strong>Code Example: Self-Attention</strong></h3>
<pre><code class="language-python">import numpy as np

# ---- Step 1: Toy input (3 tokens, embed_dim = 4) ----
X = np.array([
    [1.0, 0.0, 1.0, 0.0],  # token 1
    [0.0, 2.0, 0.0, 2.0],  # token 2
    [1.0, 1.0, 1.0, 1.0],  # token 3
])  # shape: (3, 4)

seq_len, embed_dim = X.shape
d_k = d_v = 4  # We'll keep the same dim for simplicity

# ---- Step 2: Random weight matrices (learnable in real models) ----
W_q = np.random.randn(embed_dim, d_k)
W_k = np.random.randn(embed_dim, d_k)
W_v = np.random.randn(embed_dim, d_v)

# ---- Step 3: Compute Q, K, V ----
Q = X @ W_q  # shape: (3, 4)
K = X @ W_k
V = X @ W_v

# ---- Step 4: Compute attention scores ----
scores = Q @ K.T / np.sqrt(d_k)  # shape: (3, 3)

# ---- Step 5: Softmax over rows ----
def softmax(x):
    # subtracting a constant to avoid overflow
    x_exp = np.exp(x - np.max(x, axis=-1, keepdims=True))
    return x_exp / np.sum(x_exp, axis=-1, keepdims=True)

attn_weights = softmax(scores)  # shape: (3, 3)

# ---- Step 6: Weighted sum over V ----
output = attn_weights @ V  # shape: (3, 4)</code></pre>
    
    <hr>
    
    <h3><strong>Multi-Head Attention (MHA)</strong></h3>
    <p>
        A single self-attention mechanism, or "head," captures only one type of relationship at a time. <strong>Multi-Head Attention</strong> runs multiple attention heads in parallel, each with its own set of learned projections. This allows the model to attend to different aspects of the input simultaneously.
    </p>
    <p>
        Instead of one set of weights, you have <em>h</em> heads, each computing its own output. These outputs are then concatenated and projected back to the original embedding dimension using an output weight matrix.
    </p>
<pre><code class="language-python">def multi_head_attention(X, Wq, Wk, Wv, Wo, h):
    batch_size, seq_len, embed_dim = X.shape
    d_k = embed_dim // h

    # 1. Linear projections for all heads
    Q = X @ Wq  # shape: (batch, seq_len, embed_dim)
    K = X @ Wk
    V = X @ Wv

    # 2. Split into h heads
    # Reshape and transpose to bring the head dimension forward
    Q = Q.reshape(batch_size, seq_len, h, d_k).transpose(0, 2, 1, 3)
    K = K.reshape(batch_size, seq_len, h, d_k).transpose(0, 2, 1, 3)
    V = V.reshape(batch_size, seq_len, h, d_k).transpose(0, 2, 1, 3)

    # 3. Scaled dot-product attention for each head (vectorized)
    scores = Q @ K.transpose(0, 1, 3, 2) / np.sqrt(d_k)
    attn_weights = softmax(scores)
    heads = attn_weights @ V  # (batch, h, seq_len, d_k)

    # 4. Concatenate heads
    heads_concat = heads.transpose(0, 2, 1, 3).reshape(batch_size, seq_len, embed_dim)

    # 5. Final linear projection
    output = heads_concat @ Wo  # (batch, seq_len, embed_dim)
    return output</code></pre>

    <hr>
    
    <h3><strong>Transformer Encoder Block</strong></h3>
    <p>
        Each transformer encoder block contains a multi-head attention layer followed by a position-wise <strong>Feed-Forward Network (FFN)</strong>. Residual connections and layer normalization are applied after each of these two sub-layers.
    </p>
    <ol>
        <li><strong>Multi-Head Attention</strong>: Mixes information between tokens.</li>
        <li><strong>Add & Norm</strong>: <code>x = LayerNorm(x + attn_out)</code></li>
        <li><strong>Feed-Forward Network</strong>: A two-layer MLP applied to each token independently.<br><code>FFN(x) = Linear(ReLU(Linear(x)))</code></li>
        <li><strong>Add & Norm</strong>: <code>x = LayerNorm(x + ffn_out)</code></li>
    </ol>
    <p>
        The FFN adds capacity to the model by applying the same non-linear transformation to each token separately, acting like a token-wise filter.
    </p>
<pre><code class="language-python">def encoder_block(x, mha_params, ffn_params):
    # Unpack params
    Wq, Wk, Wv, Wo = mha_params
    W1, b1, W2, b2 = ffn_params

    # 1. Multi-head attention
    attn_out = multi_head_attention(x, Wq, Wk, Wv, Wo, h=8)

    # 2. Residual + LayerNorm
    x = layer_norm(x + attn_out)

    # 3. Feedforward
    ffn_out = feedforward(x, W1, b1, W2, b2)

    # 4. Residual + LayerNorm
    x = layer_norm(x + ffn_out)

    return x</code></pre>

    <hr>
    
    <h3><strong>Positional Encoding</strong></h3>
    <p>
        Self-attention is permutation-invariant, meaning it doesn't inherently know the order of tokens. To address this, we inject positional information into the input embeddings using <strong>Positional Encoding</strong>. A popular method is Sinusoidal Positional Encoding, which uses sine and cosine functions.
    </p>
    <p>$$ PE_{(pos, 2i)} = \sin\left(\frac{pos}{10000^{2i/d_{\text{model}}}}\right) $$</p>
    <p>$$ PE_{(pos, 2i+1)} = \cos\left(\frac{pos}{10000^{2i/d_{\text{model}}}}\right) $$</p>
    <p>
        This encoding is added to the input embeddings (<code>x = x + PE</code>). Since closer positions have similar encodings, the model can learn to compute relative positions from these patterns.
    </p>
<pre><code class="language-python">def positional_encoding(seq_len, d_model):
    """
    Returns a (seq_len, d_model) positional encoding matrix.
    """
    pos = np.arange(seq_len)[:, np.newaxis]      # (T, 1)
    i = np.arange(d_model)[np.newaxis, :]       # (1, D)

    angle_rates = 1 / np.power(10000, (2 * (i // 2)) / d_model) # (1, D)
    angle_rads = pos * angle_rates                             # (T, D)

    # Apply sin to even indices; cos to odd indices
    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])
    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])

    return angle_rads # shape: (seq_len, d_model)

pe = positional_encoding(seq_len=10, d_model=16)</code></pre>
    <p>
        A typical encoder consists of multiple encoder blocks stacked on top of each other.
    </p>
<pre><code>x = input + positional_encoding
for _ in range(N):
    x = EncoderBlock(x)</code></pre>
    
    <hr>
    
    <h3><strong>Transformer Decoder Block</strong></h3>
    <p>
        The decoder's role is to generate the output sequence. It has a similar structure to the encoder but with two key differences:
    </p>
    <ol>
        <li><strong>Masked Self-Attention</strong>: Prevents the decoder from "cheating" by looking at future tokens during training. It only attends to previous and current positions.</li>
        <li><strong>Cross-Attention</strong>: The decoder attends to the output of the encoder.</li>
    </ol>
    
    <h4><strong>Masked Self-Attention</strong></h4>
    <p>
        Masking works by zeroing out the attention scores for all positions $j > i$ when calculating the attention for token $i$. This is often done by setting future scores to a very small number (like -1e9) before the softmax, effectively making their weights zero.
    </p>
<pre><code class="language-python">def masked_self_attention(x, W_q, W_k, W_v):
    B, T, D = x.shape
    d_k = D

    Q = x @ W_q
    K = x @ W_k
    V = x @ W_v

    scores = Q @ K.transpose(0, 2, 1) / np.sqrt(d_k)

    # Causal mask
    causal_mask = np.tril(np.ones((T, T))).astype(bool)
    scores = np.where(causal_mask[None, :, :], scores, -1e9)

    weights = softmax(scores) # axis=-1 is default
    out = weights @ V  # (B, T, D)
    return out</code></pre>
    
    <h4><strong>Cross-Attention</strong></h4>
    <p>
        Cross-attention is like standard attention, but the <strong>Query (Q)</strong> vectors come from the decoder, while the <strong>Key (K)</strong> and <strong>Value (V)</strong> vectors come from the encoder's output. This allows each decoder token to ask a "question" (Q) and find the most relevant information from the entire input sequence (K, V).
    </p>
<pre><code class="language-python">def cross_attention(x_dec, x_enc, W_q, W_k, W_v):
    # x_dec: (B, T_dec, D)
    # x_enc: (B, T_enc, D)
    
    # Linear projections
    Q = x_dec @ W_q  # (B, T_dec, D)
    K = x_enc @ W_k  # (B, T_enc, D)
    V = x_enc @ W_v  # (B, T_enc, D)
    
    # Compute attention scores
    scores = Q @ np.transpose(K, (0, 2, 1)) / np.sqrt(K.shape[-1])
    
    # Softmax over encoder tokens
    weights = softmax(scores) # (B, T_dec, T_enc)
    
    # Weighted sum
    output = weights @ V # (B, T_dec, D)
    return output, weights</code></pre>
    <p>A full decoder block combines these elements.</p>
<pre><code class="language-python">def decoder_block(x, enc_out, params):
    # 1. Masked Self-Attention
    x = layer_norm(x + masked_self_attention(x))

    # 2. Cross-Attention (uses encoder output)
    x = layer_norm(x + cross_attention(query=x, key=enc_out, value=enc_out))

    # 3. Feedforward
    x = layer_norm(x + feedforward(x))
    return x</code></pre>

    <hr>
    
    <h3><strong>Full Transformer Architecture</strong></h3>
    <p>
        The full transformer connects the encoder and decoder.
    </p>
    <ul>
        <li><strong>Input</strong>:
            <ul>
                <li><code>x_enc</code> (B, T_enc, D): The source sequence.</li>
                <li><code>x_dec</code> (B, T_dec, D): The target sequence, shifted right.</li>
            </ul>
        </li>
        <li><strong>Encoder</strong>:
            <ul>
                <li><code>x_enc</code> -> <code>EncoderBlock</code> -> <code>enc_out</code></li>
            </ul>
        </li>
        <li><strong>Decoder</strong>:
            <ul>
                <li><code>x_dec</code> + <code>enc_out</code> -> <code>DecoderBlock</code> -> <code>output</code></li>
            </ul>
        </li>
    </ul>
    <p>
        The final decoder output is passed through a linear layer and a softmax to produce logits, which are probability distributions over the vocabulary for predicting the next token.
    </p>
<pre><code class="language-python">class Transformer:
    def __init__(self, num_layers, d_model, d_ff, vocab_size):
        self.num_layers = num_layers
        self.d_model = d_model
        self.vocab_size = vocab_size

        # Encoder and Decoder blocks
        self.encoders = [EncoderBlock(d_model, d_ff) for _ in range(num_layers)]
        self.decoders = [DecoderBlock(d_model, d_ff) for _ in range(num_layers)]

        # Final projection to logits
        self.W_out = np.random.randn(d_model, vocab_size)
        self.b_out = np.zeros(vocab_size)

    def forward(self, src, tgt):
        # Encoder pass
        x = src
        for layer in self.encoders:
            x = layer.forward(x)
        enc_out = x

        # Decoder pass
        y = tgt
        for layer in self.decoders:
            y = layer.forward(y, enc_out)

        # Final output
        logits = y @ self.W_out + self.b_out
        return logits</code></pre>

    <script>
        document.addEventListener("DOMContentLoaded", function() {
            renderMathInElement(document.body, {
                delimiters: [
                    {left: "$$", right: "$$", display: true},
                    {left: "$", right: "$", display: false},
                    {left: "\\(", right: "\\)", display: false},
                    {left: "\\[", right: "\\]", display: true}
                ]
            });
        });
    </script>
</body>
</html>
