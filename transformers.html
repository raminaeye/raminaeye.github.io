<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Transformers Explained</title>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css" integrity="sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV" crossorigin="anonymous">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js" integrity="sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8" crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js" integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous"></script>
    <style>
        body {
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif, "Apple Color Emoji", "Segoe UI Emoji", "Segoe UI Symbol";
            line-height: 1.6;
            color: #333;
            max-width: 800px;
            margin: 20px auto;
            padding: 0 20px;
        }
        h1, h2, h3 {
            color: #111;
            border-bottom: 1px solid #eaecef;
            padding-bottom: .3em;
        }
        h3 {
             margin-top: 2em;
        }
        code {
            font-family: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, Courier, monospace;
            font-size: 0.9em;
            background-color: #f6f8fa;
            border-radius: 3px;
            padding: 0.2em 0.4em;
        }
        pre {
            background-color: #f6f8fa;
            padding: 16px;
            overflow: auto;
            border-radius: 6px;
        }
        pre code {
            padding: 0;
            background-color: transparent;
            border-radius: 0;
        }
        hr {
            height: .25em;
            padding: 0;
            margin: 24px 0;
            background-color: #e1e4e8;
            border: 0;
        }
        strong {
            font-weight: 600;
        }
    </style>
</head>
<body>
    <nav>
     <ul>
       <li><a href="index.html">Home</a></li> 
     </ul>
   </nav>
    <h1>Transformers Explained</h1>

    <h3><strong>Self-Attention</strong></h3>
    <p>
        At their core, transformers are powered by a self-attention mechanism. Self-attention allows each token in a sequence to look at other tokens and decide what is important. For instance, when parsing "ramin writes the nicest tutorials," a model might need to pay more attention to "nicest" when processing "tutorials," depending on the task.
    </p>
    <p>
        This is achieved using three vectors for each token: a <strong>Query (Q)</strong>, a <strong>Key (K)</strong>, and a <strong>Value (V)</strong>.
    </p>
    <ul>
        <li><strong>Query (Q)</strong>: What the current token is looking for.</li>
        <li><strong>Key (K)</strong>: What the token is offering.</li>
        <li><strong>Value (V)</strong>: What the token will pass on if selected.</li>
    </ul>
    <p>
        These vectors are computed by multiplying the input matrix, $X$, by learnable weight matrices: $W_q$, $W_k$, and $W_v$.
    </p>
    <pre><code>Q = X @ W_q  # Shape: (seq_len, d_k)
K = X @ W_k  # Shape: (seq_len, d_k)
V = X @ W_v  # Shape: (seq_len, d_v)</code></pre>
    <p>
        The dot product between a query vector $Q_i$ and a key vector $K_j$ produces a score indicating how much token <em>i</em> should attend to token <em>j</em>.
    </p>
    <p>$$ \text{Score} = QK^T $$</p>
    <p>
        These scores are then scaled to prevent large dot products, and a softmax function is applied to normalize them into attention weights that sum to 1.
    </p>
    <p>$$ \text{Attention Score} = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right) $$</p>
    <p>
        Finally, the output for each token is a weighted average of all other tokens' Value vectors, based on the calculated attention scores.
    </p>
    <p>$$ \text{Output} = \text{Attention Score} \cdot V $$</p>
    <p>
        For example, with the input "Cats Are Bossy":
    </p>
    <ol>
        <li>Each token ("Cats", "Are", "Bossy") gets its own Q, K, and V vector.</li>
        <li>The attention weights for "Cats" are calculated: <code>Attention weights_1 = softmax(Q_1 @ [K1, K2, K3])</code></li>
        <li>The final output for "Cats" is a blend of all token values, weighted by attention: <code>Output_1 = Attention weights_1 @ [V1, V2, V3]</code></li>
    </ol>
    
    <hr>
    
    <h3><strong>Code Example: Self-Attention</strong></h3>
<pre><code class="language-python">import numpy as np

# ---- Step 1: Toy input (3 tokens, embed_dim = 4) ----
X = np.array([
    [1.0, 0.0, 1.0, 0.0],  # token 1
    [0.0, 2.0, 0.0, 2.0],  # token 2
    [1.0, 1.0, 1.0, 1.0],  # token 3
])  # shape: (3, 4)

seq_len, embed_dim = X.shape
d_k = d_v = 4  # We'll keep the same dim for simplicity

# ---- Step 2: Random weight matrices (learnable in real models) ----
W_q = np.random.randn(embed_dim, d_k)
W_k = np.random.randn(embed_dim, d_k)
W_v = np.random.randn(embed_dim, d_v)

# ---- Step 3: Compute Q, K, V ----
Q = X @ W_q  # shape: (3, 4)
K = X @ W_k
V = X @ W_v

# ---- Step 4: Compute attention scores ----
scores = Q @ K.T / np.sqrt(d_k)  # shape: (3, 3)

# ---- Step 5: Softmax over rows ----
def softmax(x):
    # subtracting a constant to avoid overflow
    x_exp = np.exp(x - np.max(x, axis=-1, keepdims=True))
    return x_exp / np.sum(x_exp, axis=-1, keepdims=True)

attn_weights = softmax(scores)  # shape: (3, 3)

# ---- Step 6: Weighted sum over V ----
output = attn_weights @ V  # shape: (3, 4)</code></pre>
    
    <hr>
    
    <h3><strong>Multi-Head Attention (MHA)</strong></h3>
    <p>
        A single self-attention mechanism, or "head," captures only one type of relationship at a time. <strong>Multi-Head Attention</strong> runs multiple attention heads in parallel, each with its own set of learned projections. This allows the model to attend to different aspects of the input simultaneously.
    </p>
    <p>
        Instead of one set of weights, you have <em>h</em> heads, each computing its own output. These outputs are then concatenated and projected back to the original embedding dimension using an output weight matrix.
    </p>
<pre><code class="language-python">def multi_head_attention(X, Wq, Wk, Wv, Wo, h):
    batch_size, seq_len, embed_dim = X.shape
    d_k = embed_dim // h

    # 1. Linear projections for all heads
    Q = X @ Wq  # shape: (batch, seq_len, embed_dim)
    K = X @ Wk
    V = X @ Wv

    # 2. Split into h heads
    # Reshape and transpose to bring the head dimension forward
    Q = Q.reshape(batch_size, seq_len, h, d_k).transpose(0, 2, 1, 3)
    K = K.reshape(batch_size, seq_len, h, d_k).transpose(0, 2, 1, 3)
    V = V.reshape(batch_size, seq_len, h, d_k).transpose(0, 2, 1, 3)

    # 3. Scaled dot-product attention for each head (vectorized)
    scores = Q @ K.transpose(0, 1, 3, 2) / np.sqrt(d_k)
    attn_weights = softmax(scores)
    heads = attn_weights @ V  # (batch, h, seq_len, d_k)

    # 4. Concatenate heads
    heads_concat = heads.transpose(0, 2, 1, 3).reshape(batch_size, seq_len, embed_dim)

    # 5. Final linear projection
    output = heads_concat @ Wo  # (batch, seq_len, embed_dim)
    return output</code></pre>

    <hr>
    
    <h3><strong>Transformer Encoder Block</strong></h3>
    <p>
        Each transformer encoder block contains a multi-head attention layer followed by a position-wise <strong>Feed-Forward Network (FFN)</strong>. Residual connections and layer normalization are applied after each of these two sub-layers.
    </p>
    <ol>
        <li><strong>Multi-Head Attention</strong>: Mixes information between tokens.</li>
        <li><strong>Add & Norm</strong>: <code>x = LayerNorm(x + attn_out)</code></li>
        <li><strong>Feed-Forward Network</strong>: A two-layer MLP applied to each token independently.<br><code>FFN(x) = Linear(ReLU(Linear(x)))</code></li>
        <li><strong>Add & Norm</strong>: <code>x = LayerNorm(x + ffn_out)</code></li>
    </ol>
    <p>
        The FFN adds capacity to the model by applying the same non-linear transformation to each token separately, acting like a token-wise filter.
    </p>
<pre><code class="language-python">def encoder_block(x, mha_params, ffn_params):
    # Unpack params
    Wq, Wk, Wv, Wo = mha_params
    W1, b1, W2, b2 = ffn_params

    # 1. Multi-head attention
    attn_out = multi_head_attention(x, Wq, Wk, Wv, Wo, h=8)

    # 2. Residual + LayerNorm
    x = layer_norm(x + attn_out)

    # 3. Feedforward
    ffn_out = feedforward(x, W1, b1, W2, b2)

    # 4. Residual + LayerNorm
    x = layer_norm(x + ffn_out)

    return x</code></pre>

    <hr>
    
    <h3><strong>Positional Encoding</strong></h3>
    <p>
        Self-attention is permutation-invariant, meaning it doesn't inherently know the order of tokens. To address this, we inject positional information into the input embeddings using <strong>Positional Encoding</strong>. A popular method is Sinusoidal Positional Encoding, which uses sine and cosine functions.
    </p>
    <p>$$ PE_{(pos, 2i)} = \sin\left(\frac{pos}{10000^{2i/d_{\text{model}}}}\right) $$</p>
    <p>$$ PE_{(pos, 2i+1)} = \cos\left(\frac{pos}{10000^{2i/d_{\text{model}}}}\right) $$</p>
    <p>
        This encoding is added to the input embeddings (<code>x = x + PE</code>). Since closer positions have similar encodings, the model can learn to compute relative positions from these patterns.
    </p>
<pre><code class="language-python">def positional_encoding(seq_len, d_model):
    """
    Returns a (seq_len, d_model) positional encoding matrix.
    """
    pos = np.arange(seq_len)[:, np.newaxis]      # (T, 1)
    i = np.arange(d_model)[np.newaxis, :]       # (1, D)

    angle_rates = 1 / np.power(10000, (2 * (i // 2)) / d_model) # (1, D)
    angle_rads = pos * angle_rates                             # (T, D)

    # Apply sin to even indices; cos to odd indices
    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])
    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])

    return angle_rads # shape: (seq_len, d_model)

pe = positional_encoding(seq_len=10, d_model=16)</code></pre>
    <p>
        A typical encoder consists of multiple encoder blocks stacked on top of each other.
    </p>
<pre><code>x = input + positional_encoding
for _ in range(N):
    x = EncoderBlock(x)</code></pre>
    
    <hr>
    
    <h3><strong>Transformer Decoder Block</strong></h3>
    <p>
        The decoder's role is to generate the output sequence. It has a similar structure to the encoder but with two key differences:
    </p>
    <ol>
        <li><strong>Masked Self-Attention</strong>: Prevents the decoder from "cheating" by looking at future tokens during training. It only attends to previous and current positions.</li>
        <li><strong>Cross-Attention</strong>: The decoder attends to the output of the encoder.</li>
    </ol>
    
    <h4><strong>Masked Self-Attention</strong></h4>
    <p>
        Masking works by zeroing out the attention scores for all positions $j > i$ when calculating the attention for token $i$. This is often done by setting future scores to a very small number (like -1e9) before the softmax, effectively making their weights zero.
    </p>
<pre><code class="language-python">def masked_self_attention(x, W_q, W_k, W_v):
    B, T, D = x.shape
    d_k = D

    Q = x @ W_q
    K = x @ W_k
    V = x @ W_v

    scores = Q @ K.transpose(0, 2, 1) / np.sqrt(d_k)

    # Causal mask
    causal_mask = np.tril(np.ones((T, T))).astype(bool)
    scores = np.where(causal_mask[None, :, :], scores, -1e9)

    weights = softmax(scores) # axis=-1 is default
    out = weights @ V  # (B, T, D)
    return out</code></pre>
    
    <h4><strong>Cross-Attention</strong></h4>
    <p>
        Cross-attention is like standard attention, but the <strong>Query (Q)</strong> vectors come from the decoder, while the <strong>Key (K)</strong> and <strong>Value (V)</strong> vectors come from the encoder's output. This allows each decoder token to ask a "question" (Q) and find the most relevant information from the entire input sequence (K, V).
    </p>
<pre><code class="language-python">def cross_attention(x_dec, x_enc, W_q, W_k, W_v):
    # x_dec: (B, T_dec, D)
    # x_enc: (B, T_enc, D)
    
    # Linear projections
    Q = x_dec @ W_q  # (B, T_dec, D)
    K = x_enc @ W_k  # (B, T_enc, D)
    V = x_enc @ W_v  # (B, T_enc, D)
    
    # Compute attention scores
    scores = Q @ np.transpose(K, (0, 2, 1)) / np.sqrt(K.shape[-1])
    
    # Softmax over encoder tokens
    weights = softmax(scores) # (B, T_dec, T_enc)
    
    # Weighted sum
    output = weights @ V # (B, T_dec, D)
    return output, weights</code></pre>
    <p>A full decoder block combines these elements.</p>
<pre><code class="language-python">def decoder_block(x, enc_out, params):
    # 1. Masked Self-Attention
    x = layer_norm(x + masked_self_attention(x))

    # 2. Cross-Attention (uses encoder output)
    x = layer_norm(x + cross_attention(query=x, key=enc_out, value=enc_out))

    # 3. Feedforward
    x = layer_norm(x + feedforward(x))
    return x</code></pre>

    <hr>
    
    <h3><strong>Full Transformer Architecture</strong></h3>
    <p>
        The full transformer connects the encoder and decoder.
    </p>
    <ul>
        <li><strong>Input</strong>:
            <ul>
                <li><code>x_enc</code> (B, T_enc, D): The source sequence.</li>
                <li><code>x_dec</code> (B, T_dec, D): The target sequence, shifted right.</li>
            </ul>
        </li>
        <li><strong>Encoder</strong>:
            <ul>
                <li><code>x_enc</code> -> <code>EncoderBlock</code> -> <code>enc_out</code></li>
            </ul>
        </li>
        <li><strong>Decoder</strong>:
            <ul>
                <li><code>x_dec</code> + <code>enc_out</code> -> <code>DecoderBlock</code> -> <code>output</code></li>
            </ul>
        </li>
    </ul>
    <p>
        The final decoder output is passed through a linear layer and a softmax to produce logits, which are probability distributions over the vocabulary for predicting the next token.
    </p>
<pre><code class="language-python">class Transformer:
    def __init__(self, num_layers, d_model, d_ff, vocab_size):
        self.num_layers = num_layers
        self.d_model = d_model
        self.vocab_size = vocab_size

        # Encoder and Decoder blocks
        self.encoders = [EncoderBlock(d_model, d_ff) for _ in range(num_layers)]
        self.decoders = [DecoderBlock(d_model, d_ff) for _ in range(num_layers)]

        # Final projection to logits
        self.W_out = np.random.randn(d_model, vocab_size)
        self.b_out = np.zeros(vocab_size)

    def forward(self, src, tgt):
        # Encoder pass
        x = src
        for layer in self.encoders:
            x = layer.forward(x)
        enc_out = x

        # Decoder pass
        y = tgt
        for layer in self.decoders:
            y = layer.forward(y, enc_out)

        # Final output
        logits = y @ self.W_out + self.b_out
        return logits</code></pre>

    <script>
        document.addEventListener("DOMContentLoaded", function() {
            renderMathInElement(document.body, {
                delimiters: [
                    {left: "$$", right: "$$", display: true},
                    {left: "$", right: "$", display: false},
                    {left: "\\(", right: "\\)", display: false},
                    {left: "\\[", right: "\\]", display: true}
                ]
            });
        });
    </script>
</body>
</html>
