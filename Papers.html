<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>What I'm Reading | Ramin Anushiravani</title>
  <style>
    body { 
      font-family: Arial, sans-serif; 
      margin: 0; 
      padding: 0; 
      background-color: #f4f4f4; 
      line-height: 1.6;
      color: #333;
    }
    .container { 
      max-width: 900px; 
      margin: 20px auto; 
      background: #fff; 
      padding: 20px; 
      border-radius: 8px; 
      box-shadow: 0px 0px 10px rgba(0,0,0,0.1);
    }
    header { text-align: center; padding-bottom: 20px; border-bottom: 1px solid #ddd; }
    nav { text-align: center; margin-top: 10px; }
    nav a { margin: 0 10px; text-decoration: none; color: #0073e6; font-weight: bold; }
    h2 { border-bottom: 2px solid #0073e6; padding-bottom: 5px; }
    ul { list-style-type: none; padding: 0; }
    ul li { margin: 10px 0; }
    .paper { margin-bottom: 20px; }
    footer { text-align: center; margin-top: 40px; font-size: 0.9em; color: #777; }
  </style>
</head>
<body>
  <nav>
      <a href="index.html">Home</a>
  </nav>
  <div class="container">
    <header>
      <h1>What I'm Reading</h1>

    </header>

    <section class="mb-10 text-lg text-gray-700 bg-white p-6 rounded-lg shadow-md">
            <p>These are some of the papers I've been reading and listening to (via NotebookLM). I've included small snippets of these papers that I summarized using LLMs.</p>
    </section>

    <section id="papers" class="mb-10">
        <h2 class="text-3xl font-semibold text-gray-900 mb-6 border-b-2 border-blue-500 pb-2">Papers</h2>
        <div class="space-y-8">
            <article class="bg-white p-6 rounded-lg shadow-lg hover:shadow-xl transition-shadow duration-300">
                <h3 class="text-2xl font-semibold text-blue-700 mb-3">
                    <a href="https://scholar.google.com/scholar?q=Open-Set+Sound+Event+Classification+using+Self-Supervised+Learning" target="_blank" rel="noopener noreferrer" class="card-title-link">
                        Open-Set Sound Event Classification using Self-Supervised Learning
                    </a>
                </h3>
                <p class="mb-2"><strong class="text-gray-700">Analogy:</strong> A security system that recognizes known faces and flags unknown faces.</p>
                <p class="mb-2"><strong class="text-gray-700">Novelty & Interest:</strong> Deep learning approach mapping known sounds while detecting unseen audio events using self-supervised techniques.</p>
                <p class="mb-2"><strong class="text-gray-700">Conclusion:</strong> Blends center loss and supervised contrastive loss for enhanced recognition of familiar and novel sound events.</p>
                <p class="text-sm text-gray-600"><strong class="text-gray-700">Citation:</strong> (Open-Set Sound Event Classification using Self-Supervised Learning)</p>
            </article>

            <article class="bg-white p-6 rounded-lg shadow-lg hover:shadow-xl transition-shadow duration-300">
                <h3 class="text-2xl font-semibold text-blue-700 mb-3">
                    <a href="https://scholar.google.com/scholar?q=Show+and+Tell:+A+Neural+Image+Caption+Generator" target="_blank" rel="noopener noreferrer" class="card-title-link">
                        Show and Tell: A Neural Image Caption Generator
                    </a>
                </h3>
                <p class="mb-2"><strong class="text-gray-700">Analogy:</strong> A bilingual tour guide describing a landmark in detail.</p>
                <p class="mb-2"><strong class="text-gray-700">Novelty & Interest:</strong> Combines CNN (image vision) with RNN (storytelling) to bridge computer vision and natural language processing.</p>
                <p class="mb-2"><strong class="text-gray-700">Conclusion:</strong> Generates fluent, accurate image captions.</p>
                <p class="text-sm text-gray-600"><strong class="text-gray-700">Citation:</strong> (Show and Tell: A Neural Image Caption Generator)</p>
            </article>

            <article class="bg-white p-6 rounded-lg shadow-lg hover:shadow-xl transition-shadow duration-300">
                <h3 class="text-2xl font-semibold text-blue-700 mb-3">
                    <a href="https://scholar.google.com/scholar?q=VideoBERT:+A+Joint+Model+for+Video+and+Language+Representation+Learning" target="_blank" rel="noopener noreferrer" class="card-title-link">
                        VideoBERT: A Joint Model for Video and Language Representation Learning
                    </a>
                </h3>
                <p class="mb-2"><strong class="text-gray-700">Analogy:</strong> A film editor aligning visual frames with textual context.</p>
                <p class="mb-2"><strong class="text-gray-700">Novelty & Interest:</strong> Adapts BERT to video by transforming video frames into tokens and training with masked prediction tasks.</p>
                <p class="mb-2"><strong class="text-gray-700">Conclusion:</strong> Leads to richer video representations for tasks like captioning and generation.</p>
                <p class="text-sm text-gray-600"><strong class="text-gray-700">Citation:</strong> (VideoBERT: A Joint Model for Video and Language Representation Learning)</p>
            </article>

            <article class="bg-white p-6 rounded-lg shadow-lg hover:shadow-xl transition-shadow duration-300">
                <h3 class="text-2xl font-semibold text-blue-700 mb-3">
                    <a href="https://scholar.google.com/scholar?q=LLaMA:+Open+and+Efficient+Foundation+Language+Models" target="_blank" rel="noopener noreferrer" class="card-title-link">
                        LLaMA: Open and Efficient Foundation Language Models
                    </a>
                </h3>
                <p class="mb-2"><strong class="text-gray-700">Analogy:</strong> Building a library from public books instead of rare manuscripts.</p>
                <p class="mb-2"><strong class="text-gray-700">Novelty & Interest:</strong> Trains on massive public datasets, challenging the status quo of relying on exclusive data.</p>
                <p class="mb-2"><strong class="text-gray-700">Conclusion:</strong> Democratizes access to advanced language modeling.</p>
                <p class="text-sm text-gray-600"><strong class="text-gray-700">Citation:</strong> (LLaMA: Open and Efficient Foundation Language Models)</p>
            </article>

            <article class="bg-white p-6 rounded-lg shadow-lg hover:shadow-xl transition-shadow duration-300">
                <h3 class="text-2xl font-semibold text-blue-700 mb-3">
                    <a href="https://scholar.google.com/scholar?q=Conformer:+Convolution-augmented+Transformer+for+Speech+Recognition" target="_blank" rel="noopener noreferrer" class="card-title-link">
                        Conformer: Convolution-augmented Transformer for Speech Recognition
                    </a>
                </h3>
                <p class="mb-2"><strong class="text-gray-700">Analogy:</strong> A musician reading both the overall score and fine details of each note.</p>
                <p class="mb-2"><strong class="text-gray-700">Novelty & Interest:</strong> Interleaves self-attention with convolutional layers to capture long-range and local dependencies in audio data.</p>
                <p class="mb-2"><strong class="text-gray-700">Conclusion:</strong> State-of-the-art performance on benchmarks like LibriSpeech.</p>
                <p class="text-sm text-gray-600"><strong class="text-gray-700">Citation:</strong> (Conformer: Convolution-augmented Transformer for Speech Recognition)</p>
            </article>

            <article class="bg-white p-6 rounded-lg shadow-lg hover:shadow-xl transition-shadow duration-300">
                <h3 class="text-2xl font-semibold text-blue-700 mb-3">
                    <a href="https://scholar.google.com/scholar?q=AudioPaLM:+Speech+and+Text+Multimodal+Language+Model" target="_blank" rel="noopener noreferrer" class="card-title-link">
                        AudioPaLM: Speech and Text Multimodal Language Model
                    </a>
                </h3>
                <p class="mb-2"><strong class="text-gray-700">Analogy:</strong> A multilingual storyteller switching between spoken word and written text.</p>
                <p class="mb-2"><strong class="text-gray-700">Novelty & Interest:</strong> Leverages pre-trained text models alongside innovative audio tokenization to bridge two modalities.</p>
                <p class="mb-2"><strong class="text-gray-700">Conclusion:</strong> Impressive results in speech recognition and translation.</p>
                <p class="text-sm text-gray-600"><strong class="text-gray-700">Citation:</strong> (AudioPaLM: Speech and Text Multimodal Language Model)</p>
            </article>

            <article class="bg-white p-6 rounded-lg shadow-lg hover:shadow-xl transition-shadow duration-300">
                <h3 class="text-2xl font-semibold text-blue-700 mb-3">
                    <a href="https://scholar.google.com/scholar?q=Wav2vec+2.0:+Self-Supervised+Speech+Representation+Learning+Framework" target="_blank" rel="noopener noreferrer" class="card-title-link">
                        Wav2vec 2.0: Self-Supervised Speech Representation Learning Framework
                    </a>
                </h3>
                <p class="mb-2"><strong class="text-gray-700">Analogy:</strong> A detective recognizing patterns in conversations without labeled transcripts.</p>
                <p class="mb-2"><strong class="text-gray-700">Novelty & Interest:</strong> Masks speech signal and trains the model to predict missing pieces.</p>
                <p class="mb-2"><strong class="text-gray-700">Conclusion:</strong> State-of-the-art speech recognition performance with limited labeled data.</p>
                <p class="text-sm text-gray-600"><strong class="text-gray-700">Citation:</strong> (wav2vec 2.0: Self-Supervised Speech Representation Learning Framework)</p>
            </article>

            <article class="bg-white p-6 rounded-lg shadow-lg hover:shadow-xl transition-shadow duration-300">
                <h3 class="text-2xl font-semibold text-blue-700 mb-3">
                    <a href="https://scholar.google.com/scholar?q=GAMA:+Audio-Language+Model+with+Reasoning+Abilities" target="_blank" rel="noopener noreferrer" class="card-title-link">
                        GAMA: Audio-Language Model with Reasoning Abilities
                    </a>
                </h3>
                <p class="mb-2"><strong class="text-gray-700">Analogy:</strong> An expert panel listening to a conversation and reasoning through its implications.</p>
                <p class="mb-2"><strong class="text-gray-700">Novelty & Interest:</strong> Integrates an audio-specific transformer with a large language model and custom instruction-tuning dataset.</p>
                <p class="mb-2"><strong class="text-gray-700">Conclusion:</strong> Outperforms existing models in audio understanding and reasoning.</p>
                <p class="text-sm text-gray-600"><strong class="text-gray-700">Citation:</strong> (GAMA: Audio-Language Model with Reasoning Abilities)</p>
            </article>

            <article class="bg-white p-6 rounded-lg shadow-lg hover:shadow-xl transition-shadow duration-300">
                <h3 class="text-2xl font-semibold text-blue-700 mb-3">
                    <a href="https://scholar.google.com/scholar?q=Video-LLaMA:+Audio-Visual+Language+Model+for+Video+Understanding" target="_blank" rel="noopener noreferrer" class="card-title-link">
                        Video-LLaMA: Audio-Visual Language Model for Video Understanding
                    </a>
                </h3>
                <p class="mb-2"><strong class="text-gray-700">Analogy:</strong> A film critic watching and listening to understand a movie's story.</p>
                <p class="mb-2"><strong class="text-gray-700">Novelty & Interest:</strong> Employs specialized modules to capture temporal changes and multimodal interactions in video data.</p>
                <p class="mb-2"><strong class="text-gray-700">Conclusion:</strong> Enhanced video understanding by integrating visual and auditory signals.</p>
                <p class="text-sm text-gray-600"><strong class="text-gray-700">Citation:</strong> (Video-LLaMA: Audio-Visual Language Model for Video Understanding)</p>
            </article>

            <article class="bg-white p-6 rounded-lg shadow-lg hover:shadow-xl transition-shadow duration-300">
                <h3 class="text-2xl font-semibold text-blue-700 mb-3">
                    <a href="https://scholar.google.com/scholar?q=IMAGEBIND:+A+Joint+Embedding+Across+Six+Modalities" target="_blank" rel="noopener noreferrer" class="card-title-link">
                        IMAGEBIND: A Joint Embedding Across Six Modalities
                    </a>
                </h3>
                <p class="mb-2"><strong class="text-gray-700">Analogy:</strong> A universal translator aligning six different languages.</p>
                <p class="mb-2"><strong class="text-gray-700">Novelty & Interest:</strong> Achieves emergent alignment across modalities by training on image-paired data.</p>
                <p class="mb-2"><strong class="text-gray-700">Conclusion:</strong> Outperforms many specialized models by harmonizing diverse data sources.</p>
                <p class="text-sm text-gray-600"><strong class="text-gray-700">Citation:</strong> (IMAGEBIND: A Joint Embedding Across Six Modalities)</p>
            </article>

            <article class="bg-white p-6 rounded-lg shadow-lg hover:shadow-xl transition-shadow duration-300">
                <h3 class="text-2xl font-semibold text-blue-700 mb-3">
                    <a href="https://scholar.google.com/scholar?q=Moshi:+Speech-Text+Foundation+Model+for+Real-Time+Dialogue" target="_blank" rel="noopener noreferrer" class="card-title-link">
                        Moshi: Speech-Text Foundation Model for Real-Time Dialogue
                    </a>
                </h3>
                <p class="mb-2"><strong class="text-gray-700">Analogy:</strong> A live translator processing and speaking back instantly.</p>
                <p class="mb-2"><strong class="text-gray-700">Novelty & Interest:</strong> Processes speech directly to speech, reducing latency and preserving nuance.</p>
                <p class="mb-2"><strong class="text-gray-700">Conclusion:</strong> Enables more natural and effective real-time dialogue.</p>
                <p class="text-sm text-gray-600"><strong class="text-gray-700">Citation:</strong> (Moshi: Speech-Text Foundation Model for Real-Time Dialogue)</p>
            </article>

            <article class="bg-white p-6 rounded-lg shadow-lg hover:shadow-xl transition-shadow duration-300">
                <h3 class="text-2xl font-semibold text-blue-700 mb-3">
                    <a href="https://scholar.google.com/scholar?q=Pretrained+Audio+Neural+Networks+for+Audio+Pattern+Recognition" target="_blank" rel="noopener noreferrer" class="card-title-link">
                        Pretrained Audio Neural Networks for Audio Pattern Recognition
                    </a>
                </h3>
                <p class="mb-2"><strong class="text-gray-700">Analogy:</strong> A musician internalizing sound patterns to identify them quickly.</p>
                <p class="mb-2"><strong class="text-gray-700">Novelty & Interest:</strong> Trained on AudioSet, these CNN architectures excel in audio tagging and transfer learning.</p>
                <p class="mb-2"><strong class="text-gray-700">Conclusion:</strong> Validates that PANNs can generalize well to diverse audio applications.</p>
                <p class="text-sm text-gray-600"><strong class="text-gray-700">Citation:</strong> (Pretrained Audio Neural Networks for Audio Pattern Recognition)</p>
            </article>

            <article class="bg-white p-6 rounded-lg shadow-lg hover:shadow-xl transition-shadow duration-300">
                <h3 class="text-2xl font-semibold text-blue-700 mb-3">
                    <a href="https://scholar.google.com/scholar?q=SoundStream:+An+End-to-End+Neural+Audio+Codec" target="_blank" rel="noopener noreferrer" class="card-title-link">
                        SoundStream: An End-to-End Neural Audio Codec
                    </a>
                </h3>
                <p class="mb-2"><strong class="text-gray-700">Analogy:</strong> A high-performance compression algorithm packing a symphony into a small digital package.</p>
                <p class="mb-2"><strong class="text-gray-700">Novelty & Interest:</strong> Uses a fully convolutional network with a residual vector quantizer for scalable bitrate compression.</p>
                <p class="mb-2"><strong class="text-gray-700">Conclusion:</strong> Superior audio compression and enhancement.</p>
                <p class="text-sm text-gray-600"><strong class="text-gray-700">Citation:</strong> (SoundStream: An End-to-End Neural Audio Codec)</p>
            </article>

            <article class="bg-white p-6 rounded-lg shadow-lg hover:shadow-xl transition-shadow duration-300">
                <h3 class="text-2xl font-semibold text-blue-700 mb-3">
                    <a href="https://scholar.google.com/scholar?q=Llama+2:+Open+Foundation+and+Fine-Tuned+Chat+Models" target="_blank" rel="noopener noreferrer" class="card-title-link">
                        Llama 2: Open Foundation and Fine-Tuned Chat Models
                    </a>
                </h3>
                <p class="mb-2"><strong class="text-gray-700">Analogy:</strong> An upgraded smartphone with better hardware and software.</p>
                <p class="mb-2"><strong class="text-gray-700">Novelty & Interest:</strong> Incorporates advancements in pretraining and fine-tuning to boost conversational quality and safety.</p>
                <p class="mb-2"><strong class="text-gray-700">Conclusion:</strong> Impressive benchmark performances and safer, more engaging interactions.</p>
                <p class="text-sm text-gray-600"><strong class="text-gray-700">Citation:</strong> (Llama 2: Open Foundation and Fine-Tuned Chat Models)</p>
            </article>

            <article class="bg-white p-6 rounded-lg shadow-lg hover:shadow-xl transition-shadow duration-300">
                <h3 class="text-2xl font-semibold text-blue-700 mb-3">
                    <a href="https://scholar.google.com/scholar?q=Llama+3:+Open+Foundation+Model+for+Responsible+AGI+Development" target="_blank" rel="noopener noreferrer" class="card-title-link">
                        Llama 3: Open Foundation Model for Responsible AGI Development
                    </a>
                </h3>
                <p class="mb-2"><strong class="text-gray-700">Analogy:</strong> A next-generation supercomputer with built-in safeguards and multilingual support.</p>
                <p class="mb-2"><strong class="text-gray-700">Novelty & Interest:</strong> Emphasizes responsible development, integrating multimodal capabilities and rigorous safety measures.</p>
                <p class="mb-2"><strong class="text-gray-700">Conclusion:</strong> Sets a new standard for large-scale foundation models.</p>
                <p class="text-sm text-gray-600"><strong class="text-gray-700">Citation:</strong> (Llama 3: Open Foundation Model for Responsible AGI Development)</p>
            </article>

            <article class="bg-white p-6 rounded-lg shadow-lg hover:shadow-xl transition-shadow duration-300">
                <h3 class="text-2xl font-semibold text-blue-700 mb-3">
                    <a href="https://scholar.google.com/scholar?q=Towards+Audio+Language+Modeling+-+an+Overview" target="_blank" rel="noopener noreferrer" class="card-title-link">
                        Towards Audio Language Modeling - an Overview
                    </a>
                </h3>
                <p class="mb-2"><strong class="text-gray-700">Analogy:</strong> A comprehensive travel guide surveying every possible route.</p>
                <p class="mb-2"><strong class="text-gray-700">Novelty & Interest:</strong> Systematically compares methods for tokenizing audio and adapting language modeling techniques.</p>
                <p class="mb-2"><strong class="text-gray-700">Conclusion:</strong> Lays a strong foundation for future research.</p>
                <p class="text-sm text-gray-600"><strong class="text-gray-700">Citation:</strong> (Towards Audio Language Modeling - an Overview)</p>
            </article>

            <article class="bg-white p-6 rounded-lg shadow-lg hover:shadow-xl transition-shadow duration-300">
                <h3 class="text-2xl font-semibold text-blue-700 mb-3">
                    <a href="https://scholar.google.com/scholar?q=AudioLM:+Language+Modeling+for+High-Quality+Audio+Generation" target="_blank" rel="noopener noreferrer" class="card-title-link">
                        AudioLM: Language Modeling for High-Quality Audio Generation
                    </a>
                </h3>
                <p class="mb-2"><strong class="text-gray-700">Analogy:</strong> An artist sketching a broad outline and then adding intricate details.</p>
                <p class="mb-2"><strong class="text-gray-700">Novelty & Interest:</strong> Hierarchical approach allows for the generation of long, coherent audio sequences.</p>
                <p class="mb-2"><strong class="text-gray-700">Conclusion:</strong> Produces high-quality, natural-sounding audio continuations.</p>
                <p class="text-sm text-gray-600"><strong class="text-gray-700">Citation:</strong> (AudioLM: Language Modeling for High-Quality Audio Generation)</p>
            </article>

            <article class="bg-white p-6 rounded-lg shadow-lg hover:shadow-xl transition-shadow duration-300">
                <h3 class="text-2xl font-semibold text-blue-700 mb-3">
                    <a href="https://scholar.google.com/scholar?q=Audiobox+Aesthetics:+Automatic+Audio+Quality+Assessment+with+Refined+Metrics" target="_blank" rel="noopener noreferrer" class="card-title-link">
                        Audiobox Aesthetics: Automatic Audio Quality Assessment with Refined Metrics
                    </a>
                </h3>
                <p class="mb-2"><strong class="text-gray-700">Analogy:</strong> A gourmet food critic assessing multiple dimensions.</p>
                <p class="mb-2"><strong class="text-gray-700">Novelty & Interest:</strong> Introduces refined metrics and annotation guidelines to assess audio quality.</p>
                <p class="mb-2"><strong class="text-gray-700">Conclusion:</strong> Paves the way for enhancing audio generation using refined aesthetic metrics.</p>
                <p class="text-sm text-gray-600"><strong class="text-gray-700">Citation:</strong> (Audiobox Aesthetics: Automatic Audio Quality Assessment with Refined Metrics)</p>
            </article>

            <article class="bg-white p-6 rounded-lg shadow-lg hover:shadow-xl transition-shadow duration-300">
                <h3 class="text-2xl font-semibold text-blue-700 mb-3">
                    <a href="https://arxiv.org/abs/2304.07193" target="_blank" rel="noopener noreferrer" class="card-title-link">
                        DINOv2: Learning Robust Visual Features without Supervision
                    </a>
                </h3>
                <p class="mb-2"><strong class="text-gray-700">Analogy:</strong> Training an artist who learns details without being told what to look for.</p>
                <p class="mb-2"><strong class="text-gray-700">Novelty & Interest:</strong> Leverages self-supervised learning to extract robust visual features.</p>
                <p class="mb-2"><strong class="text-gray-700">Conclusion:</strong> Self-supervised methods can achieve high-quality feature extraction.</p>
                <p class="text-sm text-gray-600"><strong class="text-gray-700">Citation:</strong> (DINOv2: Learning Robust Visual Features without Supervision, <a href="https://arxiv.org/abs/2304.07193" target="_blank" rel="noopener noreferrer" class="content-link">arXiv:2304.07193</a>)</p>
            </article>

            <article class="bg-white p-6 rounded-lg shadow-lg hover:shadow-xl transition-shadow duration-300">
                <h3 class="text-2xl font-semibold text-blue-700 mb-3">
                    <a href="https://arxiv.org/abs/2304.06906" target="_blank" rel="noopener noreferrer" class="card-title-link">
                        Swin3D: A Pretrained Transformer Backbone for 3D Indoor Scene Understanding
                    </a>
                </h3>
                <p class="mb-2"><strong class="text-gray-700">Analogy:</strong> Constructing a detailed 3D blueprint by stitching together small scans.</p>
                <p class="mb-2"><strong class="text-gray-700">Novelty & Interest:</strong> Extends the transformer architecture to 3D data, capturing spatial relationships.</p>
                <p class="mb-2"><strong class="text-gray-700">Conclusion:</strong> Establishes a new baseline for transformer-based approaches in 3D indoor scene analysis.</p>
                <p class="text-sm text-gray-600"><strong class="text-gray-700">Citation:</strong> (Swin3D: A Pretrained Transformer Backbone for 3D Indoor Scene Understanding, <a href="https://arxiv.org/abs/2304.06906" target="_blank" rel="noopener noreferrer" class="content-link">arXiv:2304.06906</a>)</p>
            </article>

            <article class="bg-white p-6 rounded-lg shadow-lg hover:shadow-xl transition-shadow duration-300">
                <h3 class="text-2xl font-semibold text-blue-700 mb-3">
                    <a href="https://arxiv.org/abs/1905.11946" target="_blank" rel="noopener noreferrer" class="card-title-link">
                        EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks
                    </a>
                </h3>
                <p class="mb-2"><strong class="text-gray-700">Analogy:</strong> Picture a finely tuned machine where every component is optimized to work in perfect harmony—each adjustment leads to better overall efficiency and performance.</p>
                <p class="mb-2"><strong class="text-gray-700">Novelty & Interest:</strong> EfficientNet introduces a novel compound scaling method that uniformly scales network depth, width, and input resolution. This balanced approach allows the creation of a family of models that deliver state-of-the-art accuracy while using significantly fewer parameters and less computation compared to traditional CNNs.</p>
                <p class="mb-2"><strong class="text-gray-700">Conclusion:</strong> The method has redefined best practices in model scaling, setting new performance benchmarks on image recognition tasks and influencing a generation of efficient neural architectures.</p>
                <p class="text-sm text-gray-600"><strong class="text-gray-700">Citation:</strong> (EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks, <a href="https://arxiv.org/abs/1905.11946" target="_blank" rel="noopener noreferrer" class="content-link">arXiv:1905.11946</a>)</p>
            </article>

            <article class="bg-white p-6 rounded-lg shadow-lg hover:shadow-xl transition-shadow duration-300">
                <h3 class="text-2xl font-semibold text-blue-700 mb-3">
                    <a href="https://arxiv.org/abs/2103.15691" target="_blank" rel="noopener noreferrer" class="card-title-link">
                        ViViT: A Video Vision Transformer
                    </a>
                </h3>
                <p class="mb-2"><strong class="text-gray-700">Analogy:</strong> Imagine reading a graphic novel where every panel (frame) contributes to a fluid narrative—the model pieces together individual frames to understand the entire video story.</p>
                <p class="mb-2"><strong class="text-gray-700">Novelty & Interest:</strong> ViViT adapts the transformer architecture to video by extending self-attention mechanisms into the temporal domain. It treats sequences of video frames as tokens and explores various strategies for tokenization and attention across both space and time, enabling effective modeling of complex video dynamics.</p>
                <p class="mb-2"><strong class="text-gray-700">Conclusion:</strong> The paper demonstrates that transformer-based models can successfully capture the spatiotemporal structure of videos, achieving competitive results on video classification benchmarks and opening new avenues for video understanding.</p>
                <p class="text-sm text-gray-600"><strong class="text-gray-700">Citation:</strong> (ViViT: A Video Vision Transformer, <a href="https://arxiv.org/abs/2103.15691" target="_blank" rel="noopener noreferrer" class="content-link">arXiv:2103.15691</a>)</p>
            </article>

            <article class="bg-white p-6 rounded-lg shadow-lg hover:shadow-xl transition-shadow duration-300">
                <h3 class="text-2xl font-semibold text-blue-700 mb-3">
                    <a href="https://arxiv.org/abs/2010.11929" target="_blank" rel="noopener noreferrer" class="card-title-link">
                        An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale
                    </a>
                </h3>
                <p class="mb-2"><strong class="text-gray-700">Analogy:</strong> Like breaking a detailed painting into a grid of small, manageable tiles, then interpreting the entire artwork by understanding the relationship between each tile—this method reinterprets images as a sequence of patches.</p>
                <p class="mb-2"><strong class="text-gray-700">Novelty & Interest:</strong> This seminal work introduces the Vision Transformer (ViT), which treats images as sequences of fixed-size patches (each equivalent to a “word”) and applies a transformer architecture traditionally used for natural language. The approach challenges conventional CNNs by showing that pure transformer models can excel at image recognition tasks when provided with sufficient training data.</p>
                <p class="mb-2"><strong class="text-gray-700">Conclusion:</strong> ViT has revolutionized computer vision by achieving state-of-the-art results in image classification and inspiring further research into transformer-based architectures for various vision applications.</p>
                <p class="text-sm text-gray-600"><strong class="text-gray-700">Citation:</strong> (An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale, <a href="https://arxiv.org/abs/2010.11929" target="_blank" rel="noopener noreferrer" class="content-link">arXiv:2010.11929</a>)</p>
            </article>
        </div>
        </section>
    
    <footer>
      <p>Back to <a href="index.html">Home</a></p>
    </footer>
  </div>
</body>
</html>
