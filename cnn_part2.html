<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>1D Convolutions for Time-Series Data Explained</title>
  <style>
    body {
      font-family: sans-serif;
      max-width: 900px;
      margin: 0 auto;
      padding: 2rem;
      line-height: 1.6;
      color: #333;
    }
    h1, h2, h3, h4 {
      color: #2c3e50; /* Darker shade for better contrast */
      margin-top: 1.5em;
      margin-bottom: 0.5em;
    }
    h1 { font-size: 2.5em; border-bottom: 2px solid #3498db; padding-bottom: 0.3em;}
    h2 { font-size: 2em; border-bottom: 1px solid #bdc3c7; padding-bottom: 0.2em;}
    h3 { font-size: 1.5em; }
    h4 { font-size: 1.2em; color: #555;}

    nav { margin-bottom: 30px; padding: 10px; background: #ecf0f1; border: 1px solid #bdc3c7; border-radius: 4px;}
    nav ul { list-style: none; padding: 0; }
    nav li { display: inline-block; margin-right: 15px; }
    nav a { text-decoration: none; color: #3498db; font-weight: bold;}
    nav a:hover { text-decoration: underline; color: #2980b9;}

    pre {
      background: #f8f9f9; /* Lighter background for code blocks */
      padding: 1rem;
      overflow-x: auto;
      border: 1px solid #e1e4e8; /* Softer border */
      border-left: 4px solid #3498db; /* Accent border */
      border-radius: 4px;
      font-size: 0.9em;
    }
    code {
      font-family: 'SFMono-Regular', Consolas, 'Liberation Mono', Menlo, Courier, monospace;
    }
    /* For inline code */
    p > code, li > code, table td > code, ul ul li > code {
      background: #e8eaed;
      padding: 0.2em 0.4em;
      border-radius: 3px;
      font-size: 0.85em;
    }
    pre code { /* Reset for code inside pre, already handled by pre styling */
        background: none;
        padding: 0;
        font-size: 1em; /* Ensure pre's font size is inherited */
    }
    ul, ol {
        padding-left: 20px;
    }
    li {
        margin-bottom: 0.5em;
    }
    strong {
        color: #2980b9;
    }
    hr {
      border: 0;
      height: 1px;
      background: #bdc3c7;
      margin-top: 2em;
      margin-bottom: 2em;
    }
  </style>
 
  <script type="text/javascript" async
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
  </script>
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [['$','$'], ['\\(','\\)']],
        displayMath: [['$$','$$'], ['\\[','\\]']],
        processEscapes: true
      }
    });
  </script>

  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
  <script>hljs.highlightAll();</script>
</head>
<body>
    <nav>
        <ul>
            <li><a href="#fundamentals">Fundamentals</a></li>
            <li><a href="#padding">Padding & Causality</a></li>
            <li><a href="#dilation">Dilation & Receptive Field</a></li>
            <li><a href="#design">Design Considerations</a></li>
            <li><a href="#normalization">Normalization</a></li>
            <li><a href="#separable-convs">Separable Convolutions</a></li>
            <li><a href="#advanced-arch">Advanced Architectures</a></li>
        </ul>
    </nav>

    <h1 id="fundamentals">1D Convolutions for Time-Series Data</h1>

    <h2>Stride, Receptive Field, and Aliasing</h2>
    <p>
        In a 1D convolution layer, <strong>stride</strong> is how many steps you move the filter at each slide along the sequence. A <strong>stride of 1</strong> provides maximum resolution, where every neighboring point contributes to an output. A stride greater than 1 produces fewer output points and thus performs downsampling.
    </p>
    <p>
        The <strong>receptive field</strong> is how much of the input sequence each output point "sees." A larger stride doesn’t change the receptive field of a single filter (that’s set by the kernel size), but it does change the spacing of the outputs. As you stack layers, striding effectively makes the overall receptive field grow faster because each deeper unit corresponds to bigger chunks of the original signal.
    </p>
    <p>
        Using a stride greater than 1 reduces the number of output points, which means fewer multiplications, lower latency, and lower memory cost. That’s why striding is often used instead of pooling for downsampling. However, any downsampling risks <strong>aliasing</strong>.
    </p>
    <p>
        At a sampling rate of $f_s$, any frequency content above the Nyquist frequency ($f_s/2$) folds back into the lower frequency band, causing aliasing. We typically use a low-pass filter to attenuate those high frequencies before downsampling.
    </p>
    <p>
        If you stride by $s$, you downsample by a factor of $s$. If your original sampling rate is 1kHz and you use a stride of 2, the effective sampling rate becomes 500 Hz ($f_s/2$), with the new Nyquist frequency being 250 Hz ($f_s/4$). Now, if the input has frequency content above 250 Hz, it will alias unless the convolution filter itself acts as a low-pass filter before the downsampling. In a CNN, the convolutional filter can act as a crude low-pass filter if its weights are averaging-like, but this is not guaranteed.
    </p>
    <p>
        In the early layers of a CNN for time-series, we often use a <strong>stride of 1</strong> to preserve fine timing details. As we go deeper into the network, we can use a larger stride to capture a larger context without causing the computational cost to explode.
    </p>

    <hr>

    <h2 id="padding">Padding and Causality</h2>
    <p>
        When we apply a convolution, the filter slides across the input. The filter would "hang off" at the ends unless you pad the sequence. If you pass a filter of size $k$ over a sequence of length $L_{in}$ with no padding, the output length will be:
    </p>
    $$L_{out} = L_{in} - k + 1$$
    <p>
        <strong>Zero-padding</strong> is commonly used to address this. With <code>'same'</code> padding, zeros are typically added to both sides of the sequence so that the output length matches the input length (at a stride of 1). Without it, the output shrinks at each layer, and you might lose important information at the edges of the sequence.
    </p>
    <p>The full equation for output length is:</p>
    $$L_{out} = \frac{(L_{in} + 2P - k)}{s} + 1$$
    <p>where $P$ is the padding, $k$ is the kernel size, and $s$ is the stride.</p>
    
    <h3>Causal Padding</h3>
    <p>
        Typical <code>'same'</code> padding looks at both backward and forward in time. With <strong>causal padding</strong>, the filter looks only at the current and past inputs. Causal padding is achieved by padding only the <strong>left</strong> side of the sequence. For real-time inference, you can’t peek into the future; causal convolution ensures that predictions at time $t$ depend only on data up to time $t$. If you use future samples during training, you might get better performance on an offline batch dataset than you will in a real-time deployment.
    </p>
    <p>
        In PyTorch/TensorFlow, when you use <code>padding='same'</code>, the implementation pads the input so the kernel is centered at each output position, meaning it's already looking ahead. To make a convolution causal, you must manually add $k-1$ zeros to the left of the input.
    </p>
    
    <hr>
    
    <h2 id="dilation">Dilated Causal Convolutions and Receptive Field</h2>
    <p>
        To make causal convolutions more powerful, we can use <strong>dilation</strong>, which involves skipping input points at a certain rate $d$. When dilation is 1, the receptive field (RF) is just the kernel size. If we introduce a dilation <code>d > 1</code>, we can look farther back in time. The total span of a single dilated kernel becomes:
    </p>
    $$ \text{Total Span} = (k-1)d + 1 $$
    <p>
        Now let’s stack causal dilated convolution layers. In the first layer, you usually don’t want to dilate because you don't want to miss samples in the raw input or introduce blind spots. When you stack layers, the new layer sees the entire receptive field of the layer below.
    </p>
    <p>
        Let’s define <strong>jump</strong>, the effective stride in raw input units. The jump is how far apart two adjacent outputs at layer <code>l</code> are in terms of raw input indices. At each layer, the jump is updated:
    </p>
    $$ \text{jump}_l = \text{jump}_{l-1} \cdot s_l \cdot d_l $$
    <p>
        This is because stride ($s_l$) skips inputs from the previous layer, and dilation ($d_l$) spaces out the taps inside the kernel, both of which multiply the step size in the raw input. The receptive field can then be calculated iteratively:
    </p>
    $$ RF_l = RF_{l-1} + (k_l - 1) \cdot \text{jump}_{l-1} $$

    <h4>Example: Designing a CNN for a 300ms Receptive Field</h4>
    <p>
        Let’s consider sensory data coming in at a <strong>1kHz sampling rate</strong>. We know that in our trial window, the activity corresponding to our label is about <strong>300 ms</strong> long. Therefore, we want our model’s receptive field to be at least 300 samples to see the entire structure.
    </p>
    <p>
        Let's design a CNN model that has a receptive field (RF) greater than 300. We will mix standard convolutions, striding, and dilations. We start with an input receptive field of 1 and a jump of 1.
    </p>
    <ul>
        <li>
            <strong>Layer 1 (Input):</strong>
            <ul>
                <li><code>RF = 1</code></li>
                <li><code>Jump = 1</code></li>
            </ul>
        </li>
        <li>
            <strong>Layer 2 (Conv Block 1):</strong> Use a standard convolution to capture initial fine-grained features.
            <ul>
                <li>Parameters: <code>Kernel (k)=7</code>, <code>Stride (s)=1</code>, <code>Dilation (d)=1</code></li>
                <li>New Jump: $1 \times 1 \times 1 = \mathbf{1}$</li>
                <li>New RF: $1 + (7-1) \times 1 = \mathbf{7}$</li>
            </ul>
        </li>
        <li>
            <strong>Layer 3 (Conv Block 2):</strong> Downsample the sequence with a strided convolution.
            <ul>
                <li>Parameters: <code>k=5</code>, <code>s=2</code>, <code>d=1</code></li>
                <li>New Jump: $1 \times 2 \times 1 = \mathbf{2}$</li>
                <li>New RF: $7 + (5-1) \times 1 = \mathbf{11}$</li>
            </ul>
        </li>
        <li>
            <strong>Layer 4 (Conv Block 3):</strong> Start expanding the receptive field with a small dilation.
            <ul>
                <li>Parameters: <code>k=3</code>, <code>s=1</code>, <code>d=2</code></li>
                <li>New Jump: $2 \times 1 \times 2 = \mathbf{4}$</li>
                <li>New RF: $11 + (3-1) \times 2 = \mathbf{15}$</li>
            </ul>
        </li>
        <li>
            <strong>Layer 5 (Conv Block 4):</strong> Continue expanding with a larger dilation.
            <ul>
                <li>Parameters: <code>k=3</code>, <code>s=1</code>, <code>d=4</code></li>
                <li>New Jump: $4 \times 1 \times 4 = \mathbf{16}$</li>
                <li>New RF: $15 + (3-1) \times 4 = \mathbf{23}$</li>
            </ul>
        </li>
        <li>
            <strong>Layer 6 (Conv Block 5):</strong> Double the dilation again.
            <ul>
                <li>Parameters: <code>k=3</code>, <code>s=1</code>, <code>d=8</code></li>
                <li>New Jump: $16 \times 1 \times 8 = \mathbf{128}$</li>
                <li>New RF: $23 + (3-1) \times 16 = \mathbf{55}$</li>
            </ul>
        </li>
         <li>
            <strong>Layer 7 (Conv Block 6):</strong> One final large dilation to cross our target.
            <ul>
                <li>Parameters: <code>k=3</code>, <code>s=1</code>, <code>d=16</code></li>
                <li>New Jump: $128 \times 1 \times 16 = \mathbf{2048}$</li>
                <li>New RF: $55 + (3-1) \times 128 = \mathbf{311}$</li>
            </ul>
        </li>
    </ul>
    <p>
        After 6 convolutional layers, we have successfully designed a network with a receptive field of <strong>311 samples</strong>, which covers our target of 300 ms at a 1kHz sampling rate.
    </p>

    <hr>

    <h2 id="design">Design Considerations for Sensory Data CNNs</h2>
    <p>We want to design a CNN model that has a receptive field of, say, 256. With a kernel size of 3, mixing pooling and dilation, it could take us around 9 layers. We have to make some model decisions.</p>
    <ul>
        <li><strong>Do we pool early or late?</strong> If we pool early, the sequence length reduces quickly, leading to faster computation and smaller memory, but we could be throwing away fine details. Early pooling can also reduce latency. If we pool late, there’s a good chance we capture those fine details, but it would be more expensive. For noisy, high-frequency signals, you may want to do pooling early; for low-rate signals, you can postpone it.</li>
        <li><strong>Dilation</strong> can help too, but too much dilation in the early layers can risk holes in the signal and missing local details. This comes at no extra cost, but you may want to apply it once features have stabilized.</li>
        <li><strong>Small kernels</strong> are typically standard, as they stack well to approximate large filters. Sometimes, large kernels can be used in the first layer to capture spectral features.</li>
    </ul>

    <h3>The Role of Pooling in Final Layers</h3>
    <p>
        Both <strong>pooling</strong> and <strong>striding</strong> downsample the time resolution. However, a strided convolution uses a learnable filter, while pooling is a fixed function. In early layers, you may want to keep stride 1 and no pooling. In mid-layers, you can start to compress time.
    </p>
    <p>
        The final layers of a CNN often use <strong>Global Average Pooling (GAP)</strong> across time before a classification task. This squeezes the entire sequence dimension and gives you one feature value per channel. GAP produces a fixed-size representation regardless of the input length and provides translation invariance in time. Instead of flattening a $C \times T$ feature map into a huge vector, GAP collapses it to just $C$ features, which prevents the classification head from exploding in size.
    </p>
    <p>
        Pooling is not the only way. You can learn a weight per time step and let the model focus on the most informative parts of the signal.
    </p>
    $$ z = \sum_t \alpha_t h_t \quad \text{where} \quad \alpha_t = \text{softmax}(W h_t) $$
    <p>
        We can also feed the CNN features into an LSTM or Transformer to model temporal dependencies.
    </p>
    
    <hr>
    
    <h2 id="normalization">Normalization for Time-Series Data</h2>
    <p>
        Let's talk about normalization. <strong>Batch Normalization (BN)</strong> is typically used for images in CNNs to reduce internal covariate shift and stabilize training. For time-series signals with long windows, GPU memory is often tight, so the batch size might be small. If the signals are non-stationary and drift over time, batch normalization can wash out useful information.
    </p>
    <p>
        In images (<code>N, C, H, W</code>), BN computes the mean/variance per channel across the batch <code>N</code> and spatial dimensions <code>H, W</code>. For a 1D time-series signal (<code>N, C, T</code>), BN computes statistics per channel across the batch <code>N</code> and time <code>T</code>. If you normalize a signal that has both weak and strong amplitudes using BN, the absolute difference in amplitude is lost. It erases scale cues that might indicate one burst was stronger than another.
    </p>
    <p>Other forms of normalization are often more suitable:</p>
    <ul>
        <li><strong>InstanceNorm (IN)</strong>: Normalizes per sample, per channel, across the time dimension only. Each trial and channel gets its own mean and standard deviation. This can remove per-trial baseline shifts but also loses absolute amplitude information within that trial.</li>
        <li><strong>LayerNorm (LN)</strong>: Normalizes per sample, across all channels, at each individual time step. This works well for RNNs and Transformers with small batches and preserves differences between trials. However, if one channel is consistently dominant, normalization could suppress that important cue.</li>
        <li><strong>GroupNorm (GN)</strong>: Splits channels into groups and normalizes within each group. It’s a middle ground between IN and LN.</li>
    </ul>
    
    <hr>

    <h2 id="separable-convs">Depthwise Separable Convolutions in 1D</h2>
    <p>
        The number of parameters in a standard 1D convolution is $k \times C_{in} \times C_{out}$. Each filter mixes time and channels jointly. <strong>Depthwise separable convolution</strong> breaks this into two cheaper steps:
    </p>
    <ol>
        <li>A <strong>depthwise convolution</strong> applies one filter per input channel independently (by setting <code>groups=in_channels</code>). This step captures temporal patterns <em>within</em> each channel. The number of parameters is $k \times C_{in}$.</li>
        <li>It is followed by a <strong>pointwise convolution</strong>, which is a <code>1x1</code> convolution that mixes information <em>across</em> channels. The number of parameters is $C_{in} \times C_{out}$.</li>
    </ol>
    <p>
        The total number of parameters is $k \times C_{in} + C_{in} \times C_{out}$, which is significantly less than a standard convolution.
    </p>
    <pre><code class="language-python">
import torch
import torch.nn as nn

class SeparableConv1d(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0):
        super().__init__()
        
        # Depthwise: one conv per channel (groups=in_channels)
        self.depthwise = nn.Conv1d(
            in_channels, 
            in_channels, 
            kernel_size=kernel_size, 
            stride=stride, 
            padding=padding, 
            groups=in_channels,  # <- key!
            bias=False
        )
        
        # Pointwise: 1x1 conv to mix channels
        self.pointwise = nn.Conv1d(
            in_channels, 
            out_channels, 
            kernel_size=1, 
            bias=False
        )

    def forward(self, x):
        x = self.depthwise(x)  # per-channel temporal filtering
        x = self.pointwise(x)  # cross-channel mixing
        return x
    </code></pre>
    
    <hr>

    <h2 id="advanced-arch">Advanced Architectures for Temporal Signals</h2>
    
    <h3>Time-Depth Separable (TDS) Block</h3>
    <p>
        A <strong>Time-Depth Separable (TDS) block</strong> is inspired by depthwise separable convolution and is tailored for temporal signals. It has three main steps: a time-channel separable convolution, a pointwise convolution, and a residual connection with normalization and dropout. It’s basically a temporal pattern extractor + channel mixer + residual block. <strong>LayerNorm</strong> is often used because it respects the time-series structure.
    </p>
    <pre><code class="language-python">
import torch.nn.functional as F

class TDSBlock(nn.Module):
    """
    Time-Depth Separable (TDS) block for temporal signals (EMG/IMU/Audio).
    Steps:
      1) Depthwise temporal conv (per-channel).
      2) Pointwise conv (1x1) to mix across channels.
      3) Residual + normalization + dropout.
    """
    def __init__(self, channels, kernel_size=5, dropout=0.1):
        super().__init__()
        
        self.depthwise = nn.Conv1d(
            in_channels=channels,
            out_channels=channels,
            kernel_size=kernel_size,
            groups=channels,  # <- depthwise!
            padding=kernel_size // 2,
            bias=False
        )
        
        self.pointwise1 = nn.Conv1d(channels, channels, kernel_size=1, bias=False)
        self.pointwise2 = nn.Conv1d(channels, channels, kernel_size=1, bias=False)
        
        self.norm = nn.LayerNorm(channels)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x):
        # x: (batch, channels, time)
        residual = x
        
        out = self.depthwise(x)
        out = F.relu(out)
        
        out = self.pointwise1(out)
        out = F.relu(out)
        out = self.pointwise2(out)
        
        out = self.dropout(out)
        out = out + residual  # residual connection
        
        # LayerNorm expects (batch, time, channels)
        out = out.transpose(1, 2)
        out = self.norm(out)
        out = out.transpose(1, 2)
        
        return out
    </code></pre>

    <h3>Multi-Scale TDS</h3>
    <p>
        You can create <strong>Multi-Scale TDS blocks</strong> by running TDS blocks in parallel, with each branch operating on a different temporal scale achieved through downsampling. This allows the network to capture both fine-grained bursts and long-range temporal structures simultaneously.
    </p>
    <p>
        <strong>Average pooling</strong> is used for downsampling as it acts as a low-pass filter to prevent aliasing. The output from each branch is then upsampled back to the original time resolution so it can be fused with the outputs from other branches.
    </p>
    <pre><code class="language-python">
class MultiScaleTDS(nn.Module):
    """
    Multiscale TDS design (s = 0..5).
    At each scale:
      - Downsample with average pooling (2^s).
      - Apply a TDS block.
      - Upsample back and fuse outputs.
    """
    def __init__(self, channels, kernel_size=5, num_scales=6, dropout=0.1):
        super().__init__()
        
        self.scales = nn.ModuleList()
        for s in range(num_scales):
            block = TDSBlock(channels, kernel_size=kernel_size, dropout=dropout)
            self.scales.append(block)
        
        self.num_scales = num_scales

    def forward(self, x):
        # x: (batch, channels, time)
        outputs = []
        T = x.size(-1)
        
        for s, block in enumerate(self.scales):
            # Downsample by 2^s
            pool_factor = 2**s
            pooled = F.avg_pool1d(x, kernel_size=pool_factor, stride=pool_factor, ceil_mode=True)
            
            # Apply TDS block
            out = block(pooled)
            
            # Upsample back to original time length
            out = F.interpolate(out, size=T, mode="linear", align_corners=False)
            
            outputs.append(out)
        
        # Fuse multiscale outputs
        return sum(outputs) / self.num_scales
    </code></pre>
    
</body>
</html>
