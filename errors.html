<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Understanding Errors, Optimizers, and Scaling Laws in ML</title>
  <style>
    body {
      font-family: sans-serif;
      max-width: 900px;
      margin: 0 auto;
      padding: 2rem;
      line-height: 1.6;
      color: #333;
    }
    h1, h2, h3, h4 {
      color: #2c3e50; /* Darker shade for better contrast */
      margin-top: 1.5em;
      margin-bottom: 0.5em;
    }
    h1 { font-size: 2.5em; border-bottom: 2px solid #3498db; padding-bottom: 0.3em;}
    h2 { font-size: 2em; border-bottom: 1px solid #bdc3c7; padding-bottom: 0.2em;}
    h3 { font-size: 1.5em; }
    h4 { font-size: 1.2em; color: #555;}

    nav { margin-bottom: 30px; padding: 10px; background: #ecf0f1; border: 1px solid #bdc3c7; border-radius: 4px;}
    nav ul { list-style: none; padding: 0; }
    nav li { display: inline-block; margin-right: 15px; }
    nav a { text-decoration: none; color: #3498db; font-weight: bold;}
    nav a:hover { text-decoration: underline; color: #2980b9;}

    pre {
      background: #f8f9f9; /* Lighter background for code blocks */
      padding: 1rem;
      overflow-x: auto;
      border: 1px solid #e1e4e8; /* Softer border */
      border-left: 4px solid #3498db; /* Accent border */
      border-radius: 4px;
      font-size: 0.9em;
    }
    code {
      font-family: 'SFMono-Regular', Consolas, 'Liberation Mono', Menlo, Courier, monospace;
    }
    /* For inline code */
    p > code, li > code, table td > code {
      background: #e8eaed;
      padding: 0.2em 0.4em;
      border-radius: 3px;
      font-size: 0.85em;
    }
    pre code { /* Reset for code inside pre, already handled by pre styling */
        background: none;
        padding: 0;
        font-size: 1em; /* Ensure pre's font size is inherited */
    }
    ul, ol {
        padding-left: 20px;
    }
    li {
        margin-bottom: 0.5em;
    }
    strong {
        color: #2980b9;
    }
    hr {
      border: 0;
      height: 1px;
      background: #bdc3c7;
      margin-top: 2em;
      margin-bottom: 2em;
    }
  </style>
 
  <script type="text/javascript" async
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
  </script>
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [['$','$'], ['\\(','\\)']],
        displayMath: [['$$','$$'], ['\\[','\\]']],
        processEscapes: true
      }
    });
  </script>

  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
  <script>hljs.highlightAll();</script>
</head>
<body>
    <nav>
        <ul>
            <li><a href="#error-analysis">Error Analysis</a></li>
            <li><a href="#evaluation">Evaluation</a></li>
            <li><a href="#generalization">Generalization</a></li>
            <li><a href="#preprocessing">Preprocessing</a></li>
            <li><a href="#regularization">Regularization & Augmentation</a></li>
            <li><a href="#optimizers">Optimizers</a></li>
            <li><a href="#schedulers">Schedulers</a></li>
            <li><a href="#scaling-laws">Scaling Laws</a></li>
        </ul>
    </nav>

    <h1 id="error-analysis">Understanding and Mitigating Errors in Machine Learning</h1>
    <p>
        Even with lots of training data, models don’t perform perfectly. There are three main sources of error that contribute to the total <strong>True Error</strong>.
    </p>
    <p><strong>True Error = Approximation Error + Estimation Error + Optimization Error</strong></p>

    <h3>1. Approximation Error (Bias from Model Class)</h3>
    <p>
        This error is due to the chosen model class not being expressive enough to represent the true underlying function. The model is too weak.
    </p>
    <ul>
        <li><strong>Symptom</strong>: The model is <strong>underfitting</strong>. Both training error and test error are high. The test error plateaus even when more data is added, or the training error doesn't drop significantly even with a more expressive model.</li>
        <li><strong>Solution</strong>: You need a stronger, more complex model.</li>
    </ul>

    <h3>2. Estimation Error (Generalization Gap, Variance)</h3>
    <p>
        This is error that arises from having insufficient data. With a small dataset, the model may overfit to noise and random fluctuations, failing to find the right parameters that generalize.
    </p>
    <ul>
        <li><strong>Symptom</strong>: The model is <strong>overfitting</strong>. The training error is low, but the test error is high, creating a large "generalization gap." The test error improves when more data is added.</li>
        <li><strong>Solution</strong>: You need more data, regularization, or data augmentation. A simpler model can also help.</li>
    </ul>

    <h3>3. Optimization Error</h3>
    <p>
        This error is due to the training algorithm failing to find the best possible parameters within the chosen model class. This can be caused by non-convex loss functions, getting stuck in bad local minima, or navigating saddle points.
    </p>
    <ul>
        <li><strong>Symptom</strong>: The training error decreases too slowly or plateaus early, even with an expressive model.</li>
        <li><strong>Solution</strong>: You need better optimizers or well-tuned learning rate schedules.</li>
    </ul>

    <h3>Irreducible Error (Bayes Error)</h3>
    <p>
        There is one more type of error that you cannot remove, no matter the model, data, or optimizer. Suppose the true relationship between an input $x$ and a target $y$ is:
    </p>
    $$ Y = f(x) + \epsilon $$
    <p>
        The term $\epsilon$ represents noise from natural randomness in the data-generating process. This is the <strong>Bayes error</strong> or <strong>irreducible error</strong>. This error sets the lower bound on the test error. If you keep improving your model and the test error plateaus, you might be hitting this floor.
    </p>
    <p>You know you've likely hit this limit when:</p>
    <ul>
        <li>Training error is low (optimization succeeded).</li>
        <li>Test error is close to training error (low variance).</li>
        <li>Increasing model size or data volume doesn’t dramatically reduce the error.</li>
    </ul>
    <p>
        At this point, the model has learned the signal, and what remains is inherent noise. The only way to potentially reduce this error floor is to introduce data from another modality that can provide complementary signals about the true latent state.
    </p>

    <hr>

    <h2 id="evaluation">Model Evaluation Strategies</h2>
    <p>When it comes to evaluating a model, what comes to mind?</p>
    <ul>
        <li><strong>Accuracy</strong>: Does it classify correctly?</li>
        <li><strong>Latency</strong>: Is it fast enough for real-time use?</li>
        <li><strong>Robustness</strong>: Does it handle new users or noisy sensors?</li>
        <li><strong>Usability</strong>: Does it work for daily scenarios, not just on lab data?</li>
    </ul>
    <p>There are three main testing strategies:</p>
    <ul>
        <li><strong>Offline Testing</strong>: Using cross-validation (within-subject and cross-subject) and measuring standard metrics on a static dataset.</li>
        <li><strong>Online Testing</strong>: Testing the system in a live, streaming mode to measure real-world latency, stability, and false positive rates. What is the throughput? How many errors occur per hour?</li>
        <li><strong>Stress Testing</strong>: Intentionally varying conditions like sensor placement or noise levels. Augmenting data with synthetic perturbations. Is the system able to recover quickly from an error?</li>
    </ul>

    <hr>

    <h2 id="generalization">Generalization, Overfitting, and Adaptation</h2>
    <p>
        <strong>Overfitting</strong> happens when a model learns the training data too well, including its noise and irrelevant patterns, causing it to perform poorly on new, unseen data.
    </p>
    
    <h3>Generalization in the Context of Wearables</h3>
    <p>
        When can you claim a model generalizes? For wearables, training and testing on the <em>same</em> subjects is a much easier problem because the model can memorize a subject's idiosyncrasies. The true test is generalization to new people.
    </p>
    <ul>
        <li><strong>Cross-Subject Generalization</strong>: Train on a set of people and test on an entirely new set.</li>
        <li><strong>Temporal Generalization</strong>: Does a model trained on a subject's data from Day 1 still work on their data from Day 2? Is it robust to time variance?</li>
        <li><strong>Domain Shift</strong>: Does varying the sensor placement or recording conditions cause the model's performance to collapse?</li>
    </ul>
    <p>
        If your model works for some users but not for new users, it has likely overfit to subject-specific patterns. The solution involves data, the model, and adaptation strategies.
    </p>
    <ul>
        <li><strong>Data</strong>: Collect more diverse training data.</li>
        <li><strong>Adaptation</strong>: Efficiently adapt the model to new subjects.
            <ul>
                <li><strong>Fine-tuning</strong>: Freeze most of the model and fine-tune only the final layers.</li>
                <li><strong>Adapter Modules</strong>: Introduce lightweight modules like LoRA or bottleneck layers for efficient fine-tuning.</li>
                <li><strong>Layer-wise Unfreezing</strong>: Start by fine-tuning only the head, and if performance plateaus, unfreeze deeper layers one by one to prevent catastrophic forgetting.</li>
                <li><strong>Regularization</strong>: Use weight regularization to keep personalized weights close to the pre-trained ones.</li>
            </ul>
        </li>
    </ul>

    <hr>
    
    <h2 id="preprocessing">Preprocessing and Data Handling for Sensory Data</h2>
    <p>
        Sensory data from wearables can be noisy due to powerline interference, baseline drifts, or sensor impedance issues. Signal processing can improve the signal-to-noise ratio.
    </p>
    <ul>
        <li><strong>Bandpass filtering</strong> to remove low-frequency drift and high-frequency noise.</li>
        <li><strong>Notch filtering</strong> to suppress powerline interference.</li>
        <li><strong>Rectification and enveloping</strong> by taking the absolute value and then low-pass filtering.</li>
        <li><strong>Z-scoring</strong> per channel or scaling signals to reduce inter-subject amplitude variability.</li>
    </ul>

    <h3>Handling Missing Signals</h3>
    <p>Signals can be missing due to Bluetooth packet loss or sweat increasing sensor impedance.</p>
    <ul>
        <li><strong>Interpolation</strong>: Detect flatlined or saturated channels and interpolate their values from spatial or temporal neighbors.</li>
        <li><strong>Masking</strong>: Make the model aware of noise by providing a binary mask of which sensors are valid, so it learns to ignore missing ones.</li>
        <li><strong>Attention</strong>: Use an attention mechanism to allow the model to learn to weight more reliable sensors higher.</li>
    </ul>
    
    <h3>Fusing Modalities</h3>
    <p>
        If one modality is missing, another may still provide information.
    </p>
    <ul>
        <li><strong>Separate Encoders</strong>: Use separate encoders for each modality and concatenate their latent features before feeding them to a joint decoder.</li>
        <li><strong>Cross-Modal Attention</strong>: Let features from one sensor form the query and features from other sensors form the keys and values. Attention can then learn to synchronize and weight the modalities based on context and reliability.</li>
    </ul>
    
    <hr>

    <h2 id="regularization">Regularization and Data Augmentation</h2>
    
    <h3>Regularization: Dropout</h3>
    <p>
        <strong>Dropout</strong> randomly sets some activations to zero during training with a probability $p$. This prevents the network from relying too heavily on any single neuron. Each forward pass effectively trains a slightly different, "thinned" sub-network. To keep the expected activation value the same during training and testing, the remaining activations are scaled up by a factor of $1/(1-p)$. At inference time, the full network is used without dropping any neurons.
    </p>

    <h3>Data Augmentation for Wearables</h3>
    <p>Beyond adding noise or scaling, we can use realistic augmentations for wearable data:</p>
    <ul>
        <li><strong>Channel Dropout</strong>: Randomly drop entire sensor channels during training to mimic loss of contact.</li>
        <li><strong>Channel Shuffling</strong>: Permute sensors within a local neighborhood to mimic the band shifting along an arm.</li>
        <li><strong>Spatial Mixing</strong>: Replace a sensor's signal with a weighted average of its neighbors to simulate displacement blur.</li>
        <li><strong>Rotation Augmentation</strong>: Apply 3D rotations to sensor data to mimic the device being worn at different angles. A general rotation matrix $R(\alpha, \beta, \gamma) = R_x R_y R_z$ can be applied to each sensor vector $v$ as $v' = v \cdot R^T$.</li>
        <li><strong>Random Scaling</strong>: Multiply signals by random factors to mimic changes in strap tightness or skin impedance.</li>
        <li><strong>Temporal Jitter</strong>: Slightly shift or stretch signals in time.</li>
        <li><strong>Cross-Modal Masking</strong>: Temporarily drop one modality to encourage robustness to partial signal availability.</li>
    </ul>

    <h3>Handling Class Imbalance</h3>
    <p>Class imbalance can cause overfitting to majority classes.</p>
    <ul>
        <li><strong>Resampling</strong>: Oversample minority classes or undersample majority classes. <strong>SMOTE</strong> is a classic technique that oversamples by interpolating between existing minority samples.</li>
        <li><strong>Loss-level Techniques</strong>: Use a <strong>class-weighted loss</strong> to give more importance to minority classes, or a <strong>focal loss</strong> to focus training on hard-to-classify examples.</li>
        <li><strong>Balanced Batch Sampling</strong>: Ensure each batch has a roughly equal representation of all classes.</li>
    </ul>
    
    <hr>

    <h2 id="optimizers">Optimizers</h2>
    
    <h3>SGD and Momentum</h3>
    <p>
        <strong>Stochastic Gradient Descent (SGD)</strong> updates model parameters using a random subset (mini-batch) of data.
    </p>
    $$ \theta_{t+1} = \theta_t - \eta \cdot \nabla_{\theta_t} L $$
    <p>
        It is noisy but fast and can escape poor local minima. <strong>SGD with Momentum</strong> smooths the update path by accumulating a running average of past gradients.
    </p>
    $$ v_{t+1} = \mu v_t + \eta \cdot \nabla_{\theta_t} L $$
    $$ \theta_{t+1} = \theta_t - v_{t+1} $$
    <p>
        Here, $v_t$ is the velocity and $\mu$ is the momentum coefficient (typically ~0.9). It builds up speed in consistent directions. <strong>Nesterov Accelerated Gradient (NAG)</strong> improves on this by "looking ahead." It calculates the gradient not at the current position, but at a future-leaning position where momentum would have taken it, preventing overshooting.
    </p>
    
    <h3>Adaptive Learning Rate Optimizers</h3>
    <p>These optimizers adapt the learning rate for each parameter individually.</p>
    <ul>
        <li><strong>AdaGrad</strong>: Scales each parameter's learning rate based on the cumulative sum of its squared gradients. It gives larger updates for infrequent parameters and smaller updates for frequent ones. However, the learning rate shrinks monotonically and can eventually become too small.</li>
        <li><strong>RMSProp</strong>: Uses an <em>exponentially decaying average</em> of squared gradients instead of a cumulative sum. This allows the optimizer to adapt more quickly to recent gradient information and prevents the learning rate from vanishing.</li>
        <li><strong>Adam (Adaptive Moment Estimation)</strong>: Combines the momentum (first moment) of SGD with the adaptive learning rates (second moment) of RMSProp. It also includes a bias-correction step to ensure stable updates, especially at the beginning of training.
        $$ m_t = \beta_1 m_{t-1} + (1-\beta_1)g_t \quad (\text{First moment}) $$
        $$ v_t = \beta_2 v_{t-1} + (1-\beta_2)g_t^2 \quad (\text{Second moment}) $$
        $$ \theta_{t+1} = \theta_t - \eta \frac{\hat{m}_t}{\sqrt{\hat{v}_t} + \epsilon} $$
        where $\hat{m}_t$ and $\hat{v}_t$ are the bias-corrected moments.
        </li>
    </ul>

    <h3>Regularization: L1 vs. L2</h3>
    <p>Regularization is used to prevent overfitting by penalizing large weights.</p>
    <ul>
        <li><strong>L2 Regularization (Weight Decay)</strong>: Adds a penalty proportional to the squared magnitude of the weights ($\lambda/2 \cdot ||\theta||^2$). This encourages the model to keep weights small and spread out, leading to <strong>shrinkage</strong>.</li>
        <li><strong>L1 Regularization</strong>: Adds a penalty proportional to the absolute magnitude of the weights ($\lambda \cdot ||\theta||_1$). This encourages some weights to become exactly zero, leading to <strong>sparsity</strong>.</li>
    </ul>

    <h3>Other Optimizers and Techniques</h3>
    <ul>
        <li><strong>AdamW</strong> is a variant of Adam that decouples the L2 regularization from the gradient update. Instead of mixing the weight decay term into the adaptive moments, it applies it directly to the weights after the main Adam step. This often leads to better generalization.</li>
        <li><strong>Nadam</strong>: Incorporates Nesterov momentum into the Adam algorithm for potentially faster convergence.</li>
        <li><strong>SAM (Sharpness-Aware Minimization)</strong>: A wrapper that can be used with another optimizer (like AdamW). Instead of finding the point of lowest loss, it finds parameters that lie in a "flat" minima, where the loss remains low even if the weights are slightly perturbed. This has been shown to improve model generalization.</li>
    </ul>
    
    <hr>
    
    <h2 id="schedulers">Learning Rate Schedulers</h2>
    <p>A constant learning rate is rarely optimal. Schedulers adapt the learning rate over time.</p>
    <ul>
        <li><strong>Step Decay</strong>: Drops the learning rate by a fixed factor every few epochs.</li>
        <li><strong>Exponential Decay</strong>: Smoothly shrinks the learning rate at every step.</li>
        <li><strong>Cosine Annealing</strong>: Makes the learning rate follow a cosine curve, starting high, decreasing smoothly to a minimum, and potentially restarting in cycles. This keeps the learning rate higher for longer, encouraging exploration.</li>
        <li><strong>One-Cycle Policy</strong>: Increases the learning rate for the first part of training and then decreases it for the second part. This can help the model escape bad minima early and settle into a good solution later.</li>
    </ul>
    <p>
        <strong>Gradient Clipping</strong> is another technique that prevents exploding gradients by capping their magnitude (norm) at a certain threshold.
    </p>

    <hr>

    <h2 id="scaling-laws">Scaling Laws</h2>
    <p>
        <strong>Scaling laws</strong> describe how model performance (usually test loss) improves as you increase data size, model size, or compute. A common formulation is the <strong>power law</strong>:
    </p>
    $$ L(N) = L_{\infty} + K \cdot N^{-\alpha} $$
    <ul>
        <li>$L(N)$ is the loss for a dataset of size $N$.</li>
        <li>$L_{\infty}$ is the irreducible error floor.</li>
        <li>$K$ and $\alpha$ are constants that can be estimated by fitting the curve on a log-log plot.</li>
    </ul>
    <p>
        These laws help you predict the benefit of further investment. If $L_{\infty}$ is high, your data modality may be inherently limited. If the scaling exponent $\alpha$ is large, you will get a noticeable gain from adding more data or parameters. This can be extended to joint scaling laws that trade off between data ($N$) and model parameters ($P$):
    </p>
    $$ L(N, P) \approx L_{\infty} + K \cdot N^{-\alpha} + C \cdot P^{-\beta} $$
</body>
</html>
